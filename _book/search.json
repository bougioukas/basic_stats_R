[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Practical Statistics in Medicine with R",
    "section": "",
    "text": "Preface\nThis textbook is based on my notes from a series of lectures given for a few years at the Aristotle University of Thessaloniki.\nThe textbook can be used as support material for practical labs on basic statistics using R at any level from beginner to advanced. It can also be used as a support for self-teaching.\nI have paid particular attention to the form of the book, which I think should aid understanding the most common statistical tests using Base R and pipe-friendly functions, coherent with the ‘tidyverse’ design philosophy. The {ggplot2} package and many ggplot2 extensions are the preferred tools of choice for constructing data visualizations in our textbook.\nThroughout this textbook we will use R via RStudio. It is also recommended to work with RStudio Projects. This enables to organize our files and switch between different projects without getting the data, scripts, or output files all mixed up. Everything gets read in or saved to the right directory.\nThe proposed RStudio Project should contain at least the following subfolders:"
  },
  {
    "objectID": "index.html#reproducibility-and-license",
    "href": "index.html#reproducibility-and-license",
    "title": "Practical Statistics in Medicine with R",
    "section": "Reproducibility and License",
    "text": "Reproducibility and License\nAll sections of this textbook are reproducible as they were made using Quarto® which is an open-source scientific and technical publishing system built on Pandoc.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\nThis textbook is free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License."
  },
  {
    "objectID": "introduction.html#the-discipline-of-statistics",
    "href": "introduction.html#the-discipline-of-statistics",
    "title": "1  Introduction",
    "section": "\n1.1 The discipline of statistics",
    "text": "1.1 The discipline of statistics\nStatistics is an empirical or practical method for collecting, organizing, summarizing, and presenting data, and for making inferences about the population from which the data are drawn.\nThe discipline of traditional (frequentist) statistics includes two main branches (Figure 1.1):\n\ndescriptive statistics that includes measures of frequency and measures of location and dispersion. It also includes a description of the general shape of the distribution of the data.\ninferential statistics that aims at generalizing conclusions made on a sample to a whole population. It includes estimation and hypothesis testing.\n\n\n\n\n\nflowchart LR\n  \n    A[Traditional &lt;br/&gt; Statistics]--- B[Descriptive statistics]\n    A --- C[Inferential statistics]\n    B --- D[Measures of frequency: &lt;br/&gt; e.g., frequency, percentage.]\n    B --- E[Measures of location &lt;br/&gt; and dispersion: &lt;br/&gt; e.g., mean, standard deviation.]\n    C --- H[Estimation]\n    C --- I[Hypothesis Testing]\n    style A color:#980000, stroke:#333,stroke-width:4px\n    \n\n\nFigure 1.1: The discipline of statistics and its two branches, descriptive statistics and inferential statistics"
  },
  {
    "objectID": "introduction.html#data-and-variables",
    "href": "introduction.html#data-and-variables",
    "title": "1  Introduction",
    "section": "\n1.2 Data and Variables",
    "text": "1.2 Data and Variables\nBiomedical Data\nBiomedical data have unique features compared with data in other domains. The data may include administrative health data, biomarker data, biometric data (for example, from wearable technologies) and imaging, and may originate from many different sources, including Electronic Health Records (EHRs), surveillance systems, clinical registries, biobanks, the internet (e.g. social media) and patient self-reports.\nBiomedical data can be transformed into information. This information can become knowledge if the researchers and clinicians understand it (?fig-info).\n\n\n\n\nFrom data to knowledge.\n\n\n\n\nThere are three main data structures: structured data, unstructured data, and semi-structured data.\nStructured data is generally tabular data that is represented by columns and rows in a database.\nSemi-Structured data is a form of structured data that does not obey the tabular structure, yet does have some structural properties. Emails, for example, are semi-structured by sender, recipient,subject, date, time etc. and they are also organized into folders, like Inbox, Sent, Trash, etc.\nUnstructured data usually open text (such as social media posts), images, videos, etc., that have no predetermined organization or design.\nIn this textbook we use data organized in a structured format (spreadsheets). In statistics, tabular data refers to data that are organized in a table with rows and columns. A row is a observation (or record), which corresponds to the statistical unit of the dataset. The columns are the variables (or characteristics) of interest.\n \nVariables\nA variable is a quantity or characteristic that is free to vary, or take on different values. To gain information on variables and their associations, it is necessary to design and conduct scientific experiments.\nResearchers design experiments to test if changes to one or more variables are associated with changes to another variable of interest. For example, if researchers hypothesize that a new therapy is more effective than the usual care for migraine pain, they could design an experiment to test this hypothesis; participants should randomly assigned to one of two groups: the experimental group receiving the new treatment that is being tested, and the control group receiving an conventional treatment. In this experiment, the type of treatment each participant received (i.e., new treatment vs. conventional treatment) is the independent variable (IV), while the pain relief is the dependent variable (DV) or the outcome variable.\n\n\n\n\n\n\nIndependent Vs Dependent variables\n\n\n\nAn independent variable is the variable that is changed or controlled in a scientific experiment to test the effects on another variable.\nA dependent (outcome) variable is the variable being tested in a scientific experiment and is affected by the independent variable(s) of interest."
  },
  {
    "objectID": "introduction.html#types-of-data",
    "href": "introduction.html#types-of-data",
    "title": "1  Introduction",
    "section": "\n1.3 Types of Data",
    "text": "1.3 Types of Data\nData in variables can be either categorical or numerical (otherwise known as qualitative and quantitative) in nature (?fig-types_data).\n\n\n\n\nBroad classification of the different types of data with examples.\n\n\n\n\n \n\n\n\n\n\n\nNote\n\n\n\nThe degree of measurement accuracy and the type of data are both important factors in the decision to perform a statistical analysis.\n\n\n \nCategorical Data\nA. Nominal Data\nNominal data can be labeled creating distinct unordered categories. They are not measured but simply counted. They can be either binary such as dead/alive; cured/not cured or have more than two categories, for example, blood group A, B, AB, O; type I, type II or gestational diabetes; eye color (e.g., brown, blue, green, gray).\n\n\n\n\n\n\nNumerical representation of categories are just codes\n\n\n\nWe can denote dead/alive as 1/0 for health status and denote A/B/AB/O as 1/2/3/4 for blood type. Unlike numerical data, the numbers representing different categories do not have mathematical meaning (they are just codes).\n\n\n \nΒ. Ordinal Data\nWhen the categories can be ordered, the data are of ordinal type. For example, patients may classify their degree of pain as minimal, moderate, severe, or unbearable. In this case, there is a natural order of the values, since moderate pain is more intense than minimal and less than severe pain.\n\n\n\n\n\n\nCollapsion of categories leads to a loss of information\n\n\n\nOrdinal data are often transformed into binary data to simplify analysis, presentation and interpretation, which may result in a considerable loss of information.\n\n\n \nNumerical Data\nA. Discrete Data\nDiscrete data can take only a finite number of values (usually integers) in a range, for example, the number of children in a family or the number of days missed from work. Other examples are often counts per unit of time such as the number of deaths in a hospital per year, the number of visits to the general practitioner in a year, or the number of epileptic seizures a patient has per month. In dentistry, a common measure is the number of decayed, filled or missing teeth.\n\n\n\n\n\n\nDiscrete Vs Ordinal data\n\n\n\nIn practice discrete data are often treated in statistical analyses as if they were ordinal data. Although this may be acceptable, we do not take the most out of our data.\n\n\n \nΒ. Continuous Data\nContinuous data are numbers (usually with units) that can take any value within a given range. However, in practice, they are restricted by the accuracy of the measuring instrument. Height, weight, blood pressure, cholesterol level, body temperature, body mass index (BMI) are just few examples of variables that take continuous values.\n\n\n\n\n\n\nCategorization of numerical data leads to a loss of information\n\n\n\nContinuous data are often categorized creating categorical variables. For example, the BMI, which is a continuous variable, is usually converted into an ordinal variable with four categories (underweight, normal, overweight and obese). However, dividing continuous variables into categories leads to a loss of information."
  },
  {
    "objectID": "descriptive.html#packages-we-need",
    "href": "descriptive.html#packages-we-need",
    "title": "2  Descriptive statistics",
    "section": "\n2.1 Packages we need",
    "text": "2.1 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(EnvStats)\nlibrary(scales)\nlibrary(questionr)\nlibrary(MESS)\nlibrary(ggsci)\nlibrary(patchwork)\n\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "descriptive.html#importing-the-data",
    "href": "descriptive.html#importing-the-data",
    "title": "2  Descriptive statistics",
    "section": "\n2.2 Importing the data",
    "text": "2.2 Importing the data\nWe will use the dataset named arrhythmia which is a .xlsx file. We can read the data with the following code:\n\n\nNOTE: It is supposed that we work with RStudio Projects and the file “arrhythmia.xlsx” is stored in a subfolder with the name “data” inside the RStudio Project folder.\nThe function here() allows us to navigate throughout each of the subfolders and files within a given Project using relative paths such as “data/arrhythmia.xlsx”.\n\nlibrary(readxl)\narrhythmia &lt;- read_excel(here(\"data\", \"arrhythmia.xlsx\"))\n\n\n\n\nFigure 2.1: Table with raw data of arrhythmia data set.\n\n\n\nWe take a look at the data:\n\nglimpse(arrhythmia)\n\nRows: 428\nColumns: 8\n$ age        &lt;dbl&gt; 75, 56, 54, 55, NA, 40, 49, 44, 50, 62, 45, 54, 30, 44, 47,…\n$ sex        &lt;chr&gt; \"male\", \"female\", \"male\", \"male\", \"male\", \"female\", \"female…\n$ height     &lt;dbl&gt; 190, 165, 172, 175, 190, 160, 162, 168, 167, 170, 165, 172,…\n$ weight     &lt;dbl&gt; 80, 64, 95, 94, 80, 52, 54, 56, 67, 72, 86, 58, 73, 88, 48,…\n$ QRS        &lt;dbl&gt; 91, 81, 138, 100, 88, 77, 78, 84, 89, 102, 77, 78, 91, 77, …\n$ heart_rate &lt;dbl&gt; 63, 53, 75, 71, 75, 70, 67, 64, 63, 70, 72, 73, 56, 72, 76,…\n$ bmi        &lt;dbl&gt; 22.2, 23.5, 32.1, 30.7, 22.2, 20.3, 20.6, 19.8, 24.0, 24.9,…\n$ bmi_cat    &lt;chr&gt; \"normal\", \"normal\", \"obese\", \"obese\", \"normal\", \"normal\", \"…\n\n\nAdditionally, we can get some basic summary measures for each variable:\n\nsummary(arrhythmia)\n\n      age            sex                height        weight     \n Min.   :18.00   Length:428         Min.   :146   Min.   : 18.0  \n 1st Qu.:37.00   Class :character   1st Qu.:160   1st Qu.: 60.0  \n Median :47.00   Mode  :character   Median :165   Median : 70.0  \n Mean   :48.44                      Mean   :165   Mean   : 70.1  \n 3rd Qu.:59.00                      3rd Qu.:170   3rd Qu.: 80.0  \n Max.   :83.00                      Max.   :190   Max.   :176.0  \n NA's   :3                                                       \n      QRS          heart_rate          bmi          bmi_cat         \n Min.   : 55.0   Min.   : 44.00   Min.   : 5.20   Length:428        \n 1st Qu.: 80.0   1st Qu.: 65.00   1st Qu.:22.90   Class :character  \n Median : 86.0   Median : 72.00   Median :25.40   Mode  :character  \n Mean   : 88.4   Mean   : 73.33   Mean   :25.72                     \n 3rd Qu.: 94.0   3rd Qu.: 80.00   3rd Qu.:28.10                     \n Max.   :188.0   Max.   :120.00   Max.   :61.60                     \n                                                                    \n\n\nThe data set arrhythmia has 428 patients (rows) and includes 8 variables (columns) as follows:\n\nage: age (yrs)\nsex: sex (male, female)\nheight: height (cm)\nweight: weight (kg)\nQRS: mean duration of QRS (ms) \n\nheart_rate: heart rate (beats/min)\n\nWe might have noticed that the categorical variables sex and bmi_cat are recognized of character &lt;chr&gt; type. We can use the factor() function inside the mutate() to convert the variables to factors as follows:\n\narrhythmia &lt;- arrhythmia %&gt;% \n  mutate(sex = factor(sex),\n         bmi_cat = factor(bmi_cat, levels = c(\"underweight\", \"normal\", \n                                              \"overweight\", \"obese\")))\n\nLet’s look at the data again with the glipmse() function:\n\nglimpse(arrhythmia)\n\nRows: 428\nColumns: 8\n$ age        &lt;dbl&gt; 75, 56, 54, 55, NA, 40, 49, 44, 50, 62, 45, 54, 30, 44, 47,…\n$ sex        &lt;fct&gt; male, female, male, male, male, female, female, male, femal…\n$ height     &lt;dbl&gt; 190, 165, 172, 175, 190, 160, 162, 168, 167, 170, 165, 172,…\n$ weight     &lt;dbl&gt; 80, 64, 95, 94, 80, 52, 54, 56, 67, 72, 86, 58, 73, 88, 48,…\n$ QRS        &lt;dbl&gt; 91, 81, 138, 100, 88, 77, 78, 84, 89, 102, 77, 78, 91, 77, …\n$ heart_rate &lt;dbl&gt; 63, 53, 75, 71, 75, 70, 67, 64, 63, 70, 72, 73, 56, 72, 76,…\n$ bmi        &lt;dbl&gt; 22.2, 23.5, 32.1, 30.7, 22.2, 20.3, 20.6, 19.8, 24.0, 24.9,…\n$ bmi_cat    &lt;fct&gt; normal, normal, obese, obese, normal, normal, normal, norma…\n\n\nNow, both variables, sex and bmi_cat, have become factors with levels."
  },
  {
    "objectID": "descriptive.html#summarizing-categorical-data-frequency-statistics",
    "href": "descriptive.html#summarizing-categorical-data-frequency-statistics",
    "title": "2  Descriptive statistics",
    "section": "\n2.3 Summarizing Categorical Data (Frequency Statistics)",
    "text": "2.3 Summarizing Categorical Data (Frequency Statistics)\nThe first step to analyze categorical data is to count the different types of labels and calculate the frequencies. The set of frequencies of all the possible categories is called the frequency distribution of the variable. Additionally, we can express the frequencies as proportions of the total sample size (relative frequencies, %).\nWe can generate a frequency table for the sex variable using the freq() function from the {questionr} package:\n\nfreq(arrhythmia$sex, cum = T, total = T, valid = F)\n\n         n     %  %cum\nfemale 237  55.4  55.4\nmale   191  44.6 100.0\nTotal  428 100.0 100.0\n\n\nThe first column shows the levels of sex variable (male, female), the second (n) shows the number of patients in each category (absolute frequency), the third (%) the percentage contribution of each category to the total (relative frequency), and the fourth (%cum) the commutative percentage. Of note, the percentages add up to 100%.\nSimilarly, we can create the frequency table for the bmi_cat variable:\n\nfreq(arrhythmia$bmi_cat, cum = T, total = T, valid = F)\n\n              n     %  %cum\nunderweight  11   2.6   2.6\nnormal      192  44.9  47.4\noverweight  167  39.0  86.4\nobese        58  13.6 100.0\nTotal       428 100.0 100.0\n\n\n \nWe can also sort the BMI categories in a decreasing order of frequencies:\n\nfreq(arrhythmia$bmi_cat, cum = T, total = T, valid = F, sort = \"dec\")\n\n              n     %  %cum\nnormal      192  44.9  44.9\noverweight  167  39.0  83.9\nobese        58  13.6  97.4\nunderweight  11   2.6 100.0\nTotal       428 100.0 100.0\n\n\nIn the above table we observe that a large proportion of patients are overweight (167 out of 428, 39.0%).\n \nIn addition to tabulating each variable separately, we might be interested in whether the distribution of patients across each sex is different for each BMI category.\n\ntab &lt;- table(arrhythmia$sex, arrhythmia$bmi_cat)\nrprop(tab, percent = T, total = F, n = T)\n\n        \n         underweight normal overweight obese  n  \n  female   3.0%       48.5%  32.1%      16.5% 237\n  male     2.1%       40.3%  47.6%       9.9% 191\n\n\nWe can see that the percentage of overweight male patients (47.6%) is higher than overweight female patients (32.1%). In contrast, the percentage of obese male patients (9.9%) is lower than obese female patients (16.5%)."
  },
  {
    "objectID": "descriptive.html#displaying-categorical-data",
    "href": "descriptive.html#displaying-categorical-data",
    "title": "2  Descriptive statistics",
    "section": "\n2.4 Displaying Categorical Data",
    "text": "2.4 Displaying Categorical Data\nWhile frequency tables are extremely useful, the best way to investigate a dataset is to plot it. For categorical variables, such as sex and bmi_cat, it is straightforward to present the number in each category, usually indicating the frequency and percentage of the total number of patients. When shown graphically this is called a bar plot.\nA. Simple Bar Plot\nA simple bar plot is an easy way to make comparisons across categories. ?fig-simplebar shows the BMI categories for 428 patients. Along the horizontal axis (x-axis) are the different BMI categories whilst on the vertical axis (y-axis) is the percentage (%). The height of each bar represents the percentage of the total patients in that category. For example, it can be seen that the percentage of overweight participants is 39% (167/428).\n\n# create a data frame with ordered BMI categories and their counts\ndat1 &lt;- arrhythmia %&gt;%\n  count(bmi_cat) %&gt;% \n  mutate(pct = round_percent(n, 1))\n\n# plot the data\nggplot(dat1, aes(x = bmi_cat, y = pct)) +\n  geom_col(width=0.65, fill = \"steelblue4\") +\n  geom_text(aes(label=paste0(pct, \"%\")),\n            vjust=1.6, color = \"white\", size = 3) +\n  labs(x = \"BMI category\", y = \"Percent\",\n       caption = \"Number of patients: 428\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  theme_minimal(base_size = 12)\n\n\n\nBar plot showing the BMI category distribution for 428 patients.\n\n\n\n\n\n\n\n\n\n\nBasic Properties of a Simple Bar plot\n\n\n\n\nAll bars should have equal width and should have equal space between them.\nThe height of bar is equivalent to the data they represent.\nThe bars must be plotted against a common zero-valued baseline.\n\n\n\n \nB. Side-by-side and Grouped Bar Plots\nIf the sample is further classified into whether the patient was male or female then it becomes impossible to present the data as a single bar plot. We could present the data as a side by side bar plot (?fig-sidebar1) but is preferable to present the data in one graph with the same scales and axes to make the visual comparisons easier (grouped bar plot) (?fig-sidebar2).\n\n# create a data frame with ordered BMI categories and their counts by sex\ndat2 &lt;- arrhythmia %&gt;%\n  count(bmi_cat, sex) %&gt;% \n  group_by(sex) %&gt;% \n  mutate(pct = round_percent(n, 1)) %&gt;% \n  ungroup()\n\nggplot(dat2) + \n  geom_col(aes(bmi_cat, pct, fill = sex), width=0.7, position = \"dodge\") +\n  geom_text(aes(bmi_cat, pct, label = paste0(pct, \"%\"), \n                group = sex), color = \"white\", size = 3,vjust=1.2,\n            position = position_dodge(width = .9)) +\n  labs(x = \"BMI category\", y = \"Percent\",\n       caption = \"female: n=237, male: n=191\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  scale_fill_jco() +\n  theme_minimal(base_size = 12) +\n  theme(legend.position=\"none\",\n        axis.text.x = element_text(angle = 45, hjust = 1)) +\n  facet_wrap(~sex, ncol = 2)\n\n\n\nSide-by-side bar plot showing by BMI category and sex.\n\n\n\n\n\nggplot(dat2) + \n  geom_col(aes(bmi_cat, pct, fill = sex), width = 0.8, position = \"dodge\") +\n  geom_text(aes(bmi_cat, pct, label = paste0(pct, \"%\"), \n                group = sex), color = \"white\", size = 3,vjust=1.2,\n            position = position_dodge(width = .9)) +\n  labs(x = \"BMI category\", y = \"Percent\",\n       caption = \"female: n=237, male: n=191\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  scale_fill_jco() +\n  theme_minimal(base_size = 12)\n\n\n\nGrouped bar plot showing 428 patients by BMI category and sex.\n\n\n\n\n \nC. Stacked Bar Plot\nUnlike a side-by-side or grouped graphs, stacked bar plots segment their bars. A 100% Stack Bar Plot shows the percentage-of-the-whole of each group. This makes it easier to see if relative differences exist between quantities in each group (?fig-stacked).\n\n# create a data frame with ordered BMI categories and their counts by sex\ndat3 &lt;- arrhythmia %&gt;% \n  group_by(sex) %&gt;% \n  count(bmi_cat) %&gt;% \n  mutate(pct = round_percent(n, 2)) %&gt;% \n  ungroup()\n\nggplot(dat3, aes(x = sex, y = pct, fill = forcats::fct_rev(bmi_cat)))+\n  geom_bar(stat = \"identity\", width = 0.8)+\n  geom_text(aes(label = paste0(round(pct, 1), \"%\"), y = pct), \n            position = position_stack(vjust = 0.5)) +\n  coord_flip()+\n  scale_fill_simpsons() +\n  scale_y_continuous(labels = scales::percent_format(scale = 1))+\n  labs(x = \"Sex\", y = \"Percent\", fill = \"BMI category\") +\n  theme_minimal(base_size = 12)\n\n\n\nA horizontal 100% stacked bar plot showing the distribution of BMI by sex.\n\n\n\n\n\n\n\n\n\n\nStacked bar plots tend to become confusing when the variable has many levels\n\n\n\nOne issue to consider when using stacked bar plots is the number of variable levels: when dealing with many categories, stacked bar plots tend to become rather confusing."
  },
  {
    "objectID": "descriptive.html#summarizing-numerical-data",
    "href": "descriptive.html#summarizing-numerical-data",
    "title": "2  Descriptive statistics",
    "section": "\n2.5 Summarizing Numerical Data",
    "text": "2.5 Summarizing Numerical Data\nSummary measures are single numerical values that summarize a large number of values. Numeric data are described with two main types of summary measures (Table 2.1):\n\nmeasures of central location (where the center of the distribution of the values in a variable is located)\nmeasures of dispersion (how widely the values are spread above and below the central value)\n\n\n\nTable 2.1: Common summary measures of central location and dispersion\n\n\n\n\n\nMeasures of central location\nMeasures of dispersion\n\n\n\nmean\nmedian\nmode\n\n\nvariance\nstandard deviation\nrange (minimum, maximum)\ninterquartile range (1st and 3rd quartiles)\n\n\n\n\n \nA. Summary statistics\nLet’s calculate the summary statistics for the age variable in our dataset.\n\n\n\n\n\n\nSummary statistics: Variable age\n\n\n\n\n\ndplyr\ndlookr\npsych\n\n\n\n\narrhythmia %&gt;%\n  dplyr::summarise(\n    n = n(),\n    count_na = sum(is.na(age)),\n    min = min(age, na.rm = TRUE),\n    q1 = quantile(age, 0.25, na.rm = TRUE),\n    median = quantile(age, 0.5, na.rm = TRUE),\n    q3 = quantile(age, 0.75, na.rm = TRUE),\n    max = max(age, na.rm = TRUE),\n    mean = mean(age, na.rm = TRUE),\n    sd = sd(age, na.rm = TRUE),\n    skewness = EnvStats::skewness(age, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(age, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 11\n      n count_na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1   428        3    18    37     47    59    83  48.4  14.3   0.0806   -0.671\n\n\n\n\n\narrhythmia %&gt;% \n  dlookr::describe(age) %&gt;% \n  select(described_variables, n, na, mean, sd, p25, p50, p75, skewness, kurtosis)\n\n# A tibble: 1 × 10\n  described_variables     n    na  mean    sd   p25   p50   p75 skewness kurto…¹\n  &lt;chr&gt;               &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 age                   425     3  48.4  14.3    37    47    59   0.0806  -0.671\n# … with abbreviated variable name ¹​kurtosis\n\n\n\n\n\npsych::describe(arrhythmia$age)\n\n   vars   n  mean    sd median trimmed   mad min max range skew kurtosis   se\nX1    1 425 48.44 14.27     47   48.32 16.31  18  83    65 0.08    -0.69 0.69\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary statistics: Variable QRS\n\n\n\n\n\ndplyr\ndlookr\npsych\n\n\n\n\narrhythmia %&gt;%\n  dplyr::summarise(\n    n = n(),\n    count_na = sum(is.na(QRS)),\n    min = min(QRS, na.rm = TRUE),\n    q1 = quantile(QRS, 0.25, na.rm = TRUE),\n    median = quantile(QRS, 0.5, na.rm = TRUE),\n    q3 = quantile(QRS, 0.75, na.rm = TRUE),\n    max = max(QRS, na.rm = TRUE),\n    mean = mean(QRS, na.rm = TRUE),\n    sd = sd(QRS, na.rm = TRUE),\n    skewness = EnvStats::skewness(QRS, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(QRS, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 11\n      n count_na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1   428        0    55    80     86    94   188  88.4  14.4     2.47     11.1\n\n\n\n\n\narrhythmia %&gt;% \n  dlookr::describe(QRS) %&gt;% \n  select(described_variables, n, na, mean, sd, p25, p50, p75, skewness, kurtosis)\n\n# A tibble: 1 × 10\n  described_variables     n    na  mean    sd   p25   p50   p75 skewness kurto…¹\n  &lt;chr&gt;               &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 QRS                   428     0  88.4  14.4    80    86    94     2.47    11.1\n# … with abbreviated variable name ¹​kurtosis\n\n\n\n\n\npsych::describe(arrhythmia$QRS)\n\n   vars   n mean    sd median trimmed   mad min max range skew kurtosis  se\nX1    1 428 88.4 14.45     86   86.81 10.38  55 188   133 2.45    10.87 0.7\n\n\n\n\n\n\n\n \nB. Summary statistics by group\nNext, we are interested in calculating the summary statistics of the age variable for males and females, separately.\n\n\n\n\n\n\nSummary statistics: age stratified by sex\n\n\n\n\n\ndplyr\ndlookr\npsych\n\n\n\n\nsummary_age_sex &lt;- arrhythmia %&gt;%\n  group_by(sex) %&gt;% \n  dplyr::summarise(\n    n = n(),\n    count_na = sum(is.na(age)),\n    min = min(age, na.rm = TRUE),\n    q1 = quantile(age, 0.25, na.rm = TRUE),\n    median = quantile(age, 0.5, na.rm = TRUE),\n    q3 = quantile(age, 0.75, na.rm = TRUE),\n    max = max(age, na.rm = TRUE),\n    mean = mean(age, na.rm = TRUE),\n    sd = sd(age, na.rm = TRUE),\n    skewness = EnvStats::skewness(age, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(age, na.rm = TRUE)\n  ) %&gt;% \n  ungroup()\n\nsummary_age_sex\n\n# A tibble: 2 × 12\n  sex       n count…¹   min    q1 median    q3   max  mean    sd skewn…² kurto…³\n  &lt;fct&gt; &lt;int&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 fema…   237       2    19    35   47    58.5    83  47.3  14.7  0.203   -0.797\n2 male    191       1    18    41   48.5  59      80  49.9  13.6 -0.0534  -0.374\n# … with abbreviated variable names ¹​count_na, ²​skewness, ³​kurtosis\n\n\n\n\n\narrhythmia %&gt;% \n  group_by(sex) %&gt;% \n  dlookr::describe(age) %&gt;% \n  select(described_variables, sex, n, na, mean, sd, p25, p50, p75, skewness, kurtosis)\n\n# A tibble: 2 × 11\n  described_va…¹ sex       n    na  mean    sd   p25   p50   p75 skewn…² kurto…³\n  &lt;chr&gt;          &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 age            fema…   235     2  47.3  14.7    35  47    58.5  0.203   -0.797\n2 age            male    190     1  49.9  13.6    41  48.5  59   -0.0534  -0.374\n# … with abbreviated variable names ¹​described_variables, ²​skewness, ³​kurtosis\n\n\n\n\n\npsych::describeBy(arrhythmia$age, group = arrhythmia$sex)\n\n\n Descriptive statistics by group \ngroup: female\n   vars   n  mean    sd median trimmed   mad min max range skew kurtosis   se\nX1    1 235 47.28 14.73     47   46.95 17.79  19  83    64  0.2    -0.82 0.96\n------------------------------------------------------------ \ngroup: male\n   vars   n  mean    sd median trimmed   mad min max range  skew kurtosis   se\nX1    1 190 49.86 13.58   48.5   49.95 13.34  18  80    62 -0.05    -0.42 0.98\n\n\n\n\n\n\n\nIf we want to save our descriptive statistics, calculated in R, we can use the write_xlsx() function from {writexl} package. In the example below, we are saving the summary_age_sex table to a .xlsx file in the data folder of our RStudio Project:\n\nlibrary(writexl)\nwrite_xlsx(summary_age_sex, here(\"data\", \"summary_age_sex.xlsx\"))\n\n \n\n\n\n\n\n\nSummary statistics: QRS stratified by sex\n\n\n\n\n\ndplyr\ndlookr\npsych\n\n\n\n\narrhythmia %&gt;%\n  group_by(sex) %&gt;% \n  dplyr::summarise(\n    n = n(),\n    count_na = sum(is.na(QRS)),\n    min = min(QRS, na.rm = TRUE),\n    q1 = quantile(QRS, 0.25, na.rm = TRUE),\n    median = quantile(QRS, 0.5, na.rm = TRUE),\n    q3 = quantile(QRS, 0.75, na.rm = TRUE),\n    max = max(QRS, na.rm = TRUE),\n    mean = mean(QRS, na.rm = TRUE),\n    sd = sd(QRS, na.rm = TRUE),\n    skewness = EnvStats::skewness(QRS, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(QRS, na.rm = TRUE)\n  ) %&gt;% \n  ungroup()\n\n# A tibble: 2 × 12\n  sex       n count…¹   min    q1 median    q3   max  mean    sd skewn…² kurto…³\n  &lt;fct&gt; &lt;int&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 fema…   237       0    55    77     82    87   163  83.5  12.8    3.00    14.3\n2 male    191       0    71    87     92    99   188  94.4  14.1    2.95    14.5\n# … with abbreviated variable names ¹​count_na, ²​skewness, ³​kurtosis\n\n\n\n\n\narrhythmia %&gt;% \n  group_by(sex) %&gt;% \n  dlookr::describe(QRS) %&gt;% \n  select(described_variables, sex, n, na, mean, sd, p25, p50, p75, skewness, kurtosis)\n\n# A tibble: 2 × 11\n  described_va…¹ sex       n    na  mean    sd   p25   p50   p75 skewn…² kurto…³\n  &lt;chr&gt;          &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 QRS            fema…   237     0  83.5  12.8    77    82    87    3.00    14.3\n2 QRS            male    191     0  94.4  14.1    87    92    99    2.95    14.5\n# … with abbreviated variable names ¹​described_variables, ²​skewness, ³​kurtosis\n\n\n\n\n\npsych::describeBy(arrhythmia$QRS, group = arrhythmia$sex)\n\n\n Descriptive statistics by group \ngroup: female\n   vars   n  mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 237 83.53 12.8     82   82.09 7.41  55 163   108 2.96    13.83 0.83\n------------------------------------------------------------ \ngroup: male\n   vars   n  mean    sd median trimmed mad min max range skew kurtosis   se\nX1    1 191 94.44 14.12     92   92.72 8.9  71 188   117  2.9    13.89 1.02\n\n\n\n\n\n\n\n\n\n\n\n\n\nReporting summary statistics for numerical data\n\n\n\nA. Mean (sd) for data with symmetric distribution. A distribution, or dataset, is symmetric if its left and right sides are mirror images.\nB. Median (Q1, Q3) for data with skewed (or asymmetrical) distribution."
  },
  {
    "objectID": "descriptive.html#displaying-numerical-data",
    "href": "descriptive.html#displaying-numerical-data",
    "title": "2  Descriptive statistics",
    "section": "\n2.6 Displaying Numerical Data",
    "text": "2.6 Displaying Numerical Data\nA. Histogram\nThe most common way of presenting a frequency distribution of a continuous variable is a histogram. Histograms (Figure 2.2) depict the distribution of the data as a series of bars without space between them. Each bar typically covers a range of numeric values called a bin or class; a bar’s height indicates the frequency of observations with a value within the corresponding bin.\n\n# Histogram of age\narrhythmia %&gt;% \n  ggplot(aes(x = age)) +\n  geom_histogram(binwidth = 8, fill = \"steelblue4\", color = \"#8fb4d9\", alpha = 0.6) +\n  theme_minimal(base_size = 16) +\n  labs(title = \"Histogram: age\", y = \"Frequency\")\n\n# Histogram of QRS\narrhythmia %&gt;% \n  ggplot(aes(x = QRS)) +\n  geom_histogram(binwidth = 8, fill = \"steelblue4\", color = \"#8fb4d9\", alpha = 0.6) +\n  theme_minimal(base_size = 16) +\n  labs(title = \"Histogram: QRS\", y = \"Frequency\")\n\n\n\n\nHistogram of age for the 425 patients.\n\n\n\n\nHistogram of QRS for the 428 patients.\n\n\n\nFigure 2.2: Distributions of age and QRS variables.\n\n\n\nA histogram gives information about:\n\nHow the data are distributed (symmetrical or asymmetrical) and if there are any outliers.\nWhere the peak (or peaks) of the distribution is.\nThe amount of variability in the data.\n\n\n# density plot of age\narrhythmia %&gt;% \n  ggplot(aes(x = age)) +\n  geom_density(fill=\"steelblue4\", color=\"#8fb4d9\", adjust = 1.5, alpha=0.6) +\n  theme_minimal(base_size = 16) +\n  labs(title = \"Density Plot: age\", y = \"Density\")\n\n# density plot of QRS\narrhythmia %&gt;% \n  ggplot(aes(x = QRS)) +\n  geom_density(fill=\"steelblue4\", color=\"#8fb4d9\", adjust = 1.5, alpha=0.6) +\n  theme_minimal(base_size = 16) +\n  labs(title = \"Density Plot: QRS\", y = \"Density\")\n\n\n\n\nDensity plot of age for the 425 patients.\n\n\n\n\nDensity plot of QRS for the 428 patients.\n\n\n\nFigure 2.3: Density plot of age and QRS variables.\n\n\n\n \nB. Box Plot\nBox plots can be used for displaying location and dispersion for continuous data, particularly when comparing distributions between many groups (Figure 2.4). This type of graph uses boxes and lines to depict the distributions. Box limits indicate the range of the central 50% of the data, with a horizontal line in the box corresponding to the median. Whiskers extend from each box to capture the range of the remaining data, with dots located outside the whiskers indicating outliers.11 An outlier is an observation that is distant from the rest of the data.\n\n# box plot of age stratified by sex\narrhythmia %&gt;% \n  ggplot(aes(x = sex, y = age, fill = sex)) +\n  geom_boxplot(alpha = 0.6, width = 0.5) +\n  theme_minimal(base_size = 16) +\n  labs(title = \"Grouped Box Plot: age by sex\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n# box plot of QRS stratified by sex\narrhythmia %&gt;% \n  ggplot(aes(x = sex, y = QRS, fill = sex)) +\n  geom_boxplot(alpha = 0.6, width = 0.5) +\n  theme_minimal(base_size = 16) +\n  labs(title = \"Grouped Box Plot: QRS by sex\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n\n\n\nBox plot of age stratified by sex (female: 235; male = 190).\n\n\n\n\nBox plot of QRS stratified by sex (female: 237; male = 191).\n\n\n\nFigure 2.4: Box plots of age and QRS variables stratified by sex.\n\n\n\n \nC. Raincloud Plot\nThere are many variations of the box plot. For example, there is a way to combine raw data (dots), probability density, and key summary statistics such as median, and relevant intervals of a range of likely values for the population parameter, in an appealing and flexible format with minimal redundancy, using the raincloud plot (Figure 2.5):\n\n# raincloud plot of age stratified by sex\narrhythmia %&gt;% \n  ggplot(aes(x = sex, y = age, fill = sex)) +\n  stat_slab(aes(thickness = stat(pdf*n)), \n                scale = 0.5) +\n  stat_dotsinterval(side = \"bottom\", \n                    scale = 0.5, \n                    slab_size = 0.2) +\n  theme_minimal(base_size = 16) +\n  labs(title = \"Grouped Raincloud Plots: age by sex\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n# raincloud plot of QRS stratified by sex\narrhythmia %&gt;% \n  ggplot(aes(x = sex, y = QRS, fill = sex)) +\n  stat_slab(aes(thickness = stat(pdf*n)), \n                scale = 0.5) +\n  stat_dotsinterval(side = \"bottom\", \n                    scale = 0.5, \n                    slab_size = 0.2) +\n  theme_minimal(base_size = 16) +\n  labs(title = \"Grouped Raincloud Plots: QRS by sex\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n\n\n\nRaincloud of age stratified by sex (female: 235; male = 190).\n\n\n\n\nRaincloud of QRS stratified by sex (female: 237; male = 191).\n\n\n\nFigure 2.5: A raincloud plot of age and QRS variables stratified by sex."
  },
  {
    "objectID": "normal.html#packages-we-need",
    "href": "normal.html#packages-we-need",
    "title": "3  Normal distribution",
    "section": "3.1 Packages we need",
    "text": "3.1 Packages we need\nWe need to load the following packages:\n\nlibrary(stevemisc)\n\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "normal.html#the-shape-of-a-normal-distribution",
    "href": "normal.html#the-shape-of-a-normal-distribution",
    "title": "3  Normal distribution",
    "section": "3.2 The shape of a normal distribution",
    "text": "3.2 The shape of a normal distribution\nA normal distribution is a symmetric “bell-shaped” probability distribution where most of the observed data are clustered around a central location. Data farther from the central location occur less frequently (?fig-normal).\n\n1x &lt;- seq(-4, 4, length=200)\ndf &lt;- data.frame(x)\n\nggplot(df, aes(x)) +                                                           \n  stat_function(fun = dnorm) +                                                 \n2  scale_x_continuous(breaks = c(-3, -2, -1, 0, 1, 2, 3),\n                     labels = expression(-3*sigma, -2*sigma, -1*sigma,\n                                         mu, 1*sigma, 2*sigma, 3*sigma)) +\n  labs(x = \"Variable\",\n       y = \"Probability density\") +\n  theme(text = element_text(size = 20))                                    \n\n\n1\n\nCreate a sequence of 200 numbers between -4 and +4.\n\n2\n\nAdd the standard deviation notation in x-axis.\n\n\n\n\n\n\n\nThe area underneath a Normal Distribution"
  },
  {
    "objectID": "normal.html#the-properties-of-a-normal-distribution",
    "href": "normal.html#the-properties-of-a-normal-distribution",
    "title": "3  Normal distribution",
    "section": "3.3 The properties of a normal distribution",
    "text": "3.3 The properties of a normal distribution\n\nnormal_dist(\"#522d80\",\"#00868B\") + \n  labs(y = \"Probability density\", \n       x = \"Variable\") +\n  scale_x_continuous(breaks = c(-3, -2.58, -1.96, -1, \n                              0, 1, 1.96, 2.58, 3),\n                   labels = expression(-3*sigma, -2.58*sigma, -1.96*sigma, -1*sigma, \n                                       mu, 1*sigma, 1.96*sigma, 2.58*sigma, 3*sigma)) +\n  theme(text = element_text(size = 20), \n        axis.text.x = element_text(size = 12))\n\n\n\n\nThe area underneath a Normal Distribution\n\n\n\n\nThe Normal distribution has the properties summarized as follows:\n\nBell shaped and symmetrical around the mean. Shape statistics, skewness and excess kurtosis are zero.\nThe peak of the curve lies above the mean.\nAny position along the horizontal axis (x-axis) can be expressed as a number of standard deviations from the mean.\nAll three measures of central location mean, median, and mode are the same.\nThe empirical rule (also called the “68-95-99 rule”). Much of the area (68%) of the distribution is between -1 \\(\\sigma\\) below the mean and +1 \\(\\sigma\\) above the mean, the large majority (95%) between -1.96 \\(\\sigma\\) below the mean and +1.96 \\(\\sigma\\) above the mean (often used as a reference range), and almost all (99%) between -2.58 \\(\\sigma\\) below the mean and +2.58 \\(\\sigma\\) above the mean. The total area under the curve equals to 1 (or 100%), almost -3 \\(\\sigma\\) below the mean and +3 \\(\\sigma\\) above the mean.\n\n \n\nShape statistics and normality\nThere are two shape statistics that can indicate deviation from normality: skewness and kurtosis.\nA. Skewness\nSkewness is usually described as a measure of a distribution’s symmetry – or lack of symmetry. Skewness values that are negative indicate a tail to the left (Figure 3.1 a), zero value indicate a symmetric distribution (Figure 3.1 b), while values that are positive indicate a tail to the right (Figure 3.1 c).\nSkewness values between −1 and +1 indicate an approximate bell-shaped curve. Values from −1 to −3 or from +1 to +3 indicate that the distribution is tending away from a bell shape with &gt;1 indicating moderate skewness and &gt;2 indicating severe skewness. Any values above +3 or below−3 are a good indication that the variable is not normally distributed.\n\n\nShow the code\n# left skewed distribution\ncurve(dbeta(x, 7, 2), col=\"green\", type=\"l\", lwd = 2, xlab=\"x\", ylab=\"Probability density\", bty=\"n\")\nsegments(x0 = 0.7, x1 = 0.7, y0 = 0, y1 = 1.75, lty = 2, lwd = 2, col = \"black\") \ntext(0.7, 1.7, expression(\"mean\"), font = 2, cex = 1.5, col = \"black\")\nsegments(x0 = 0.75, x1 = 0.75, y0 = 0, y1 = 2.35, lty = 2, lwd = 2, col = \"blue\") \ntext(0.73, 2.5, expression(\"median\"), font=2, cex = 1.5, col = \"blue\")\nsegments(x0 = 0.85, x1 = 0.85, y0 = 0, y1 = 3.1, lty = 2, lwd = 2, col = \"orange\") \ntext(0.88, 3.15, expression(\"mode\"), font =2, cex = 1.5, col = \"orange\")\n\n# symmetric distribution\ncurve(dbeta(x, 7, 7), col=\"green\", type=\"l\", lwd = 2, xlab=\"x\", ylab=\"Probability density\", bty=\"n\")\nsegments(x0 = 0.48, x1 = 0.48, y0 = 0, y1 = 2.9, lty = 2, lwd = 2, col = \"orange\") \ntext(0.4, 2.8, expression(\"mode\"), font =2, cex = 1.5, col = \"orange\")\nsegments(x0 = 0.5, x1 = 0.5, y0 = 0, y1 = 3.0, lty = 2, lwd = 2, col = \"black\") \ntext(0.5, 2.95, expression(\"mean\"), font = 2, cex = 1.5, col = \"black\")\nsegments(x0 = 0.52, x1 = 0.52, y0 = 0, y1 = 2.9, lty = 2, lwd = 2, col = \"blue\") \ntext(0.63, 2.8, expression(\"median\"), font=2, cex = 1.5, col = \"blue\")\n       \n# right skewed distribution\ncurve(dbeta(x, 2, 7), col=\"green\", type=\"l\",lwd = 2, xlab=\"x\", ylab=\"Probability density\", bty=\"n\")\nsegments(x0 = 0.15, x1 = 0.15, y0 = 0, y1 = 3.1, lty = 2, lwd = 2, col = \"orange\") \ntext(0.22, 3.1, expression(\"mode\"), font =2, cex = 1.5, col = \"orange\")\nsegments(x0 = 0.25, x1 = 0.25, y0 = 0, y1 = 2.5, lty = 2, lwd = 2, col = \"blue\") \ntext(0.25, 2.6, expression(\"median\"), font = 2, cex = 1.5, col = \"blue\")\nsegments(x0 = 0.3, x1 = 0.3, y0 = 0, y1 = 1.9, lty = 2, lwd = 2, col = \"black\") \ntext(0.34, 2.0, expression(\"mean\"), font=2, cex = 1.5, col = \"black\")\n\n\n\n\n\n\n\nLeft skewed distribution (negatively skewed). The mean and the meadian are too left to the mode.\n\n\n\n\n\nSymmetric distribution (zero skewness). The mean, median and mode are the same.\n\n\n\n\n\nRight skewed distribution (positively skewed). The mean and median are to the right of the mode.\n\n\n\nFigure 3.1: Types of distribution according to the summetry.\n\n\n\nB. Kurtosis\nThe other way that distributions can deviate from normality is kurtosis. The excess kurtosis parameter is a measure of the combined weight of the tails relative to the rest of the distribution. Kurtosis is associated indirect with the peak of the distribution (if the peak of the distribution is too high or too low compared to a “normal” distribution).\nDistributions with negative excess kurtosis are called platykurtic (Figure 3.2 a). If the measure of excess kurtosis is 0 the distribution is mesokurtic (Figure 3.2 b). Finally, distributions with positive excess kurtosis are called leptokurtic (Figure 3.2 c).\nA kurtosis value between −1 and +1 indicates normality and a value between −1 and −3 or between +1 and +3 indicates a tendency away from normality. Values below −3 or above +3 strongly indicate non-normality.\n\n\nShow the code\nx &lt;- seq(-6, 6, length=200)\ny1 &lt;- dnorm(x)\ny2 &lt;- dnorm(x, sd= 2)\ny3 &lt;- dnorm(x, sd= 0.5)\n\n# platykurtic distribution\nplot(x, y1, main = \"Platykurtic distribution\", type = \"l\", lty = 3, lwd = 2,  xlab = \"\", ylab = \"Probability Density\", col=\"grey10\", xaxt='n', bty=\"n\")\naxis(1, at = -4:4, lab=expression(-4*sigma, -3*sigma, -2*sigma,-1*sigma, mu, 1*sigma, 2*sigma, 3*sigma, 4*sigma))\nlines(x,y2,col=\"deeppink\", lwd = 3)\ntext(1.5, 0.3, expression(normal))\ntext(3.5, 0.10, expression(platykurtic), col=\"deeppink\")\n\n# mesokurtic distribution\nplot(x, y1, main = \"Normal distribution\", type = \"l\", lwd = 2,  xlab = \"\", ylab = \"Probability Density\", col=\"deeppink\", xaxt='n', bty=\"n\")\naxis(1, at = -4:4, lab=expression(-4*sigma, -3*sigma, -2*sigma,-1*sigma, mu, 1*sigma, 2*sigma, 3*sigma, 4*sigma))\n\n# leptokurtic distribution\nplot(x, y1, main = \"Leptokurtic distribution\", type = \"l\", lty = 3, lwd = 2,  xlab = \"\", ylab = \"Probability Density\", col=\"grey10\", xaxt='n', ylim=c(0,0.8), bty=\"n\")\naxis(1, at = -4:4, lab=expression(-4*sigma, -3*sigma, -2*sigma,-1*sigma, mu, 1*sigma, 2*sigma, 3*sigma, 4*sigma))\nlines(x,y3,col=\"deeppink\", lwd = 3)\ntext(1.7, 0.25, expression(normal))\ntext(1.5, 0.7, expression(leptokurtic), col=\"deeppink\")\n\n\n\n\n\n\n\nPlatykurtic distribution (negative excess kurtosis).\n\n\n\n\n\nMesokurtic distribution (zero excess kurtosis).\n\n\n\n\n\nLeptokurtic distribution (positive excess kurtosis).\n\n\n\nFigure 3.2: Types of distribution according to the summetry."
  },
  {
    "objectID": "inference.html#hypothesis-testsing",
    "href": "inference.html#hypothesis-testsing",
    "title": "4  Foundations for statistical inference",
    "section": "\n4.1 Hypothesis Testsing",
    "text": "4.1 Hypothesis Testsing\nHypothesis testing is a method of deciding whether the data are consistent with the null hypothesis. The calculation of the p-value is an important part of the procedure. Given a study with a single outcome measure and a statistical test, hypothesis testing can be summarized in five steps.\n\n\n\n\n\n\nSteps of Hypothesis Testsing\n\n\n\nStep 1: State the null hypothesis, \\(H_{0}\\), and alternative hypothesis, \\(H_{1}\\), based on the research question.\n\nNOTE: We decide a non-directional \\(H_{1}\\) (also known as two-sided hypothesis) whether we test for effects in both directions (most common), otherwise a directional (also known as one-sided) hypothesis.\n\nStep 2: Set the level of significance, α (usually 0.05).\nStep 3: Identify the appropriate test statistic and check the assumptions. Calculate the test statistic using the data.\n\nNOTE: There are two basic types of statistical tests and they are described as parametric and non-parametric. The parametric tests (e.g., t-test, ANOVA), make certain assumptions about the distribution of the unknown parameter of interest and thus the test statistic is valid under these assumptions. For non-parametric tests (e.g., Mann-Whitney U test, Kruskal-Wallis test), there are no such assumptions. Most nonparametric tests use some way of ranking the measurements. Non-parametric tests are about 95% as powerful as parametric tests.\n\nStep 4: Decide whether or not the result is statistically significant.\n\nThe p-value is the probability of obtaining the observed results, or something more extreme, if the null hypothesis is true.\n\nUsing the known distribution of the test statistic and according to the result of our statistical test, we calculate the corresponding p-value. Then we compare the p-value to the significance level α:\n\nIf p − value &lt; α, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ α, do not reject the null hypothesis, \\(H_{0}\\).\n\nThe ?tbl-p demonstrates how to interpret the strength of the evidence. However, always keep in mind the size of the study being considered.\n\nStrength of the evidence against \\(H_{0}\\). {#tbl-p}\n\np-value\nInterpretation\n\n\n\n\\(p \\geq{0.10}\\)\nNo evidence to reject \\(H_{0}\\)\n\n\n\n\\(0.05\\leq p &lt; 0.10\\)\nWeak evidence to reject \\(H_{0}\\)\n\n\n\n\\(0.01\\leq p &lt; 0.05\\)\nEvidence to reject \\(H_{0}\\)\n\n\n\n\\(0.001\\leq p &lt; 0.01\\)\nStrong evidence to reject \\(H_{0}\\)\n\n\n\n\\(p &lt; 0.001\\)\nVery strong evidence to reject \\(H_{0}\\)\n\n\n\n\nStep 5: Interpret the results."
  },
  {
    "objectID": "inference.html#type-of-errors-in-hypothesis-testing",
    "href": "inference.html#type-of-errors-in-hypothesis-testing",
    "title": "4  Foundations for statistical inference",
    "section": "\n4.2 Type of Errors in hypothesis testing",
    "text": "4.2 Type of Errors in hypothesis testing\nType I error: we reject the null hypothesis when it is true (false positive), and conclude that there is an effect when, in reality, there is none. The maximum chance (probability) of making a Type I error is denoted by α (alpha). This is the significance level of the test; we reject the null hypothesis if our p-value is less than the significance level, i.e. if p &lt; a (?fig-type_errors).\nType II error: we do not reject the null hypothesis when it is false (false negative), and conclude that there is no effect when one really exists. The chance of making a Type II error is denoted by β (beta); its compliment, (1 - β), is the power of the test. The power, therefore, is the probability of rejecting the null hypothesis when it is false; i.e. it is the chance (usually expressed as a percentage) of detecting, as statistically significant, a real treatment effect of a given size (?fig-type_errors). Table 4.1 presents the main factors that can influence the power in a study.\n\n\n\n\nTypes of error in hypothesis testing.\n\n\n\n\n\n\nTable 4.1: Factors Influencing Power.\n\n\n\n\n\nFactor\nInfluence on study’s power\n\n\n\n\nEffect Size  (e.g., mean difference, risk ratio)\nAs effect size increases, power tends to increase (a larger effect size is easier to be detected by the statistical test, leading to a greater probability of a statistically significant result).\n\n\nSample Size\nAs the sample size goes up, power generally goes up (this factor is the most easily manipulated by researchers).\n\n\nStandard deviation\nAs variability decreases, power tends to increase (variability can be reduced by controlling extraneous variables such as inclusion and exclusion criteria defining the sample in a study).\n\n\nSignificance level α\nAs α goes up, power goes up (it would be easier to find statistical significance with a larger α, e.g. α=0.1, compared to a smaller α, e.g. α=0.05)."
  },
  {
    "objectID": "student_t_test.html#research-question-and-hypothesis-testing",
    "href": "student_t_test.html#research-question-and-hypothesis-testing",
    "title": "5  Two-sample t-test (Student’s t-test)",
    "section": "\n5.1 Research question and Hypothesis Testing",
    "text": "5.1 Research question and Hypothesis Testing\nWe consider the data in depression dataset. In an experiment designed to test the effectiveness of paroxetine for treating bipolar depression, the participants were randomly assigned into two groups (paroxetine Vs placebo). The researchers used the Hamilton Depression Rating Scale (HDRS) to measure the depression state of the participants and wanted to find out if the HDRS score is different in paroxetine group as compared to placebo group at the end of the experiment. The significance level \\(\\alpha\\) was set to 0.05.\nNote: A score of 0–7 in HDRS is generally accepted to be within the normal range, while a score of 20 or higher indicates at least moderate severity.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): the means of HDRS in the two groups are equal (\\(\\mu_{1} = \\mu_{2}\\))\n\n\\(H_1\\): the means of HDRS in the two groups are not equal (\\(\\mu_{1} \\neq \\mu_{2}\\))"
  },
  {
    "objectID": "student_t_test.html#packages-we-need",
    "href": "student_t_test.html#packages-we-need",
    "title": "5  Two-sample t-test (Student’s t-test)",
    "section": "\n5.2 Packages we need",
    "text": "5.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(PupillometryR)\nlibrary(gtsummary)\n\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "student_t_test.html#preraring-the-data",
    "href": "student_t_test.html#preraring-the-data",
    "title": "5  Two-sample t-test (Student’s t-test)",
    "section": "\n5.3 Preraring the data",
    "text": "5.3 Preraring the data\nWe import the data depression in R:\n\nlibrary(readxl)\ndepression &lt;- read_excel(here(\"data\", \"depression.xlsx\"))\n\n\n\n\nFigure 5.1: Table with data from “depression” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(depression)\n\nRows: 76\nColumns: 2\n$ intervention &lt;chr&gt; \"placebo\", \"placebo\", \"placebo\", \"placebo\", \"placebo\", \"p…\n$ HDRS         &lt;dbl&gt; 19, 21, 28, 22, 22, 28, 23, 17, 19, 20, 26, 23, 23, 22, 1…\n\n\nThe data set depression has 76 patients (rows) and includes two variables (columns). The numeric (&lt;dbl&gt;) HDRS variable and the character (&lt;chr&gt;) intervention variable which should be converted to a factor (&lt;fct&gt;) variable using the factor() function as follows:\n\ndepression &lt;- depression %&gt;% \n  mutate(intervention = factor(intervention))\nglimpse(depression)\n\nRows: 76\nColumns: 2\n$ intervention &lt;fct&gt; placebo, placebo, placebo, placebo, placebo, placebo, pla…\n$ HDRS         &lt;dbl&gt; 19, 21, 28, 22, 22, 28, 23, 17, 19, 20, 26, 23, 23, 22, 1…"
  },
  {
    "objectID": "student_t_test.html#assumptions",
    "href": "student_t_test.html#assumptions",
    "title": "5  Two-sample t-test (Student’s t-test)",
    "section": "\n5.4 Assumptions",
    "text": "5.4 Assumptions\n\n\n\n\n\n\nCheck if the following assumptions are satisfied\n\n\n\n\nThe data are normally distributed in both groups\nThe data in both groups have similar variance (also named as homogeneity of variance or homoscedasticity)\n\n\n\nA. Explore the characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\nGraphs\nWe can visualize the distribution of HDRS for the two groups:\n\nset.seed(123)\nggplot(depression, aes(x=intervention, y=HDRS)) + \n  geom_flat_violin(aes(fill = intervention), scale = \"count\") +\n  geom_boxplot(width = 0.14, outlier.shape = NA, alpha = 0.5) +\n  geom_point(position = position_jitter(width = 0.05), \n             size = 1.2, alpha = 0.6) +\n  ggsci::scale_fill_jco() +\n  theme_classic(base_size = 14) +\n  theme(legend.position=\"none\", \n        axis.text = element_text(size = 14))\n\n\n\nRain cloud plot.\n\n\n\n\nThe above figure shows that the data are close to symmetry and the assumption of a normal distribution is reasonable.\n \nSummary statistics\nThe HDRS summary statistics for each group are:\n\n\n\n\n\n\nSummary statistics by group\n\n\n\n\n\ndplyr\ndlookr\n\n\n\n\nHDRS_summary &lt;- depression %&gt;%\n  group_by(intervention) %&gt;%\n  dplyr::summarise(\n    n = n(),\n    min = min(HDRS, na.rm = TRUE),\n    q1 = quantile(HDRS, 0.25, na.rm = TRUE),\n    median = quantile(HDRS, 0.5, na.rm = TRUE),\n    q3 = quantile(HDRS, 0.75, na.rm = TRUE),\n    max = max(HDRS, na.rm = TRUE),\n    mean = mean(HDRS, na.rm = TRUE),\n    sd = sd(HDRS, na.rm = TRUE),\n    skewness = EnvStats::skewness(HDRS, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(HDRS, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\nHDRS_summary\n\n# A tibble: 2 × 11\n  intervention     n   min    q1 median    q3   max  mean    sd skewness kurto…¹\n  &lt;fct&gt;        &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 paroxetine      33    13    18     21    22    27  20.3  3.65  0.00167  -0.574\n2 placebo         43    14    19     21    24    28  21.5  3.41  0.0276   -0.403\n# … with abbreviated variable name ¹​kurtosis\n\n\n\n\n\ndepression %&gt;% \n  group_by(intervention) %&gt;% \n  dlookr::describe(HDRS) %&gt;% \n  select(described_variables,  intervention, n, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup()\n\n# A tibble: 2 × 10\n  described_variab…¹ inter…²     n  mean    sd   p25   p50   p75 skewn…³ kurto…⁴\n  &lt;chr&gt;              &lt;fct&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 HDRS               paroxe…    33  20.3  3.65    18    21    22 0.00167  -0.574\n2 HDRS               placebo    43  21.5  3.41    19    21    24 0.0276   -0.403\n# … with abbreviated variable names ¹​described_variables, ²​intervention,\n#   ³​skewness, ⁴​kurtosis\n\n\n\n\n\n\n\nThe means are close to medians (20.3 vs 21 and 21.5 vs 21). The skewness is approximately zero (symmetric distribution) and the (excess) kurtosis falls into the acceptable range of [-1, 1] indicating approximately normal distributions for both groups.\n \nNormality test\n\n\n\n\n\n\nRemember: Hypothesis testing for Shapiro-Wilk test for normality\n\n\n\n\\(H_{0}\\): the data came from a normally distributed population.\n\\(H_{1}\\): the data tested are not normally distributed.\n\nIf p − value &lt; 0.05, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\).\n\n\n\nThe Shapiro-Wilk test for normality for each group is:\n\ndepression %&gt;%\n  group_by(intervention) %&gt;%\n  shapiro_test(HDRS) %&gt;% \n  ungroup()\n\n# A tibble: 2 × 4\n  intervention variable statistic     p\n  &lt;fct&gt;        &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 paroxetine   HDRS         0.976 0.670\n2 placebo      HDRS         0.979 0.614\n\n\nThe tests of normality suggest that the data for the HDRS in both groups are normally distributed (p=0.67 &gt;0.05 and p=0.61 &gt;0.05, respectively).\n \nB. Levene’s test for equality of variances\n\n\n\n\n\n\nRemember: Hypothesis testing for Levene’s test for equality of variances\n\n\n\n\\(H_{0}\\): the variances of HDRs in two groups are equal\n\\(H_{1}\\): the variances of HDRs in two groups are not equal\n\nIf p − value &lt; 0.05, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\).\n\n\n\nThe Levene’s test for equality of variances is:\n\ndepression %&gt;% \n  levene_test(HDRS ~ intervention)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1    74     0.176 0.676\n\n\nSince the p-value = 0.676 &gt;0.05, the null hypothesis that the variances of HDRs in two groups are equal is not rejected."
  },
  {
    "objectID": "student_t_test.html#run-the-t-test",
    "href": "student_t_test.html#run-the-t-test",
    "title": "5  Two-sample t-test (Student’s t-test)",
    "section": "\n5.5 Run the t-test",
    "text": "5.5 Run the t-test\nWe will perform a pooled variance t-test (Student’s t-test) to test the null hypothesis that the mean HDRS score is the same for both groups (paroxetine and placebo).\n\n\n\n\n\n\nStudent’s t-test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nt.test(HDRS ~ intervention, var.equal = T, data=depression)\n\n\n    Two Sample t-test\n\ndata:  HDRS by intervention\nt = -1.4185, df = 74, p-value = 0.1602\nalternative hypothesis: true difference in means between group paroxetine and group placebo is not equal to 0\n95 percent confidence interval:\n -2.777498  0.467420\nsample estimates:\nmean in group paroxetine    mean in group placebo \n                20.33333                 21.48837 \n\n\n\n\n\ndepression %&gt;% \n    t_test(HDRS ~ intervention, var.equal = T, detailed = T)\n\n# A tibble: 1 × 15\n  estimate estimate1 estim…¹ .y.   group1 group2    n1    n2 stati…²     p    df\n*    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    -1.16      20.3    21.5 HDRS  parox… place…    33    43   -1.42  0.16    74\n# … with 4 more variables: conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt;, method &lt;chr&gt;,\n#   alternative &lt;chr&gt;, and abbreviated variable names ¹​estimate2, ²​statistic\n\n\n\n\n\n\n\nThe p-value = 0.16 is greater than 0.05. There is no evidence of a significant difference in mean HDRS between the two groups (failed to reject \\(H_0\\)). The difference between means (20.33 - 21.49) equals to -1.16 units of the HDRS and note that the 95% confidence interval of the difference in means (-2.78 to 0.47) includes the hypothesized null value of 0. Based on these results, there is not evidence that paroxetine is effective as a treatment for bipolar depression.\nNote that the paroxetine group (n=33) has df (degrees of freedom) = 33-1 = 32 and the placebo sample (n= 43) has df = 43-1 = 42 , so we have df = 32 + 42 = 74 in total. Another way of thinking of this is that the complete sample size is 76, and we have estimated two parameters from the data (the two means), so we have df = 76-2 = 74 .\nThe Student t-test for two independent samples does not have any restrictions on \\(n_1\\) and \\(n_2\\) —they can be equal or unequal. However, equal samples are preferred because when a total of 2n subjects are available, their equal division among the groups maximizes the power to detect a specified difference.\n\n\n\n\n\n\nRemember\n\n\n\nIf the variance is different between the two groups then the degrees of freedom and the t-value associated with a two-sample t-test are calculated differently. In this case, we have to write var.equal = F (or write nothing because this is the default) in the function so the Welch-Satterthwaite approximation is applied to the degrees of freedom.\n\n\n \nPresent the results in a summary table\n\nShow the codedepression %&gt;% \n  tbl_summary(\n    by = intervention, \n    statistic = HDRS ~ \"{mean} ({sd})\", \n    digits = list(everything() ~ 1),\n    label = list(HDRS ~ \"HDRS score\"), \n    missing = c(\"no\")) %&gt;% \n  add_p(test = HDRS ~ \"t.test\", purrr::partial(style_pvalue, digits = 2),\n        test.args = all_tests(\"t.test\") ~ list(var.equal = TRUE)) %&gt;% \n  add_overall() %&gt;%\n  as_flex_table() \n\n\n\n\n\n\nCharacteristic\nOverall, N = 761\nparoxetine, N = 331\nplacebo, N = 431\np-value2\n\n\nHDRS score\n21.0 (3.5)\n20.3 (3.7)\n21.5 (3.4)\n0.16\n\n\n1Mean (SD)\n2Two Sample t-test\n\n\n\n\n\nHence, there is not evidence that HDRS score is significantly different in paroxetine group, mean = 20.3 (sd = 3.7), as compared to placebo group, 21.5 (3.4), (mean difference= -1.16 units, 95% CI = -2.78 to 0.47, p = 0.16 &gt;0.05)."
  },
  {
    "objectID": "wmw_test.html#research-question-and-hypothesis-testing",
    "href": "wmw_test.html#research-question-and-hypothesis-testing",
    "title": "6  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n6.1 Research question and Hypothesis Testing",
    "text": "6.1 Research question and Hypothesis Testing\nWe consider the data in thromboglobulin dataset that contains the urinary \\(\\beta\\) thromboglobulin excretion (pg/ml) measured in 12 non-diabetic patients and 12 diabetic patients. The researchers used \\(\\alpha\\) = 0.05 significance level to test if the distribution of urinary \\(\\beta\\) thromboglobulin (b_TG) differs in the two groups.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): the distribution of urinary \\(\\beta\\) thromboglobulin is the same in the two groups\n\n\\(H_1\\): the distribution of urinary \\(\\beta\\) thromboglobulin is different in the two groups\n\n\n\nNOTE: The null hypothesis is that the observations from one group do not tend to have a higher or lower ranking than observations from the other group. This test does not test the medians of the data as is commonly thought, it tests the whole distribution. In practice, however, we use the medians to present the results. Statistical speaking, if the distributions of the two groups have similar shapes, the Wilcoxon-Mann-Whitney test can be used to determine whether there are differences in the medians between the two groups."
  },
  {
    "objectID": "wmw_test.html#packages-we-need",
    "href": "wmw_test.html#packages-we-need",
    "title": "6  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n6.2 Packages we need",
    "text": "6.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(PupillometryR)\nlibrary(gtsummary)\n\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "wmw_test.html#preraring-the-data",
    "href": "wmw_test.html#preraring-the-data",
    "title": "6  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n6.3 Preraring the data",
    "text": "6.3 Preraring the data\nWe import the data thromboglobulin in R:\n\nlibrary(readxl)\ntg &lt;- read_excel(here(\"data\", \"thromboglobulin.xlsx\"))\n\n\n\n\nFigure 6.1: Table with data from “depression” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(tg)\n\nRows: 24\nColumns: 2\n$ status &lt;chr&gt; \"non-diabetic\", \"non-diabetic\", \"non-diabetic\", \"non-diabetic\",…\n$ b_TG   &lt;dbl&gt; 4.1, 6.3, 7.8, 8.5, 8.9, 10.4, 11.5, 12.0, 13.8, 17.6, 24.3, 37…\n\n\nThe data set tg has 24 patients (rows) and includes two variables (columns). The numeric (&lt;dbl&gt;) b_TG variable and the character (&lt;chr&gt;) status variable which should be converted to a factor (&lt;fct&gt;) variable using the factor() function as follows:\n\ntg &lt;- tg %&gt;% \n  mutate(status = factor(status))\nglimpse(tg)\n\nRows: 24\nColumns: 2\n$ status &lt;fct&gt; non-diabetic, non-diabetic, non-diabetic, non-diabetic, non-dia…\n$ b_TG   &lt;dbl&gt; 4.1, 6.3, 7.8, 8.5, 8.9, 10.4, 11.5, 12.0, 13.8, 17.6, 24.3, 37…"
  },
  {
    "objectID": "wmw_test.html#explore-the-characteristics-of-distribution-for-each-group-and-check-for-normality",
    "href": "wmw_test.html#explore-the-characteristics-of-distribution-for-each-group-and-check-for-normality",
    "title": "6  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n6.4 Explore the characteristics of distribution for each group and check for normality",
    "text": "6.4 Explore the characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\n \nGraph\nWe can visualize the distribution of b_TG for the two groups:\n\nset.seed(123)\nggplot(tg, aes(x=status, y=b_TG)) + \n  geom_flat_violin(aes(fill = status), scale = \"count\") +\n  geom_boxplot(width = 0.14, outlier.shape = NA, alpha = 0.5) +\n  geom_point(position = position_jitter(width = 0.05), \n             size = 1.2, alpha = 0.6) +\n  ggsci::scale_fill_jco() +\n  theme_classic(base_size = 14) +\n  theme(legend.position=\"none\", \n        axis.text = element_text(size = 14))\n\n\n\nRain cloud plot.\n\n\n\n\nThe above figure shows that the data in both groups are positively skewed and they have similar shaped distributions.\n \nSummary statistics\nThe g_TG summary statistics for each group are:\n\n\n\n\n\n\nSummary statistics by group\n\n\n\n\n\ndplyr\ndlookr\n\n\n\n\ntg_summary &lt;- tg %&gt;%\n  group_by(status) %&gt;%\n  dplyr::summarise(\n    n = n(),\n    min = min(b_TG, na.rm = TRUE),\n    q1 = quantile(b_TG, 0.25, na.rm = TRUE),\n    median = quantile(b_TG, 0.5, na.rm = TRUE),\n    q3 = quantile(b_TG, 0.75, na.rm = TRUE),\n    max = max(b_TG, na.rm = TRUE),\n    mean = mean(b_TG, na.rm = TRUE),\n    sd = sd(b_TG, na.rm = TRUE),\n    skewness = EnvStats::skewness(b_TG, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(b_TG, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\ntg_summary\n\n# A tibble: 2 × 11\n  status           n   min    q1 median    q3   max  mean    sd skewness kurto…¹\n  &lt;fct&gt;        &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 diabetic        12  23.8 27.2    29.2  34.8  46.2  31.8  7.17     1.05   0.107\n2 non-diabetic    12   4.1  8.32   11.0  14.8  37.2  13.5  9.19     1.81   3.47 \n# … with abbreviated variable name ¹​kurtosis\n\n\n\n\n\ntg %&gt;% \n  group_by(status) %&gt;% \n  dlookr::describe(b_TG) %&gt;% \n  select(described_variables,  status, n, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup()\n\n# A tibble: 2 × 10\n  described_variables status     n  mean    sd   p25   p50   p75 skewn…¹ kurto…²\n  &lt;chr&gt;               &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 b_TG                diabe…    12  31.8  7.17 27.2   29.2  34.8    1.05   0.107\n2 b_TG                non-d…    12  13.5  9.19  8.32  11.0  14.8    1.81   3.47 \n# … with abbreviated variable names ¹​skewness, ²​kurtosis\n\n\n\n\n\n\n\nThe means are not very close to the medians (31.8 vs 29.2 and 13.5 vs 11.0). Moreover, both the skewness (1.81) and the (excess) kurtosis (3.47) for the non-diabetic group falls outside of the acceptable range of [-1, 1] indicating right-skewed and leptokurtic distribution.\n \nNormality test\nThe Shapiro-Wilk test for normality for each group is:\n\ntg %&gt;%\n  group_by(status) %&gt;%\n  shapiro_test(b_TG) %&gt;% \n  ungroup()\n\n# A tibble: 2 × 4\n  status       variable statistic      p\n  &lt;fct&gt;        &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 diabetic     b_TG         0.886 0.105 \n2 non-diabetic b_TG         0.817 0.0148\n\n\nWe can see that the data for the non-diabetic group is not normally distributed (p=0.015 &lt;0.05) according to the Shapiro-Wilk test."
  },
  {
    "objectID": "wmw_test.html#run-the-wilcoxon-mann-whitney-test",
    "href": "wmw_test.html#run-the-wilcoxon-mann-whitney-test",
    "title": "6  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n6.5 Run the Wilcoxon-Mann-Whitney test",
    "text": "6.5 Run the Wilcoxon-Mann-Whitney test\nThe difference in medians between the two groups can be tested using a rank test such as Wilcoxon-Mann-Whitney (WMW):\n\n\n\n\n\n\nWilcoxon-Mann-Whitney test\n\n\n\n\n\nBase R\nexactRankTests\nrstatix\n\n\n\n\nwilcox.test(b_TG ~ status, conf.int = T, data = tg)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  b_TG by status\nW = 134, p-value = 0.0001028\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n 13.7 24.0\nsample estimates:\ndifference in location \n                 18.95 \n\n\nHistorical Note: As you can see, in R the Mann-Whitney statistic (symbolized as W) is calculated with the wilcox.test() function and it is called Wilcoxon rank-sum test. What is the reason for this? Henry Mann and Donald Whitney (1947) reported in their article that the test was first proposed by Frank Wilcoxon (1945) and they gave their version for the test. So the right would be to call this test Wilcoxon-Mann-Whitney (WMW) test.\n\n\nAlthough a small number of ties should not have a serious impact on our results, in case of ties we can use the wilcox.exact() function from the package {exactRankTests}:\n\nwilcox.exact(b_TG ~ status, conf.int = T, data = tg)\n\n\n    Exact Wilcoxon rank sum test\n\ndata:  b_TG by status\nW = 134, p-value = 0.0001028\nalternative hypothesis: true mu is not equal to 0\n95 percent confidence interval:\n 13.7 24.0\nsample estimates:\ndifference in location \n                 18.95 \n\n\n\n\n\ntg %&gt;%\n  wilcox_test(b_TG ~ status, detailed = T)\n\n# A tibble: 1 × 12\n  estim…¹ .y.   group1 group2    n1    n2 stati…²       p conf.…³ conf.…⁴ method\n*   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; \n1    19.0 b_TG  diabe… non-d…    12    12     134 1.03e-4    13.7      24 Wilco…\n# … with 1 more variable: alternative &lt;chr&gt;, and abbreviated variable names\n#   ¹​estimate, ²​statistic, ³​conf.low, ⁴​conf.high\n\n\n\n\n\n\n\nBased on the p &lt;0.001, we reject the null hypothesis, the result (difference in medians = 18.95, 95% CI: 13.7 to 24) is significant.\n \nPresent the results in a summary table\nThe median (or another percentile) can be used as a summary measure. The choice is guided by the shape of the distributions.\n\nShow the codegt_sum2 &lt;- tg %&gt;% \n  tbl_summary(\n    by = status, \n    statistic = b_TG ~ \"{median} ({p25}, {p75})\", \n    digits = list(everything() ~ 1),\n    label = list(b_TG ~ \"b_TG \\n(pg/ml)\"), \n    missing = c(\"no\")) %&gt;% \n  add_p(test = b_TG ~ \"wilcox.test\") %&gt;% \n  add_overall() %&gt;%\n  as_gt() \n\ngt_sum2\n\n\n\n\n\n\nCharacteristic\n\nOverall, N = 241\n\n\ndiabetic, N = 121\n\n\nnon-diabetic, N = 121\n\n\np-value2\n\n\n\nb_TG (pg/ml)\n24.6 (11.2, 30.2)\n29.2 (27.1, 34.8)\n10.9 (8.3, 14.8)\n&lt;0.001\n\n\n\n\n1 Median (IQR)\n\n\n\n2 Wilcoxon rank sum exact test\n\n\n\n\n\n\nHence, the urinary \\(\\beta\\) thromboglobulin excretion is significantly higher in diabetic group, median = 29.2 (IQR: 27.1, 34.8) pg/ml, as compared to non-diabetic group, 10.9 (8.3, 14.8) pg/ml, (p &lt;0.001)."
  },
  {
    "objectID": "paired_t_test.html#research-question",
    "href": "paired_t_test.html#research-question",
    "title": "7  Paired t-test",
    "section": "\n7.1 Research question",
    "text": "7.1 Research question\nThe dataset weight contains the birth and discharge weight of 25 newborns. We might ask if the mean difference of the weight in birth and in discharge equals to zero or not. If the differences between the pairs of measurements are normally distributed, a paired t-test is the most appropriate statistical test.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): the mean difference of weight equals to zero (\\(\\mu_{d} = 0\\))\n\n\\(H_1\\): the mean difference of weight does not equal to zero (\\(\\mu_{d} \\neq 0\\))"
  },
  {
    "objectID": "paired_t_test.html#packages-we-need",
    "href": "paired_t_test.html#packages-we-need",
    "title": "7  Paired t-test",
    "section": "\n7.2 Packages we need",
    "text": "7.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(gtsummary)\n\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "paired_t_test.html#preraring-the-data",
    "href": "paired_t_test.html#preraring-the-data",
    "title": "7  Paired t-test",
    "section": "\n7.3 Preraring the data",
    "text": "7.3 Preraring the data\nWe import the data weight in R:\n\nlibrary(readxl)\nweight &lt;- read_excel(here(\"data\", \"weight.xlsx\"))\n\nWe calculate the differences using the mutate() function:\n\nweight &lt;- weight %&gt;%\n  mutate(dif_weight = birth_weight - discharge_weight)\n\n\n\n\nFigure 7.1: Table with data from “weight” file.\n\n\n\nWe inspect the data:\n\nglimpse(weight) \n\nRows: 25\nColumns: 3\n$ birth_weight     &lt;dbl&gt; 3250, 2680, 2960, 3420, 3210, 2740, 3250, 3170, 2970,…\n$ discharge_weight &lt;dbl&gt; 3220, 2640, 2940, 3350, 3140, 2730, 3220, 3150, 2890,…\n$ dif_weight       &lt;dbl&gt; 30, 40, 20, 70, 70, 10, 30, 20, 80, 103, 84, 42, -15,…"
  },
  {
    "objectID": "paired_t_test.html#assumptions",
    "href": "paired_t_test.html#assumptions",
    "title": "7  Paired t-test",
    "section": "\n7.4 Assumptions",
    "text": "7.4 Assumptions\n\n\n\n\n\n\nCheck if the following assumption is satisfied\n\n\n\nThe differences are normally distributed."
  },
  {
    "objectID": "paired_t_test.html#explore-the-characteristics-of-distribution-of-differences",
    "href": "paired_t_test.html#explore-the-characteristics-of-distribution-of-differences",
    "title": "7  Paired t-test",
    "section": "\n7.5 Explore the characteristics of distribution of differences",
    "text": "7.5 Explore the characteristics of distribution of differences\nThe distribution of the differences can be explored with appropriate plots and summary statistics.\nGraph\nWe can explore the distribution of differences visually for symmetry with a density plot (a smoothed version of the histogram):\n\nweight %&gt;%\n  ggplot(aes(x = dif_weight)) +\n  geom_density(fill = \"#76B7B2\", color=\"black\", alpha = 0.2) +\n  geom_vline(aes(xintercept=mean(dif_weight)),\n             color=\"blue\", linetype=\"dashed\", size=1.4) +\n  geom_vline(aes(xintercept=median(dif_weight)),\n             color=\"red\", linetype=\"dashed\", size=1.2) +\n  labs(x = \"Weight difference\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\")\n\n\n\nDensity plot of the weight differences.\n\n\n\n\nThe above figure shows that the data are following an approximately symmetrical distribution. Note that the arethmetic mean (blue vertical dashed line) is very close to the median (red vertical dashed line) of the data.\nSummary statistics\nSummary statistics can also be calculated for the variables.\n\n\n\n\n\n\nSummary statistics\n\n\n\n\n\ndplyr\ndlookr\n\n\n\nWe can utilize the across function to obtain the results across the three variables simultaneously:\n\nsummary_weight &lt;- weight %&gt;%\n  dplyr::summarise(across(\n    .cols = c(dif_weight, birth_weight, discharge_weight), \n    .fns = list(\n    n = ~n(),\n      min = ~min(., na.rm = TRUE),\n      q1 = ~quantile(., 0.25, na.rm = TRUE),\n      median = ~quantile(., 0.5, na.rm = TRUE),\n      q3 = ~quantile(., 0.75, na.rm = TRUE),\n      max = ~max(., na.rm = TRUE),\n      mean = ~mean(., na.rm = TRUE),\n      sd = ~sd(., na.rm = TRUE),\n      skewness = ~EnvStats::skewness(., na.rm = TRUE),\n      kurtosis= ~EnvStats::kurtosis(., na.rm = TRUE)\n    ),\n    .names = \"{col}_{fn}\")\n    )\n\n# present the results\nsummary_weight &lt;- summary_weight %&gt;% \n  mutate(across(everything(), round, 2)) %&gt;%   # round to 3 decimal places\n  pivot_longer(1:30, names_to = \"Stats\", values_to = \"Values\")  # long format\n\nsummary_weight\n\n# A tibble: 30 × 2\n   Stats               Values\n   &lt;chr&gt;                &lt;dbl&gt;\n 1 dif_weight_n         25   \n 2 dif_weight_min      -30   \n 3 dif_weight_q1        20   \n 4 dif_weight_median    40   \n 5 dif_weight_q3        70   \n 6 dif_weight_max      103   \n 7 dif_weight_mean      39.6 \n 8 dif_weight_sd        32.3 \n 9 dif_weight_skewness  -0.16\n10 dif_weight_kurtosis  -0.08\n# … with 20 more rows\n\n\n\n\n\nweight %&gt;% \n  dlookr::describe(dif_weight, birth_weight, discharge_weight) %&gt;% \n  select(described_variables, n, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup()\n\n# A tibble: 3 × 9\n  described_variables     n   mean    sd   p25   p50   p75 skewness kurtosis\n  &lt;chr&gt;               &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 dif_weight             25   39.6  32.3    20    40    70   -0.157  -0.0752\n2 birth_weight           25 3076.  248.   2960  3150  3210   -0.291  -0.935 \n3 discharge_weight       25 3036.  248.   2880  3100  3220   -0.219  -1.02  \n\n\n\n\n\n\n\nAs it was previously mentioned, the mean of the differences (39.64) is close to median (40). Moreover, both the skewness and the kurtosis are approximately zero indicating a symmetric and mesokurtic distribution for the weight differences.\nNormality test\nAdditionally, we can check the statistical test for normality of the differences.\n\n weight %&gt;%\n    shapiro_test(dif_weight)\n\n# A tibble: 1 × 3\n  variable   statistic     p\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1 dif_weight     0.974 0.742\n\n\nThe Shapiro-Wilk test suggests that the weight differences are normally distributed (p=0.74 &gt; 0.05)."
  },
  {
    "objectID": "paired_t_test.html#run-the-paired-t-test",
    "href": "paired_t_test.html#run-the-paired-t-test",
    "title": "7  Paired t-test",
    "section": "\n7.6 Run the paired t-test",
    "text": "7.6 Run the paired t-test\nWe will perform a paired t-test to test the null hypothesis that the mean differences of weight equals to zero.\n\n\n\n\n\n\nPaired t-test\n\n\n\n\n\nBase R (1st way)\nBase R (2nd way)\nrstatix\n\n\n\nOur data are in a wide format. However, we are going to use only the dif_weight variable, inside the t.test():\n\nt.test(weight$dif_weight)\n\n\n    One Sample t-test\n\ndata:  weight$dif_weight\nt = 6.1428, df = 24, p-value = 2.401e-06\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 26.32139 52.95861\nsample estimates:\nmean of x \n    39.64 \n\n\n\n\n\nt.test(weight$birth_weight, weight$discharge_weight, paired = T)\n\n\n    Paired t-test\n\ndata:  weight$birth_weight and weight$discharge_weight\nt = 6.1428, df = 24, p-value = 2.401e-06\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 26.32139 52.95861\nsample estimates:\nmean difference \n          39.64 \n\n\n\n\n\nweight %&gt;% \n  t_test(dif_weight ~ 1, detailed = T)\n\n# A tibble: 1 × 12\n  estimate .y.   group1 group2     n stati…¹      p    df conf.…² conf.…³ method\n*    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; \n1     39.6 dif_… 1      null …    25    6.14 2.4e-6    24    26.3    53.0 T-test\n# … with 1 more variable: alternative &lt;chr&gt;, and abbreviated variable names\n#   ¹​statistic, ²​conf.low, ³​conf.high"
  },
  {
    "objectID": "paired_t_test.html#present-the-results-in-a-summary-table",
    "href": "paired_t_test.html#present-the-results-in-a-summary-table",
    "title": "7  Paired t-test",
    "section": "\n7.7 Present the results in a summary table",
    "text": "7.7 Present the results in a summary table\n\nShow the codetb1 &lt;- weight %&gt;% \n  mutate(id = row_number()) %&gt;% \n  select(-dif_weight) %&gt;% \n  pivot_longer(!id, names_to = \"group\", values_to = \"weights\")\n\ngt_sum3 &lt;- tb1 %&gt;% \ntbl_summary(by = group, include = -id,\n            label = list(weights ~ \"weights (grams)\"),\n            statistic =  weights ~ \"{mean} ({sd})\") %&gt;%\n  #add_difference(test = weights ~ \"paired.t.test\", group = id) %&gt;%\n  add_p(test = weights ~ \"paired.t.test\", group = id) %&gt;% \n  #modify_spanning_header(all_stat_cols() ~ \"**Weight**\") %&gt;% \n  as_gt()\n  \ngt_sum3 \n\n\n\n\n\n\nCharacteristic\n\nbirth_weight, N = 251\n\n\ndischarge_weight, N = 251\n\n\np-value2\n\n\n\nweights (grams)\n3,076 (248)\n3,036 (248)\n&lt;0.001\n\n\n\n\n1 Mean (SD)\n\n\n\n2 Paired t-test\n\n\n\n\n\n\nThere was a significant reduction in weight (39.6 g) after the discharge (p-value &lt;0.001 that is lower than 0.05; reject \\(H_0\\)). Note that the 95% confidence interval (26.3 to 52.9) doesn’t include the null hypothesized value of 0. However, is this reduction of clinical importance?"
  },
  {
    "objectID": "wilcoxon_test.html#research-question",
    "href": "wilcoxon_test.html#research-question",
    "title": "8  Wilcoxon Signed-Rank test",
    "section": "\n8.1 Research question",
    "text": "8.1 Research question\nThe dataset eyes contains thickness of the cornea (in microns) in patients with one eye affected by glaucoma; the other eye is unaffected. We investigate if there is evidence for difference in corneal thickness in affected and unaffected eyes.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\\(H_0\\): The distribution of the differences in thickness of the cornea is symmetrical about zero\n\\(H_1\\): The distribution of the differences in thickness of the cornea is not symmetrical about zero\n\n\n\nNOTE: If we are testing the null hypothesis that the median of the paired rank differences is zero, then the paired rank differences must all come from a symmetrical distribution. Note that we do not have to assume that the distributions of the original populations are symmetrical - two very positively skewed distributions that differ only by location will produce a set of paired rank differences that are symmetrical."
  },
  {
    "objectID": "wilcoxon_test.html#packages-we-need",
    "href": "wilcoxon_test.html#packages-we-need",
    "title": "8  Wilcoxon Signed-Rank test",
    "section": "\n8.2 Packages we need",
    "text": "8.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(gtsummary)\n\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "wilcoxon_test.html#preraring-the-data",
    "href": "wilcoxon_test.html#preraring-the-data",
    "title": "8  Wilcoxon Signed-Rank test",
    "section": "\n8.3 Preraring the data",
    "text": "8.3 Preraring the data\nWe import the data eyes in R:\n\nlibrary(readxl)\neyes &lt;- read_excel(here(\"data\", \"eyes.xlsx\"))\n\nWe calculate the differences using the function mutate():\n\neyes &lt;- eyes %&gt;%\n  mutate(dif_thickness = affected_eye - unaffected_eye)\n\n\n\n\nFigure 8.1: Table with data from “weight” file.\n\n\n\nWe inspect the data:\n\nglimpse(eyes) \n\nRows: 8\nColumns: 4\n$ id             &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8\n$ affected_eye   &lt;dbl&gt; 488, 478, 480, 426, 440, 410, 458, 460\n$ unaffected_eye &lt;dbl&gt; 484, 478, 492, 444, 436, 398, 464, 476\n$ dif_thickness  &lt;dbl&gt; 4, 0, -12, -18, 4, 12, -6, -16"
  },
  {
    "objectID": "wilcoxon_test.html#explore-the-characteristics-of-distribution-of-differences",
    "href": "wilcoxon_test.html#explore-the-characteristics-of-distribution-of-differences",
    "title": "8  Wilcoxon Signed-Rank test",
    "section": "\n8.4 Explore the characteristics of distribution of differences",
    "text": "8.4 Explore the characteristics of distribution of differences\nThe distributions of differences can be explored with appropriate plots and summary statistics.\nGraph\nWe can explore the data visually for symmetry with a density plot.\n\neyes %&gt;%\n  ggplot(aes(x = dif_thickness)) +\n  geom_density(fill = \"#76B7B2\", color=\"black\", alpha = 0.2) +\n  geom_vline(aes(xintercept=mean(dif_thickness)),\n            color=\"blue\", linetype=\"dashed\", size=1.2) +\n  geom_vline(aes(xintercept=median(dif_thickness)),\n            color=\"red\", linetype=\"dashed\", size=1.2) +\n  labs(x = \"Differences of thickness (micron)\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\")\n\n\n\nDensity plot of the thickness differences.\n\n\n\n\nSummary statistics\nSummary statistics can also be calculated for the variables.\n\n\n\n\n\n\nSummary statistics\n\n\n\n\n\ndplyr\ndlookr\n\n\n\nWe can utilize the across() function to obtain the results across the three variables simultaneously:\n\nsummary_eyes &lt;- eyes %&gt;%\n  dplyr::summarise(across(\n    .cols = c(dif_thickness, affected_eye, unaffected_eye), \n    .fns = list(\n    n = ~n(),\n      min = ~min(., na.rm = TRUE),\n      q1 = ~quantile(., 0.25, na.rm = TRUE),\n      median = ~quantile(., 0.5, na.rm = TRUE),\n      q3 = ~quantile(., 0.75, na.rm = TRUE),\n      max = ~max(., na.rm = TRUE),\n      mean = ~mean(., na.rm = TRUE),\n      sd = ~sd(., na.rm = TRUE),\n      skewness = ~EnvStats::skewness(., na.rm = TRUE),\n      kurtosis= ~EnvStats::kurtosis(., na.rm = TRUE)\n    ),\n    .names = \"{col}_{fn}\")\n    )\n\n# present the results\nsummary_eyes &lt;- summary_eyes %&gt;% \n  mutate(across(everything(), round, 2)) %&gt;%   # round to 3 decimal places\n  pivot_longer(1:30, names_to = \"Stats\", values_to = \"Values\")  # long format\n\nsummary_eyes\n\n# A tibble: 30 × 2\n   Stats                  Values\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 dif_thickness_n          8   \n 2 dif_thickness_min      -18   \n 3 dif_thickness_q1       -13   \n 4 dif_thickness_median    -3   \n 5 dif_thickness_q3         4   \n 6 dif_thickness_max       12   \n 7 dif_thickness_mean      -4   \n 8 dif_thickness_sd        10.7 \n 9 dif_thickness_skewness   0.03\n10 dif_thickness_kurtosis  -1.37\n# … with 20 more rows\n\n\n\n\n\neyes %&gt;% \n  dlookr::describe(dif_thickness, affected_eye, unaffected_eye) %&gt;% \n  select(described_variables, n, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup()\n\n# A tibble: 3 × 9\n  described_variables     n  mean    sd   p25   p50   p75 skewness kurtosis\n  &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 dif_thickness           8    -4  10.7  -13     -3    4    0.0295   -1.37 \n2 affected_eye            8   455  27.7  436.   459  478.  -0.493    -0.985\n3 unaffected_eye          8   459  31.3  442    470  480.  -1.11      0.794\n\n\n\n\n\n\n\nThe differences seems to come from a population with a symmetrical distribution and the skewness is close to zero (0.03). However, the (excess) kurtosis equals to -1.37 (platykurtic) and the sample size is small. Therefore, the data may not follow the normal distribution.\nNormality test We can use Shapiro-Wilk test to check for normality of the differences. However, here, normality test is not helpful because of the small sample (the test is under-powered).\n\n eyes %&gt;%\n    shapiro_test(dif_thickness)\n\n# A tibble: 1 × 3\n  variable      statistic     p\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;\n1 dif_thickness     0.944 0.651\n\n\nThe Shapiro-Wilk test suggests that the weight differences are normally distributed (p=0.65 &gt; 0.05)."
  },
  {
    "objectID": "wilcoxon_test.html#run-the-wilcoxon-signed-rank-test",
    "href": "wilcoxon_test.html#run-the-wilcoxon-signed-rank-test",
    "title": "8  Wilcoxon Signed-Rank test",
    "section": "\n8.5 Run the Wilcoxon Signed-Rank test",
    "text": "8.5 Run the Wilcoxon Signed-Rank test\nThe differences between the two measurements can be tested using a rank test such as Wilcoxon Signed-Rank test.\n\n\n\n\n\n\nWilcoxon Signed-Rank test\n\n\n\n\n\nBase R (1st way)\nBase R (2nd way)\nrstatix\n\n\n\n\nwilcox.test(eyes$dif_thickness)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  eyes$dif_thickness\nV = 7.5, p-value = 0.3088\nalternative hypothesis: true location is not equal to 0\n\n\n\n\n\nwilcox.test(eyes$affected_eye, eyes$unaffected_eye, paired = TRUE)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  eyes$affected_eye and eyes$unaffected_eye\nV = 7.5, p-value = 0.3088\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n\n eyes %&gt;%\n     wilcox_test(dif_thickness ~ 1)\n\n# A tibble: 1 × 6\n  .y.           group1 group2         n statistic     p\n* &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 dif_thickness 1      null model     8       7.5 0.309\n\n\n\n\n\n\n\nThe result is not significant (p = 0.31 &gt; 0.05). However, we can’t be certain that there is not difference in corneal thickness in affected and unaffected eyes because the sample size is very small."
  },
  {
    "objectID": "wilcoxon_test.html#present-the-results-in-a-summary-table",
    "href": "wilcoxon_test.html#present-the-results-in-a-summary-table",
    "title": "8  Wilcoxon Signed-Rank test",
    "section": "\n8.6 Present the results in a summary table",
    "text": "8.6 Present the results in a summary table\n\nShow the codetb2 &lt;- eyes %&gt;%\n  select(-dif_thickness) %&gt;% \n  pivot_longer(!id, names_to = \"groups\", values_to = \"thickness\")\n\ngt_sum4 &lt;- tb2 %&gt;% \ntbl_summary(by = groups, include = -id,\n            label = list(thickness ~ \"thickness (microns)\"),\n            digits = list(everything() ~ 1)) %&gt;%\n  add_p(test = thickness ~ \"paired.wilcox.test\", group = id) %&gt;% \n  as_gt()\n  \ngt_sum4 \n\n\n\n\n\n\nCharacteristic\n\naffected_eye, N = 81\n\n\nunaffected_eye, N = 81\n\n\np-value2\n\n\n\nthickness (microns)\n459.0 (436.5, 478.5)\n470.0 (442.0, 479.5)\n0.3\n\n\n\n\n1 Median (IQR)\n\n\n\n2 Wilcoxon signed rank test with continuity correction\n\n\n\n\n\n\nThere is not evidence from this small study with patients of glaucoma that the thickness of the cornea in affected eyes, median = 459 \\(\\mu{m}\\) (IQR: 436.5, 478.5) \\(\\mu{m}\\), differs from unaffected eyes 470 \\(\\mu{m}\\) (442, 479.5) (p=0.30 &gt;0.05)."
  },
  {
    "objectID": "anova.html#research-question-and-hypothesis-testing",
    "href": "anova.html#research-question-and-hypothesis-testing",
    "title": "9  One-way ANOVA test",
    "section": "\n9.1 Research question and Hypothesis Testing",
    "text": "9.1 Research question and Hypothesis Testing\nWe consider the data in dataDWL dataset. In this example we explore the variations between weight loss according to four different types of diet. The question that may be asked is: does the average weight loss differ according to the diet?\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): all group means are equal (the means of weight loss in the four diets are equal; \\(\\mu_{1} = \\mu_{2} = \\mu_{3} = \\mu_{4}\\))\n\n\\(H_1\\): at least one group mean differs from the others (there is at least one diet with mean weight loss different from the others)"
  },
  {
    "objectID": "anova.html#packages-we-need",
    "href": "anova.html#packages-we-need",
    "title": "9  One-way ANOVA test",
    "section": "\n9.2 Packages we need",
    "text": "9.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(PupillometryR)\nlibrary(gtsummary)\n\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "anova.html#preraring-the-data",
    "href": "anova.html#preraring-the-data",
    "title": "9  One-way ANOVA test",
    "section": "\n9.3 Preraring the data",
    "text": "9.3 Preraring the data\nWe import the data dataDWL in R:\n\nlibrary(readxl)\ndataDWL &lt;- read_excel(here(\"data\", \"dataDWL.xlsx\"))\n\n\n\n\nFigure 9.1: Table with data from “dataDWL” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(dataDWL)\n\nRows: 60\nColumns: 2\n$ WeightLoss &lt;dbl&gt; 9.9, 9.6, 8.0, 4.9, 10.2, 9.0, 9.8, 10.8, 6.2, 8.3, 12.9, 1…\n$ Diet       &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\",…\n\n\nThe dataset dataDWL has 60 participants and includes two variables. The numeric (&lt;dbl&gt;) WeightLoss variable and the character (&lt;chr&gt;) Diet variable (with levels “A”, “B”, “C” and “D”) which should be converted to a factor variable using the factor() function as follows:\n\ndataDWL &lt;- dataDWL %&gt;% \n  mutate(Diet = factor(Diet))\nglimpse(dataDWL)\n\nRows: 60\nColumns: 2\n$ WeightLoss &lt;dbl&gt; 9.9, 9.6, 8.0, 4.9, 10.2, 9.0, 9.8, 10.8, 6.2, 8.3, 12.9, 1…\n$ Diet       &lt;fct&gt; A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, B, B, B, B, B,…"
  },
  {
    "objectID": "anova.html#assumptions",
    "href": "anova.html#assumptions",
    "title": "9  One-way ANOVA test",
    "section": "\n9.4 Assumptions",
    "text": "9.4 Assumptions\n\n\n\n\n\n\nCheck if the following assumptions are satisfied\n\n\n\n\nThe data are normally distributed in all groups\nThe data in all groups have similar variance (also named as homogeneity of variance or homoscedasticity)\n\n\n\nA. Explore the characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\nGraphs\nWe can visualize the distribution of WeightLoss for the four Diet groups:\n\nset.seed(123)\nggplot(dataDWL, aes(x=Diet, y=WeightLoss)) + \n  geom_flat_violin(aes(fill = Diet), scale = \"count\") +\n  geom_boxplot(width = 0.14, outlier.shape = NA, alpha = 0.5) +\n  geom_point(position = position_jitter(width = 0.05), \n             size = 1.2, alpha = 0.6) +\n  ggsci::scale_fill_jco() +\n  theme_classic(base_size = 14) +\n  theme(legend.position=\"none\", \n        axis.text = element_text(size = 14))\n\n\n\nRain cloud plot.\n\n\n\n\nThe above figure shows that the data are close to symmetry and the assumption of a normal distribution is reasonable. Additionally, we can observe that the largest weight loss seems to have been achieved by the participants in C diet.\nSummary statistics\nThe WeightLoss summary statistics for each diet group are:\n\n\n\n\n\n\nSummary statistics by group\n\n\n\n\n\ndplyr\ndlookr\n\n\n\n\nDWL_summary &lt;- dataDWL %&gt;%\n  group_by(Diet) %&gt;%\n  dplyr::summarise(\n    n = n(),\n    min = min(WeightLoss, na.rm = TRUE),\n    q1 = quantile(WeightLoss, 0.25, na.rm = TRUE),\n    median = quantile(WeightLoss, 0.5, na.rm = TRUE),\n    q3 = quantile(WeightLoss, 0.75, na.rm = TRUE),\n    max = max(WeightLoss, na.rm = TRUE),\n    mean = mean(WeightLoss, na.rm = TRUE),\n    sd = sd(WeightLoss, na.rm = TRUE),\n    skewness = EnvStats::skewness(WeightLoss, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(WeightLoss, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\nDWL_summary\n\n# A tibble: 4 × 11\n  Diet      n   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 A        15   4.9  8.15    9.6  10.5  12.9  9.18  2.30  -0.471    -0.302\n2 B        15   3.8  7.85    9.2  10.8  12.7  8.91  2.78  -0.467    -0.515\n3 C        15   8.7 10.8    12.2  13    15.1 12.1   1.79  -0.0451   -0.530\n4 D        15   5.8  9.5    10.5  11.8  13.7 10.5   2.23  -0.475     0.229\n\n\n\n\n\ndataDWL %&gt;% \n  group_by(Diet) %&gt;% \n  dlookr::describe(WeightLoss) %&gt;% \n  select(described_variables,  Diet, n, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup()\n\n# A tibble: 4 × 10\n  described_variables Diet      n  mean    sd   p25   p50   p75 skewness kurto…¹\n  &lt;chr&gt;               &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 WeightLoss          A        15  9.18  2.30  8.15   9.6  10.5  -0.471   -0.302\n2 WeightLoss          B        15  8.91  2.78  7.85   9.2  10.8  -0.467   -0.515\n3 WeightLoss          C        15 12.1   1.79 10.8   12.2  13    -0.0451  -0.530\n4 WeightLoss          D        15 10.5   2.23  9.5   10.5  11.8  -0.475    0.229\n# … with abbreviated variable name ¹​kurtosis\n\n\n\n\n\n\n\nThe means are close to medians and the standard deviations are also similar. Moreover, both skewness and (excess) kurtosis falls into the acceptable range of [-1, 1] indicating approximately normal distributions for all diet groups.\n \nNormality test\nThe Shapiro-Wilk test for normality for each diet group is:\n\ndataDWL %&gt;%\n  group_by(Diet) %&gt;%\n  shapiro_test(WeightLoss) %&gt;% \n  ungroup()\n\n# A tibble: 4 × 4\n  Diet  variable   statistic     p\n  &lt;fct&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1 A     WeightLoss     0.958 0.662\n2 B     WeightLoss     0.941 0.390\n3 C     WeightLoss     0.964 0.768\n4 D     WeightLoss     0.944 0.435\n\n\nThe tests of normality suggest that the data for the WeightLoss in all groups are normally distributed (p &gt; 0.05).\nB. Levene’s test for equality of variances\nThe Levene’s test for equality of variances is:\n\ndataDWL %&gt;% \n  levene_test(WeightLoss ~ Diet)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     3    56     0.600 0.617\n\n\nSince the p = 0.617 &gt; 0.05, the null hypothesis (\\(H_{0}\\): the variances of WeighLoss in four diet groups are equal) can not be rejected."
  },
  {
    "objectID": "anova.html#run-the-one-way-anova-test",
    "href": "anova.html#run-the-one-way-anova-test",
    "title": "9  One-way ANOVA test",
    "section": "\n9.5 Run the one-way ANOVA test",
    "text": "9.5 Run the one-way ANOVA test\nNow, we will perform an one-way ANOVA (with equal variances: Fisher’s classic ANOVA) to test the null hypothesis that the mean weight loss is the same for all the diet groups.\n\n\n\n\n\n\nOne-way ANOVA test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\n# Compute the analysis of variance\nanova_one_way &lt;- aov(WeightLoss ~ Diet, data = dataDWL)\n\n# Summary of the analysis\nsummary(anova_one_way)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nDiet         3  97.33   32.44   6.118 0.00113 **\nResiduals   56 296.99    5.30                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\ndataDWL %&gt;% \n  anova_test(WeightLoss ~ Diet, detailed = T)\n\nANOVA Table (type II tests)\n\n  Effect   SSn     SSd DFn DFd     F     p p&lt;.05   ges\n1   Diet 97.33 296.987   3  56 6.118 0.001     * 0.247\n\n\n\n\n\n\n\nThe statistic F=6.118 indicates the obtained F-statistic = (variation between sample means \\(/\\) variation within the samples). Note that we are comparing to an F-distribution (F-test). The degrees of freedom in the numerator (DFn) and the denominator (DFd) are 3 and 56, respectively (numarator: variation between sample means; denominator: variation within the samples).\nThe p=0.001 is lower than 0.05. There is at least one diet with mean weight loss which is different from the others means.\nFrom ANOVA table provided by the {rstatix} we can also calculate the generalized effect size (ges). The ges is the proportion of variability explained by the factor Diet (SSn) to total variability of the dependent variable (SSn + SSd), so:\n\\[\\ ges= 97.33 / (97.33 + 296.987) = 97.33 / 394.317 = 0.247\\] A ges of 0.247 (24.7%) means that 24.7% of the change in the weight loss can be accounted for the diet conditions.\n \nPresent the results in a summary table\nA summary table can also be presented:\n\nShow the codegt_sum4 &lt;- dataDWL %&gt;% \n  tbl_summary(\n    by = Diet, \n    statistic = WeightLoss ~ \"{mean} ({sd})\", \n    digits = list(everything() ~ 1),\n    label = list(WeightLoss ~ \"Weight Loss (kg)\"), \n    missing = c(\"no\")) %&gt;% \n  add_p(test = WeightLoss ~ \"aov\", purrr::partial(style_pvalue, digits = 2)) %&gt;% \n  as_gt() \n\ngt_sum4\n\n\n\n\n\n\nCharacteristic\n\nA, N = 151\n\n\nB, N = 151\n\n\nC, N = 151\n\n\nD, N = 151\n\n\np-value2\n\n\n\nWeight Loss (kg)\n9.2 (2.3)\n8.9 (2.8)\n12.1 (1.8)\n10.5 (2.2)\n0.001\n\n\n\n\n1 Mean (SD)\n\n\n\n2 One-way ANOVA"
  },
  {
    "objectID": "anova.html#post-hoc-tests",
    "href": "anova.html#post-hoc-tests",
    "title": "9  One-way ANOVA test",
    "section": "\n9.6 Post-hoc tests",
    "text": "9.6 Post-hoc tests\nA significant one-way ANOVA is generally followed up by post-hoc tests to perform multiple pairwise comparisons between groups:\n\n\n\n\n\n\nPost-hoc tests\n\n\n\n\n\nTukey test\nBonferroni\n\n\n\nIt is appropriate to use this test when one desires all the possible comparisons between a large set of means (e.g., 6 or more means) and the variances are supposed to be equal.\n\n# Pairwise comparisons\npwc_Tukey &lt;- dataDWL %&gt;% \n  tukey_hsd(WeightLoss ~ Diet)\n\npwc_Tukey \n\n# A tibble: 6 × 9\n  term  group1 group2 null.value estimate conf.low conf.high   p.adj p.adj.sig…¹\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      \n1 Diet  A      B               0   -0.273   -2.50      1.95  0.988   ns         \n2 Diet  A      C               0    2.93     0.707     5.16  0.00513 **         \n3 Diet  A      D               0    1.36    -0.867     3.59  0.377   ns         \n4 Diet  B      C               0    3.21     0.980     5.43  0.0019  **         \n5 Diet  B      D               0    1.63    -0.593     3.86  0.222   ns         \n6 Diet  C      D               0   -1.57    -3.80      0.653 0.252   ns         \n# … with abbreviated variable name ¹​p.adj.signif\n\n\nThe output contains the following columns of interest:\n\nestimate: estimate of the difference between means of the two groups\nconf.low, conf.high: the lower and the upper end point of the confidence interval at 95% (default)\np.adj: p-value after adjustment for the multiple comparisons.\n\n\n\nAlternatively, we can perform pairwise comparisons using pairwise t-test with the assumption of equal variances (pool.sd = TRUE) and calculate the adjusted p-values using Bonferroni correction:\n\npwc_Bonferroni &lt;- dataDWL %&gt;% \n  pairwise_t_test(\n    WeightLoss ~ Diet, pool.sd = TRUE,\n    p.adjust.method = \"bonferroni\"\n    )\npwc_Bonferroni \n\n# A tibble: 6 × 9\n  .y.        group1 group2    n1    n2        p p.signif   p.adj p.adj.signif\n* &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       \n1 WeightLoss A      B         15    15 0.746    ns       1       ns          \n2 WeightLoss A      C         15    15 0.000954 ***      0.00572 **          \n3 WeightLoss B      C         15    15 0.000344 ***      0.00206 **          \n4 WeightLoss A      D         15    15 0.111    ns       0.669   ns          \n5 WeightLoss B      D         15    15 0.0571   ns       0.343   ns          \n6 WeightLoss C      D         15    15 0.0666   ns       0.399   ns          \n\n\n\n\n\n\n\nPairwise comparisons were carried out using the method of Tukey (or Bonferroni) and the adjusted p-values were calculated.\nThe results in Tukey post hoc table show that the weight loss from diet C seems to be significantly larger than diet A (mean difference = 2.91 kg, 95%CI [0.71, 5.16], p=0.005 &lt;0.05) and diet B (mean difference = 3.21 kg, 95%CI [0.98, 5.43], p=0.002 &lt;0.05)."
  },
  {
    "objectID": "anova.html#welch-one-way-anova",
    "href": "anova.html#welch-one-way-anova",
    "title": "9  One-way ANOVA test",
    "section": "\n9.7 Welch one-way ANOVA",
    "text": "9.7 Welch one-way ANOVA\nIf the variance is different between the groups (unequal variances) then the degrees of freedom associated with the ANOVA test are calculated differently (Welch one-way ANOVA).\n\n# Welch one-way ANOVA test (not assuming equal variance)\n\ndataDWL %&gt;% \n  welch_anova_test(WeightLoss ~ Diet)\n\n# A tibble: 1 × 7\n  .y.            n statistic   DFn   DFd        p method     \n* &lt;chr&gt;      &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      \n1 WeightLoss    60      7.02     3  30.8 0.000989 Welch ANOVA\n\n\nIn this case, the Games-Howell post hoc test (or pairwise t-tests with no assumption of equal variances with Bonferroni correction) can be used to compare all possible combinations of group differences.\nGames-Howell post hoc test\n\n# Pairwise comparisons (Games-Howell)\n\npwc_GH &lt;- dataDWL %&gt;% \n  games_howell_test(WeightLoss ~ Diet)\n\npwc_GH\n\n# A tibble: 6 × 8\n  .y.        group1 group2 estimate conf.low conf.high p.adj p.adj.signif\n* &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 WeightLoss A      B        -0.273   -2.82      2.28  0.991 ns          \n2 WeightLoss A      C         2.93     0.872     4.99  0.003 **          \n3 WeightLoss A      D         1.36    -0.898     3.62  0.371 ns          \n4 WeightLoss B      C         3.21     0.849     5.56  0.005 **          \n5 WeightLoss B      D         1.63    -0.889     4.16  0.308 ns          \n6 WeightLoss C      D        -1.57    -3.60      0.452 0.17  ns"
  },
  {
    "objectID": "kruskal_wallis.html#research-question-and-hypothesis-testing",
    "href": "kruskal_wallis.html#research-question-and-hypothesis-testing",
    "title": "10  Kruskal-Wallis test",
    "section": "\n10.1 Research question and Hypothesis Testing",
    "text": "10.1 Research question and Hypothesis Testing\nWe consider the data in dataVO2 dataset. We wish to compare the VO2max in three different sports (runners, rowers, and triathletes).\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): the distribution of VO2max is the same in all groups (the medians of VO2max in the three sports are the same)\n\n\\(H_1\\): there is at least one group with VO2max distribution different from the others (there is at least one sport with median VO2max different from the others)\n\n\n\nNOTE: The Kruskal-Wallis test should be regarded as a test of dominance between distributions comparing the mean ranks. The null hypothesis is that the observations from one group do not tend to have a higher or lower ranking than observations from the other groups. This test does not test the medians of the data as is commonly thought, it tests the whole distribution. However, if the distributions of the two groups have similar shapes, the Kruskal-Wallis test can be used to determine whether there are differences in the medians in the two groups. In practice, we use the medians to present the results."
  },
  {
    "objectID": "kruskal_wallis.html#packages-we-need",
    "href": "kruskal_wallis.html#packages-we-need",
    "title": "10  Kruskal-Wallis test",
    "section": "\n10.2 Packages we need",
    "text": "10.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(PupillometryR)\nlibrary(gtsummary)\n\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "kruskal_wallis.html#preraring-the-data",
    "href": "kruskal_wallis.html#preraring-the-data",
    "title": "10  Kruskal-Wallis test",
    "section": "\n10.3 Preraring the data",
    "text": "10.3 Preraring the data\nWe import the data dataVO2 in R:\n\nlibrary(readxl)\ndataVO2 &lt;- read_excel(here(\"data\", \"dataVO2.xlsx\"))\n\n\n\n\nFigure 10.1: Table with data from “dataVO2” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(dataVO2)\n\nRows: 30\nColumns: 2\n$ sport  &lt;chr&gt; \"runners\", \"runners\", \"runners\", \"runners\", \"runners\", \"runners…\n$ VO2max &lt;dbl&gt; 73.8, 79.9, 75.5, 72.5, 82.2, 78.3, 77.9, 76.5, 72.3, 80.2, 71.…\n\n\nThe dataset dataVO2 has 30 participants and two variables. The numeric VO2max variable and the sport variable (with levels “roweres”, “runners”, and “triathletes”) which should be converted to a factor variable using the factor() function as follows:\n\ndataVO2 &lt;- dataVO2 %&gt;% \n  mutate(sport = factor(sport))\n\nglimpse(dataVO2)\n\nRows: 30\nColumns: 2\n$ sport  &lt;fct&gt; runners, runners, runners, runners, runners, runners, runners, …\n$ VO2max &lt;dbl&gt; 73.8, 79.9, 75.5, 72.5, 82.2, 78.3, 77.9, 76.5, 72.3, 80.2, 71.…"
  },
  {
    "objectID": "kruskal_wallis.html#explore-the-characteristics-of-distribution-for-each-group-and-check-for-normality",
    "href": "kruskal_wallis.html#explore-the-characteristics-of-distribution-for-each-group-and-check-for-normality",
    "title": "10  Kruskal-Wallis test",
    "section": "\n10.4 Explore the characteristics of distribution for each group and check for normality",
    "text": "10.4 Explore the characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\nGraph\nWe can visualize the distribution of VO2max for the three sport groups:\n\nset.seed(123)\nggplot(dataVO2, aes(x=sport, y=VO2max)) + \n  geom_flat_violin(aes(fill = sport), scale = \"count\") +\n  geom_boxplot(width = 0.14, outlier.shape = NA, alpha = 0.5) +\n  geom_point(position = position_jitter(width = 0.05), \n             size = 1.2, alpha = 0.6) +\n  ggsci::scale_fill_jco() +\n  theme_classic(base_size = 14) +\n  theme(legend.position=\"none\", \n        axis.text = element_text(size = 14))\n\n\n\nRain cloud plot.\n\n\n\n\nThe above figure shows that the data in triathletes group have some outliers. Additionally, we can observe that the runners group seems to have the largest VO2max.\nSummary statistics\nThe VO2max summary statistics for each sport group are:\n\n\n\n\n\n\nSummary statistics by group\n\n\n\n\n\ndplyr\ndlookr\n\n\n\n\nVO2_summary &lt;- dataVO2 %&gt;%\n  group_by(sport) %&gt;%\n  dplyr::summarise(\n    n = n(),\n    min = min(VO2max, na.rm = TRUE),\n    q1 = quantile(VO2max, 0.25, na.rm = TRUE),\n    median = quantile(VO2max, 0.5, na.rm = TRUE),\n    q3 = quantile(VO2max, 0.75, na.rm = TRUE),\n    max = max(VO2max, na.rm = TRUE),\n    mean = mean(VO2max, na.rm = TRUE),\n    sd = sd(VO2max, na.rm = TRUE),\n    skewness = EnvStats::skewness(VO2max, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(VO2max, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\nVO2_summary\n\n# A tibble: 3 × 11\n  sport           n   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;fct&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 rowers         10  67.1  67.8   69.6  73.1  74.7  70.3  3.04  0.502      -1.53\n2 runners        10  72.3  74.2   77.2  79.5  82.2  76.9  3.39 -0.00950    -1.16\n3 triathletes    10  63.2  64     65.4  67.6  76.6  67.0  4.40  1.51        1.60\n\n\n\n\n\ndataVO2 %&gt;% \n  group_by(sport) %&gt;% \n  dlookr::describe(VO2max) %&gt;% \n  select(described_variables,  sport, n, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup()\n\n# A tibble: 3 × 10\n  described_variables sport     n  mean    sd   p25   p50   p75 skewness kurto…¹\n  &lt;chr&gt;               &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 VO2max              rowe…    10  70.3  3.04  67.8  69.6  73.1  0.502     -1.53\n2 VO2max              runn…    10  76.9  3.39  74.2  77.2  79.5 -0.00950   -1.16\n3 VO2max              tria…    10  67.0  4.40  64    65.4  67.6  1.51       1.60\n# … with abbreviated variable name ¹​kurtosis\n\n\n\n\n\n\n\nThe sample size is relative small (10 observations in each group). Moreover, the skewness (1.5) and the (excess) kurtosis (1.6) for the triathletes fall outside of the acceptable range of [-1, 1] indicating right-skewed and leptokurtic distribution.\nNormality test\nThe Shapiro-Wilk test for normality for each sport group is:\n\ndataVO2 %&gt;%\n  group_by(sport) %&gt;%\n  shapiro_test(VO2max) %&gt;% \n  ungroup()\n\n# A tibble: 3 × 4\n  sport       variable statistic      p\n  &lt;fct&gt;       &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 rowers      VO2max       0.865 0.0872\n2 runners     VO2max       0.954 0.712 \n3 triathletes VO2max       0.816 0.0229\n\n\nWe can see that the data for the triathletes is not normally distributed (p=0.023 &lt;0.05) according to the Shapiro-Wilk test.\nBy considering all of the information together (small samples, graphs, normality test) the overall decision is against of normality."
  },
  {
    "objectID": "kruskal_wallis.html#run-the-kruskal-wallis-test",
    "href": "kruskal_wallis.html#run-the-kruskal-wallis-test",
    "title": "10  Kruskal-Wallis test",
    "section": "\n10.5 Run the Kruskal-Wallis test",
    "text": "10.5 Run the Kruskal-Wallis test\nNow, we will perform a Kruskal-Wallis test to compare the VO2max in three sports.\n\n\n\n\n\n\nKruskal-Wallis test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nkruskal.test(VO2max ~ sport, data = dataVO2)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  VO2max by sport\nKruskal-Wallis chi-squared = 16.351, df = 2, p-value = 0.0002815\n\n\n\n\n\ndataVO2 %&gt;% \n  kruskal_test(VO2max ~ sport)\n\n# A tibble: 1 × 6\n  .y.        n statistic    df        p method        \n* &lt;chr&gt;  &lt;int&gt;     &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;         \n1 VO2max    30      16.4     2 0.000281 Kruskal-Wallis\n\n\n\n\n\n\n\nThe p-value (&lt;0.001) is lower than 0.05. There is at least one sport in which the VO2max is different from the others.\n \nPresent the results in a summary table\nA summary table can also be presented:\n\nShow the codegt_sum10 &lt;- dataVO2 %&gt;% \n  tbl_summary(\n    by = sport, \n    statistic = VO2max ~ \"{median} ({p25}, {p75})\", \n    digits = list(everything() ~ 1),\n    label = list(VO2max ~ \"VO2max (mL/kg/min)\"), \n    missing = c(\"no\")) %&gt;% \n  add_p(test = VO2max ~ \"kruskal.test\", purrr::partial(style_pvalue, digits = 2)) %&gt;%\n  as_gt() \n\ngt_sum10\n\n\n\n\n\n\nCharacteristic\n\nrowers, N = 101\n\n\nrunners, N = 101\n\n\ntriathletes, N = 101\n\n\np-value2\n\n\n\nVO2max (mL/kg/min)\n69.6 (67.8, 73.1)\n77.2 (74.2, 79.5)\n65.4 (64.0, 67.6)\n&lt;0.001\n\n\n\n\n1 Median (IQR)\n\n\n\n2 Kruskal-Wallis rank sum test"
  },
  {
    "objectID": "kruskal_wallis.html#post-hoc-tests",
    "href": "kruskal_wallis.html#post-hoc-tests",
    "title": "10  Kruskal-Wallis test",
    "section": "\n10.6 Post-hoc tests",
    "text": "10.6 Post-hoc tests\nA significant WMW is generally followed up by post-hoc tests to perform multiple pairwise comparisons between groups:\n\n\n\n\n\n\nPost-hoc tests\n\n\n\n\n\nDunn’s approach\nWMW with Bonferroni\n\n\n\n\n# Pairwise comparisons\npwc_Dunn &lt;- dataVO2 %&gt;% \n  dunn_test(VO2max ~ sport, p.adjust.method = \"bonferroni\")\n\npwc_Dunn \n\n# A tibble: 3 × 9\n  .y.    group1  group2         n1    n2 statistic         p    p.adj p.adj.si…¹\n* &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;     \n1 VO2max rowers  runners        10    10      2.43 0.0152    0.0457   *         \n2 VO2max rowers  triathletes    10    10     -1.59 0.112     0.337    ns        \n3 VO2max runners triathletes    10    10     -4.01 0.0000596 0.000179 ***       \n# … with abbreviated variable name ¹​p.adj.signif\n\n\n\n\nAlternatively, we can perform pairwise comparisons using pairwise WMW’s test and calculate the adjusted p-values using Bonferroni correction:\n\n# Pairwise comparisons\n\npwc_BW &lt;- dataVO2 %&gt;% \n  pairwise_wilcox_test(VO2max ~ sport, p.adjust.method = \"bonferroni\")\n\npwc_BW\n\n# A tibble: 3 × 9\n  .y.    group1  group2         n1    n2 statistic        p p.adj p.adj.signif\n* &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 VO2max rowers  runners        10    10       8.5 0.002    0.006 **          \n2 VO2max rowers  triathletes    10    10      80.5 0.023    0.07  ns          \n3 VO2max runners triathletes    10    10      93   0.000487 0.001 **          \n\n\n\n\n\n\n\nDunn’s pairwise comparisons were carried out using the method of Bonferroni and adjusting the p-values were calculated.\nThe runners’ VO2max (median= 77.2, IQR=[74.2, 79.5] mL/kg/min) seems to differ significantly (larger based on the medians) from rowers (69.6 [67.8, 73.1] mL/kg/min, p=0.046 &lt;0.05) and triathletes (65.4 [64.0, 67.6] mL/kg/min, p &lt;0.001)."
  },
  {
    "objectID": "correlation.html#research-question-and-hypothesis-testing",
    "href": "correlation.html#research-question-and-hypothesis-testing",
    "title": "11  Correlation",
    "section": "\n11.1 Research question and Hypothesis Testing",
    "text": "11.1 Research question and Hypothesis Testing\nWe consider the data in Birthweight dataset. Let’s say that we want to explore the association between weight (in Kg) and height (in cm) for a sample of 550 infants of 1 month age.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There is no association between the two numeric variables (they are independent)\n\n\\(H_1\\): There is association between the two numeric variables (they are dependent)"
  },
  {
    "objectID": "correlation.html#packages-we-need",
    "href": "correlation.html#packages-we-need",
    "title": "11  Correlation",
    "section": "\n11.2 Packages we need",
    "text": "11.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(XICOR)\nlibrary(ggExtra)\n\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "correlation.html#preraring-the-data",
    "href": "correlation.html#preraring-the-data",
    "title": "11  Correlation",
    "section": "\n11.3 Preraring the data",
    "text": "11.3 Preraring the data\nWe import the data BirthWeight in R:\n\nlibrary(readxl)\nBirthWeight &lt;- read_excel(here(\"data\", \"BirthWeight.xlsx\"))\n\n\n\n\nFigure 11.1: Table with data from “BirthWeight” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(BirthWeight)\n\nRows: 550\nColumns: 7\n$ id        &lt;chr&gt; \"L001\", \"L003\", \"L004\", \"L005\", \"L006\", \"L007\", \"L008\", \"L00…\n$ weight    &lt;dbl&gt; 4.0, 4.6, 4.8, 3.9, 4.6, 3.6, 3.6, 4.5, 5.0, 3.7, 4.2, 4.4, …\n$ height    &lt;dbl&gt; 55.5, 57.0, 56.0, 56.0, 55.0, 51.5, 56.0, 57.0, 58.5, 52.0, …\n$ headc     &lt;dbl&gt; 37.5, 38.5, 38.5, 39.0, 39.5, 34.5, 38.0, 39.7, 39.0, 38.0, …\n$ gender    &lt;chr&gt; \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Femal…\n$ education &lt;chr&gt; \"tertiary\", \"tertiary\", \"year12\", \"tertiary\", \"year10\", \"ter…\n$ parity    &lt;chr&gt; \"2 or more siblings\", \"Singleton\", \"2 or more siblings\", \"On…\n\n\nThe data set BirthWeight has 550 infants of 1 month age (rows) and includes seven variables (columns). Both the weight and height are numeric (&lt;dbl&gt;) variables."
  },
  {
    "objectID": "correlation.html#plot-the-data",
    "href": "correlation.html#plot-the-data",
    "title": "11  Correlation",
    "section": "\n11.4 Plot the data",
    "text": "11.4 Plot the data\nA first step that is usually useful in studying the association between two numeric variables is to prepare a scatter plot of the data. The pattern made by the points plotted on the scatter plot usually suggests the basic nature and strength of the association between two variables.\n\np &lt;- ggplot(BirthWeight, aes(height, weight)) +\n  geom_point(color = \"blue\", size = 2) +\n  theme_minimal(base_size = 14)\n\nggMarginal(p, type = \"histogram\", \n           xparams = list(fill = 7),\n           yparams = list(fill = 3))\n\n\n\nScatter plot of the association between height and weight in 550 infants of 1 month age.\n\n\n\n\nThe points in the scatter plot seem to be scattered around an invisible line. The scatter plot also shows that, in general, infants with high height tend to have high weight (positive association).\nAdditionally, the marginal histograms show that the data are approximately normally distributed (we have a large sample so the graphs are reliable) for both weight and height."
  },
  {
    "objectID": "correlation.html#correlation-between-two-numeric-variables",
    "href": "correlation.html#correlation-between-two-numeric-variables",
    "title": "11  Correlation",
    "section": "\n11.5 Correlation between two numeric variables",
    "text": "11.5 Correlation between two numeric variables\nCorrelation coefficients\nPearson’s coefficient measures linear correlation, while the Spearman’s and Kendall’s coefficients compare the ranks of data and measures monotonic associations. These coefficients are very powerful for detecting linear or monotonic associations, respectively. The new \\(ξ\\) correlation coefficient is more appropriate to measure the strength of non-monotonic associations.\nNote that the correlation between variables X and Y is equal to the correlation between variables Y and X so the order of the variables in the functions does not matter. The four correlations coefficients are:\n\n\n\n\n\n\nCorrelation coefficients\n\n\n\n\n\nPearson’s \\(r\\)\nSpearman’s \\(r_{s}\\)\nKendall’s \\(\\tau\\)\nCoefficient \\(ξ\\)\n\n\n\nThe Pearson’s correlation coefficient can be calculated for any dataset with two numeric variables. However, before we calculate the Pearson’s coefficient we should make sure that the following assumptions are met:\nAssumptions for Pearson’s \\(r\\) coefficient\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\nThere is a linear association between the two variables.\nFor a valid hypothesis testing and calculation of confidence intervals both variables should have an approximately normal distribution.\n\nAbsence of outliers in the data set.\n\nPearson’s \\(r\\) coefficient is a dimensionless quantity that takes a value in the range -1 to +1. A positive value indicates that both variables increase (or decrease) together while a negative coefficient indicates that one variable decreases as the other variable increases and vice versa. The stronger the correlation, the closer the correlation coefficient comes to ±1.\n\ncor(BirthWeight$height, BirthWeight$weight)\n\n[1] 0.7127154\n\n\n\n\nThe basic idea of Spearman’s rank correlation is that the ranks of X and Y are obtained by first separately ordering their values from small to large and then computing the correlation between the two sets of ranks. The strength of correlation is denoted by the coefficient of rank correlation, named Spearman’s rank correlation coefficient, \\(r_{s}\\).\nAssumptions for Spearman’s \\(r_{s}\\) coefficient\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\nThere is a monotonic association between the two variables. In a monotonic association, the variables tend to move in the same relative direction, but not necessarily at a constant rate. So all linear correlations are monotonic but the opposite is not always true, because we can have also monotonic non-linear associations.\n\nSpearman’s \\(rho\\) coefficient is dimensionless quantity that take value in the range -1 to +1. A positive correlation coefficient indicates that both variables increase (or decrease) in value together and a negative coefficient indicates that one variable decreases in value as the other variable increases and vice versa. The stronger the correlation, the closer the correlation coefficient comes to ±1.\n\ncor(BirthWeight$height, BirthWeight$weight, method = \"spearman\")\n\n[1] 0.7109399\n\n\n\n\nThe Kendall’s \\(\\tau\\) coefficient is the best alternative to Spearman’s \\(rho\\) correlation when the sample size is small and has many tied ranks. It is used to test the similarities in the ordering of data when it is ranked by quantities. Kendall’s correlation coefficient uses pairs of observations and determines the strength of association based on the patter on concordance and discordance between the pairs.\nAssumptions for Kendall’s \\(\\tau\\) coefficient\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\nThere is a monotonic association between the two variables. In a monotonic association, the variables tend to move in the same relative direction, but not necessarily at a constant rate. So all linear correlations are monotonic but the opposite is not always true, because we can have also monotonic non-linear associations.\n\nKendall’s \\(\\tau\\) coefficient is dimensionless quantity that takes value in the range -1 to +1. A positive correlation coefficient indicates that both variables increase (or decrease) in value together and a negative coefficient indicates that one variable decreases in value as the other variable increases and vice versa. The stronger the correlation, the closer the correlation coefficient comes to ±1.\n\ncor(BirthWeight$height, BirthWeight$weight, method = \"kendal\")\n\n[1] 0.5511955\n\n\n\n\nThe correlation coefficient \\(ξ\\) converges to a limit which has an easy interpretation as a measure of dependence. The limit ranges from 0 to 1. It is 1 if and only if Y is a measurable function of X and 0 if and only if X and Y are independent. Thus, \\(ξ\\) gives an actual measure of the strength of the association and it can be used for non-monotonic associations. However, for monotonic associations, it does not indicate the direction of the association.\nAssumptions for \\(ξ\\) coefficient\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\n\n\nxicor(BirthWeight$height, BirthWeight$weight)\n\n[1] 0.3379234\n\n\n\n\n\n\n\nCorrelation tests\nA correlation test is used to test whether the correlation (denoted ρ) between two numeric variables is significantly different from 0 or not in the population.\n\n\n\n\n\n\nHypothesis Testing for correlation coefficients\n\n\n\n\n\nPearson’s \\(r\\) test\nSpearman’s \\(r_{s}\\) test\nKendal’s \\(\\tau\\) test\nCoefficient \\(ξ\\) test\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\\(H_0\\): There is no linear association between the two numeric variables (they are independent, \\(ρ = 0\\))\n\n\\(H_1\\): There is linear association between the two numeric variables (they are dependent, \\(ρ \\neq 0\\))\n\n\ncor.test(BirthWeight$height, BirthWeight$weight) # the default method is \"pearson\"\n\n\n    Pearson's product-moment correlation\n\ndata:  BirthWeight$height and BirthWeight$weight\nt = 23.785, df = 548, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6689714 0.7515394\nsample estimates:\n      cor \n0.7127154 \n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\\(H_0\\): There is no monotonic association between the two numeric variables (they are independent)\n\n\\(H_1\\): There is monotonic association between the two numeric variables (they are dependent)\n\n\ncor.test(BirthWeight$height, BirthWeight$weight, method = \"spearman\")\n\nWarning in cor.test.default(BirthWeight$height, BirthWeight$weight, method =\n\"spearman\"): Cannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  BirthWeight$height and BirthWeight$weight\nS = 8015368, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.7109399 \n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\\(H_0\\): There is no monotonic association between the two numeric variables (they are independent)\n\n\\(H_1\\): There is monotonic association between the two numeric variables (they are dependent)\n\n\ncor.test(BirthWeight$height, BirthWeight$weight, method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  BirthWeight$height and BirthWeight$weight\nz = 18.34, p-value &lt; 2.2e-16\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.5511955 \n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\\(H_0\\): There is not association between the two numeric variables (they are independent)\n\n\\(H_1\\): There is association between the two numeric variables (they are dependent)\n\n\nxicor(BirthWeight$height, BirthWeight$weight, pvalue = TRUE)\n\n$xi\n[1] 0.3379234\n\n$sd\n[1] 0.02701652\n\n$pval\n[1] 0"
  },
  {
    "objectID": "chi_square.html#research-question-and-hypothesis-testing",
    "href": "chi_square.html#research-question-and-hypothesis-testing",
    "title": "12  Chi-sqaure test of independence",
    "section": "\n12.1 Research question and Hypothesis Testing",
    "text": "12.1 Research question and Hypothesis Testing\nWe will use the “Survival from Malignant Melanoma” dataset named “meldata”. The data consist of measurements made on patients with malignant melanoma, a type of skin cancer. Each patient had their tumor removed by surgery at the Department of Plastic Surgery, University Hospital of Odense, Denmark, between 1962 and 1977.\nSuppose we are interested in the association between tumor ulceration and death from melanoma.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There is no association between the two categorical variables (they are independent)\n\n\\(H_1\\): There is association between the two categorical variables (they are dependent)\n\n\n\nNOTE: In practice, the null hypothesis of independence, for our particular question, is no difference in the proportion of patients with ulcerated tumors who die compared with non-ulcerated tumors (\\(p_{ulcerated} = p_{non-ucerated}\\))."
  },
  {
    "objectID": "chi_square.html#packages-we-need",
    "href": "chi_square.html#packages-we-need",
    "title": "12  Chi-sqaure test of independence",
    "section": "\n12.2 Packages we need",
    "text": "12.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(ggsci)\n\nlibrary(patchwork)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "chi_square.html#preraring-the-data",
    "href": "chi_square.html#preraring-the-data",
    "title": "12  Chi-sqaure test of independence",
    "section": "\n12.3 Preraring the data",
    "text": "12.3 Preraring the data\nWe import the data meldata in R:\n\nlibrary(readxl)\nmeldata &lt;- read_excel(here(\"data\", \"meldata.xlsx\"))\n\n\n\n\nFigure 12.1: Table with data from “meldata” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(meldata)\n\nRows: 205\nColumns: 7\n$ time      &lt;dbl&gt; 10, 30, 35, 99, 185, 204, 210, 232, 232, 279, 295, 355, 386,…\n$ status    &lt;chr&gt; \"Alive\", \"Alive\", \"Alive\", \"Alive\", \"Died\", \"Died\", \"Died\", …\n$ sex       &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"F…\n$ age       &lt;dbl&gt; 76, 56, 41, 71, 52, 28, 77, 60, 49, 68, 53, 64, 68, 63, 14, …\n$ year      &lt;dbl&gt; 1972, 1968, 1977, 1968, 1965, 1971, 1972, 1974, 1968, 1971, …\n$ thickness &lt;dbl&gt; 6.76, 0.65, 1.34, 2.90, 12.08, 4.84, 5.16, 3.22, 12.88, 7.41…\n$ ulcer     &lt;chr&gt; \"Present\", \"Absent\", \"Absent\", \"Absent\", \"Present\", \"Present…\n\n\nThe data set meldata has 250 patients (rows) and includes seven variables (columns). We are interested in the character (&lt;chr&gt;) ulcer variable and the character (&lt;chr&gt;) status variable which should be converted to factor (&lt;fct&gt;) variables using the convert_as_factor() function as follows:\n\nmeldata &lt;- meldata %&gt;%\n  convert_as_factor(status, ulcer)\n\nglimpse(meldata)\n\nRows: 205\nColumns: 7\n$ time      &lt;dbl&gt; 10, 30, 35, 99, 185, 204, 210, 232, 232, 279, 295, 355, 386,…\n$ status    &lt;fct&gt; Alive, Alive, Alive, Alive, Died, Died, Died, Alive, Died, D…\n$ sex       &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"F…\n$ age       &lt;dbl&gt; 76, 56, 41, 71, 52, 28, 77, 60, 49, 68, 53, 64, 68, 63, 14, …\n$ year      &lt;dbl&gt; 1972, 1968, 1977, 1968, 1965, 1971, 1972, 1974, 1968, 1971, …\n$ thickness &lt;dbl&gt; 6.76, 0.65, 1.34, 2.90, 12.08, 4.84, 5.16, 3.22, 12.88, 7.41…\n$ ulcer     &lt;fct&gt; Present, Absent, Absent, Absent, Present, Present, Present, …"
  },
  {
    "objectID": "chi_square.html#plot-the-data",
    "href": "chi_square.html#plot-the-data",
    "title": "12  Chi-sqaure test of independence",
    "section": "\n12.4 Plot the data",
    "text": "12.4 Plot the data\nWe are interested in the association between tumor ulceration and death from melanoma. It is useful to plot the data as counts but also as percentages. It is percentages we are comparing, but we really want to know the absolute numbers as well.\n\np1 &lt;- meldata %&gt;%\n  ggplot(aes(x = ulcer, fill = status)) +\n  geom_bar(width = 0.7) +\n  scale_fill_jco() +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"bottom\")\n\n\np2 &lt;- meldata %&gt;%\n  ggplot(aes(x = ulcer, fill = status)) +\n  geom_bar(position = \"fill\", width = 0.7) +\n  scale_y_continuous(labels=scales::percent) +\n  scale_fill_jco() +\n  ylab(\"Percentage\") +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"bottom\") \n\np1 + p2 + \n  plot_layout(guides = \"collect\") & theme(legend.position = 'bottom')\n\n\n\nBar plot.\n\n\n\n\nJust from the plot, death from melanoma in the ulcerated tumor group is around 40% and in the non-ulcerated group around 13%. The number of patients included in the study is not huge, however, this still looks like a real difference given its effect size."
  },
  {
    "objectID": "chi_square.html#contigency-table-and-expected-frequencies",
    "href": "chi_square.html#contigency-table-and-expected-frequencies",
    "title": "12  Chi-sqaure test of independence",
    "section": "\n12.5 Contigency table and Expected frequencies",
    "text": "12.5 Contigency table and Expected frequencies\nFirst, we will create a contingency 2x2 table (two categorical variables with exactly two levels each) with the frequencies using the Base R.\n\ntb1 &lt;- table(meldata$ulcer, meldata$status)\ntb1\n\n         \n          Alive Died\n  Absent     99   16\n  Present    49   41\n\n\nNext, we will also create a more informative table with row percentages and marginal totals.\n\n\n\n\n\n\nTable with row percentages and marginal totals\n\n\n\n\n\nfinalfit\nmodelsummary\n\n\n\nUsing the function summary_factorlist() which is included in finalfit package for obtaining row percentages and marginal totals:\n\nrow_tb1 &lt;- meldata %&gt;%\n  finalfit::summary_factorlist(dependent = \"status\", add_dependent_label = T,\n                     explanatory = \"ulcer\", add_col_totals = T,\n                     include_col_totals_percent = F,\n                     column = FALSE, total_col = TRUE)\n\nknitr::kable(row_tb1) \n\n\n\nDependent: status\n\nAlive\nDied\nTotal\n\n\n\nTotal N\n\n148\n57\n205\n\n\nulcer\nAbsent\n99 (86.1)\n16 (13.9)\n115 (100)\n\n\n\nPresent\n49 (54.4)\n41 (45.6)\n90 (100)\n\n\n\n\n\n\n\nThe contingency table using the datasummary_crosstab() from the modelsummary package:\n\nmodelsummary::datasummary_crosstab(ulcer ~ status, data = meldata)\n\n\n\nulcer\n\nAlive\nDied\nAll\n\n\n\nAbsent\nN\n99\n16\n115\n\n\n\n% row\n86.1\n13.9\n100.0\n\n\nPresent\nN\n49\n41\n90\n\n\n\n% row\n54.4\n45.6\n100.0\n\n\nAll\nN\n148\n57\n205\n\n\n\n% row\n72.2\n27.8\n100.0\n\n\n\n\n\n\n\n\n\n\nFrom the raw frequencies, there seems to be a large difference, as we noted in the plot we made above. The proportion of patients with ulcerated tumors who die equals to 45.6% compared with non-ulcerated tumors 13.9%."
  },
  {
    "objectID": "chi_square.html#assumptions",
    "href": "chi_square.html#assumptions",
    "title": "12  Chi-sqaure test of independence",
    "section": "\n12.6 Assumptions",
    "text": "12.6 Assumptions\n\n\n\n\n\n\nCheck if the following assumption is satisfied\n\n\n\nA commonly stated assumption of the chi-square test is the requirement to have an expected count of at least 5 in each cell of the 2x2 table.\nFor larger tables, all expected counts should be &gt; 1 and no more than 20% of all cells should have expected counts &lt; 5.\n\n\nWe can calculate the expected frequencies for each cell using the expected() function from {epitools} package:\n\nepitools::expected(tb1)\n\n         \n             Alive     Died\n  Absent  83.02439 31.97561\n  Present 64.97561 25.02439\n\n\nHere, as we observe the assumption is fulfilled."
  },
  {
    "objectID": "chi_square.html#run-pearsons-chi-square-test",
    "href": "chi_square.html#run-pearsons-chi-square-test",
    "title": "12  Chi-sqaure test of independence",
    "section": "\n12.7 Run Pearson’s chi-square test",
    "text": "12.7 Run Pearson’s chi-square test\nFinally, we run the chi-square test:\n\n\n\n\n\n\nchi-square test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nchisq.test(tb1)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tb1\nX-squared = 23.631, df = 1, p-value = 1.167e-06\n\n\n\n\n\nchisq_test(tb1)\n\n# A tibble: 1 × 6\n      n statistic          p    df method          p.signif\n* &lt;int&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;           &lt;chr&gt;   \n1   205      23.6 0.00000117     1 Chi-square test ****    \n\n\n\n\n\n\n\nThere is evidence for an association between the ulcer and status (reject \\(H_0\\)). The proportion of patients with ulcerated tumors who died (45.6%) is significant larger compared with non-ulcerated tumors (13.9%) (p&lt;0.001)."
  },
  {
    "objectID": "chi_square.html#risk-ratio-and-odds-ratio",
    "href": "chi_square.html#risk-ratio-and-odds-ratio",
    "title": "12  Chi-sqaure test of independence",
    "section": "\n12.8 Risk Ratio and Odds ratio",
    "text": "12.8 Risk Ratio and Odds ratio\nRisk ratio\nFrom the data in the following table\n\nepitools::table.margins(tb1)\n\n         \n          Alive Died Total\n  Absent     99   16   115\n  Present    49   41    90\n  Total     148   57   205\n\n\nwe can calculate the risk ratio by hand: \\[ Risk \\ Ratio = \\frac{\\frac{41}{90}}{\\frac{16}{115}} =\\frac{0.4556}{0.1391} = 3.27\\]\nThe risk ratio with the 95% CI using R:\n\nepitools::riskratio(tb1)$measure\n\n         risk ratio with 95% C.I.\n          estimate    lower    upper\n  Absent  1.000000       NA       NA\n  Present 3.274306 1.970852 5.439819\n\n\nThe risk of dying is 3.27 (95% CI: 1.97, 5.4) times higher for patients with ulcerated tumors compared to non-ulcerated tumors.\nOdds ratio\nWe can also calculate the odds ratio by hand: \\[ Odds \\ Ratio = \\frac{\\frac{41}{49}}{\\frac{16}{99}} =\\frac{0.837}{0.162} = 5.17\\] The odds ratio with the 95% CI using R:\n\nepitools::oddsratio(tb1, method = \"wald\")$measure\n\n         odds ratio with 95% C.I.\n          estimate    lower   upper\n  Absent  1.000000       NA      NA\n  Present 5.177296 2.645152 10.1334\n\n\nThe odds of dying is 5.17 (95% CI: 2.65, 10.13) times higher for patients with ulcerated tumors compared to non-ulcerated tumors patients.\nFinnaly, we can also reverse the odds ratio: \\[ \\frac{1}{OR} = \\frac{1}{5.17} = 0.193\\]\n\nepitools::oddsratio(tb1, method = \"wald\", rev = \"rows\")$measure\n\n         odds ratio with 95% C.I.\n          estimate      lower   upper\n  Present 1.000000         NA      NA\n  Absent  0.193151 0.09868354 0.37805\n\n\nThe non-ulcerated tumors patients has 0.193 (95% CI: 0.098, 0.378) times the odds (of dying) of the ulcerated tumors. This means that the non-ulcerated tumors patients has (0.193 - 1= -0.807) 80.7% lower odds of dying than ulcerated tumors."
  },
  {
    "objectID": "fisher_exact.html#research-question-and-hypothesis-testing",
    "href": "fisher_exact.html#research-question-and-hypothesis-testing",
    "title": "13  Fisher’s exact test",
    "section": "\n13.1 Research question and Hypothesis Testing",
    "text": "13.1 Research question and Hypothesis Testing\nWe consider the data in hemophilia dataset. In a survey there are two treatment regimens studied for controlling bleeding in 28 patients with hemophilia undergoing surgery. We want to investigate if there is an association between the treatment regimen (treatment A or B) and the bleeding complications (no or yes). The null hypothesis (\\(H_0\\)) is that the bleeding complications are independent from the treatment regimen, while the alternative (\\(H_1\\)) is that are dependent.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There is no association between the two categorical variables (they are independent)\n\n\\(H_1\\): There is association between the two categorical variables (they are dependent)\n\n\n\nNOTE: In practice, the null hypothesis of independence, for our particular question, is no difference in the proportion of patients with bleeding complications compared with patients with no bleeding complications (\\(p_{bleeding} = p_{no bleeding}\\))."
  },
  {
    "objectID": "fisher_exact.html#packages-we-need",
    "href": "fisher_exact.html#packages-we-need",
    "title": "13  Fisher’s exact test",
    "section": "\n13.2 Packages we need",
    "text": "13.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\n\n\nlibrary(patchwork)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "fisher_exact.html#preraring-the-data",
    "href": "fisher_exact.html#preraring-the-data",
    "title": "13  Fisher’s exact test",
    "section": "\n13.3 Preraring the data",
    "text": "13.3 Preraring the data\nWe import the data meldata in R:\n\nlibrary(readxl)\nhemophilia &lt;- read_excel(here(\"data\", \"hemophilia.xlsx\"))\n\n\n\n\nFigure 13.1: Table with data from “meldata” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(hemophilia)\n\nRows: 28\nColumns: 2\n$ treatment &lt;chr&gt; \"A\", \"A\", \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"A\", \"A\", \"B\", \"A\", …\n$ bleeding  &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\"…\n\n\nThe dataset hemophilia has 28 patients (rows) and includes 2 variables (columns), the character (&lt;chr&gt;) variable named treatment and the character (&lt;chr&gt;) variable named bleeding. Both of them should be converted to factor (&lt;fct&gt;) variables using the convert_as_factor() function as follows:\n\nhemophilia &lt;- hemophilia %&gt;%\n  convert_as_factor(treatment, bleeding)\n\nglimpse(hemophilia)\n\nRows: 28\nColumns: 2\n$ treatment &lt;fct&gt; A, A, A, B, A, B, B, A, A, A, B, A, B, B, A, A, B, B, B, A, …\n$ bleeding  &lt;fct&gt; no, no, no, yes, no, no, no, no, yes, no, no, no, yes, no, n…"
  },
  {
    "objectID": "fisher_exact.html#plot-the-data",
    "href": "fisher_exact.html#plot-the-data",
    "title": "13  Fisher’s exact test",
    "section": "\n13.4 Plot the data",
    "text": "13.4 Plot the data\nWe count the number of patients with bleeding in the two regimens. It is useful to plot this as counts but also as percentages and compare them.\n\np3 &lt;- hemophilia %&gt;%\n  ggplot(aes(x = treatment, fill = bleeding)) +\n  geom_bar(width = 0.7) +\n  scale_fill_jama() +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"bottom\")\n\n\np4 &lt;- hemophilia %&gt;%\n  ggplot(aes(x = treatment, fill = bleeding)) +\n  geom_bar(position = \"fill\", width = 0.7) +\n  scale_y_continuous(labels=scales::percent) +\n  scale_fill_jama() +\n  ylab(\"Percentage\") +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"bottom\") \n\np3 + p4 + \n  plot_layout(guides = \"collect\") & theme(legend.position = 'bottom')\n\n\n\nBar plot.\n\n\n\n\nThe above bar plots with counts show graphically that the number of patients who had bleeding complications was similar in the two regimens. Note that the number of patients included in the study is small (n=28)."
  },
  {
    "objectID": "fisher_exact.html#contigency-table-and-expected-frequencies",
    "href": "fisher_exact.html#contigency-table-and-expected-frequencies",
    "title": "13  Fisher’s exact test",
    "section": "\n13.5 Contigency table and Expected frequencies",
    "text": "13.5 Contigency table and Expected frequencies\nFirst, we will create a contingency 2x2 table (two categorical variables with exactly two levels each) with the frequencies using the Base R.\n\ntb2 &lt;- table(hemophilia$treatment, hemophilia$bleeding)\ntb2\n\n   \n    no yes\n  A 13   2\n  B 10   3\n\n\nNext, we will also create a more informative table with row percentages and marginal totals.\n\n\n\n\n\n\nTable with row percentages and marginal totals\n\n\n\n\n\nfinalfit\nmodelsummary\n\n\n\nUsing the function summary_factorlist() which is included in finalfit package for obtaining row percentages and marginal totals:\n\nrow_tb2 &lt;- hemophilia %&gt;%\n  finalfit::summary_factorlist(dependent = \"bleeding\", add_dependent_label = T,\n                     explanatory = \"treatment\", add_col_totals = T,\n                     include_col_totals_percent = F,\n                     column = FALSE, total_col = TRUE)\n\nknitr::kable(row_tb2) \n\n\n\nDependent: bleeding\n\nno\nyes\nTotal\n\n\n\nTotal N\n\n23\n5\n28\n\n\ntreatment\nA\n13 (86.7)\n2 (13.3)\n15 (100)\n\n\n\nB\n10 (76.9)\n3 (23.1)\n13 (100)\n\n\n\n\n\n\n\nThe contingency table using the datasummary_crosstab() from the modelsummary package:\n\nmodelsummary::datasummary_crosstab(treatment ~ bleeding, data = hemophilia)\n\n\n\ntreatment\n\nno\nyes\nAll\n\n\n\nA\nN\n13\n2\n15\n\n\n\n% row\n86.7\n13.3\n100.0\n\n\nB\nN\n10\n3\n13\n\n\n\n% row\n76.9\n23.1\n100.0\n\n\nAll\nN\n23\n5\n28\n\n\n\n% row\n82.1\n17.9\n100.0\n\n\n\n\n\n\n\n\n\n\nFrom the row frequencies, there is not actually difference, as we noted in the plot we made above.\nNow, we will calculate the expected frequencies for each cell using the expected() function from {epitools} package:\n\nepitools::expected(tb2)\n\n   \n          no      yes\n  A 12.32143 2.678571\n  B 10.67857 2.321429\n\n\nIn the above table there are 2 cells (50%) with expected counts less than 5 (specifically 2.67 and 2.32), so the Chi-square test is not the appropriate one. In this case the Fisher’s exact test should be used instead."
  },
  {
    "objectID": "fisher_exact.html#run-fishers-exact-test",
    "href": "fisher_exact.html#run-fishers-exact-test",
    "title": "13  Fisher’s exact test",
    "section": "\n13.6 Run Fisher’s exact test",
    "text": "13.6 Run Fisher’s exact test\nFinally, we run the Fisher’s exact test:\n\n\n\n\n\n\nFisher’s exact test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nfisher.test(tb2)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  tb2\np-value = 0.6389\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.1807204 26.9478788\nsample estimates:\nodds ratio \n   1.90363 \n\n\n\n\n\nfisher_test(tb2)\n\n# A tibble: 1 × 3\n      n     p p.signif\n* &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   \n1    28 0.639 ns      \n\n\n\n\n\n\n\nThe p = 0.64 is higher than 0.05. There is absence of evidence for an association between the treatment regimens and bleeding complications (failed to reject \\(H_0\\))."
  },
  {
    "objectID": "fisher_exact.html#having-only-the-counts",
    "href": "fisher_exact.html#having-only-the-counts",
    "title": "13  Fisher’s exact test",
    "section": "\n13.7 Having only the counts",
    "text": "13.7 Having only the counts\nWhen we read an article which reports a chi-square or a fisher exact analysis we will see only the counts in a table without having the raw data of the categorical variables. In this instance, we can create the table using the matrix() function and run the tests. For our example of hemophilia we have the following table:\n\ndat &lt;- c(13, 10, 2, 3)\nmx &lt;- matrix(dat, nrow = 2, dimnames = list(c(\"A\", \"B\"), c(\"no\", \"yes\")))\nmx\n\n  no yes\nA 13   2\nB 10   3"
  },
  {
    "objectID": "mcnemar.html#research-question-and-hypothesis-testing",
    "href": "mcnemar.html#research-question-and-hypothesis-testing",
    "title": "14  McNemar’s test",
    "section": "\n14.1 Research question and Hypothesis Testing",
    "text": "14.1 Research question and Hypothesis Testing\nWe consider the data in asthma dataset. The dataset contains data from a survey of 86 children with asthma who attended a camp to learn how to self-manage their asthmatic episodes. The children were asked whether they knew (yes or not) how to manage their asthmatic episodes appropriately at both the start and completion of the camp.\nIn other words, was a significant change in children’s knowledge of asthma management between the beginning and completion of the health camp?\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There was no change in children’s knowledge of asthma management between the beginning and completion of the health camp\n\n\\(H_1\\): There was change in children’s knowledge of asthma management between the beginning and completion of the health camp"
  },
  {
    "objectID": "mcnemar.html#packages-we-need",
    "href": "mcnemar.html#packages-we-need",
    "title": "14  McNemar’s test",
    "section": "\n14.2 Packages we need",
    "text": "14.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(janitor)\nlibrary(modelsummary)\nlibrary(exact2x2)\n\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "mcnemar.html#preraring-the-data",
    "href": "mcnemar.html#preraring-the-data",
    "title": "14  McNemar’s test",
    "section": "\n14.3 Preraring the data",
    "text": "14.3 Preraring the data\nWe import the data asthma in R:\n\nlibrary(readxl)\nasthma &lt;- read_excel(here(\"data\", \"asthma.xlsx\"))\n\n\n\n\nFigure 14.1: Table with data from “asthma” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(asthma)\n\nRows: 86\nColumns: 2\n$ know_begin &lt;chr&gt; \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"y…\n$ know_end   &lt;chr&gt; \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"…\n\n\nThe dataset asthma includes 86 children with asthma (rows) and 2 columns, the character (&lt;chr&gt;) know_begin and the character (&lt;chr&gt;) know_end. Therefore, we consider the dichotomous dependent variable asthma knowledge (yes/no) between two time points, know_begin and know_end.\nBoth measurements know_begin and know_end should be converted to factors (&lt;fct&gt;) using the convert_as_factor() function as follows:\n\nasthma &lt;- asthma %&gt;%\n  convert_as_factor(know_begin, know_end)\n\nglimpse(asthma)\n\nRows: 86\nColumns: 2\n$ know_begin &lt;fct&gt; yes, no, yes, no, no, no, yes, no, no, yes, no, no, yes, ye…\n$ know_end   &lt;fct&gt; yes, no, no, no, no, no, yes, yes, yes, yes, yes, no, yes, …"
  },
  {
    "objectID": "mcnemar.html#contigency-table",
    "href": "mcnemar.html#contigency-table",
    "title": "14  McNemar’s test",
    "section": "\n14.4 Contigency table",
    "text": "14.4 Contigency table\nWe can obtain the cross-tabulation table of the two measurements for the children’s knowledge of asthma:\n\ntb3 &lt;- table(know_begin = asthma$know_begin, know_end = asthma$know_end)\ntb3\n\n          know_end\nknow_begin no yes\n       no  27  29\n       yes  6  24\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThere is a basic difference between this table and the more common two-way table. In this case, the count represents the number of pairs, not the number of individuals.\n\n\nWe want to compare the proportion of children’s knowledge of asthma management at the beginning with the proportion of children’s knowledge of asthma management at the end. We can create a more informative table using the functions from janitor package for obtaining total percentages and marginal totals.\n\n\n\n\n\n\nTable with total percentages and marginal totals\n\n\n\n\n\njanitor\nmodelsummary\n\n\n\nWe can create an informative table using the functions from janitor package for obtaining total percentages and marginal totals:\n\ntotal_tb2 &lt;- asthma %&gt;%\n  tabyl(know_begin, know_end) %&gt;%\n  adorn_totals(c(\"row\", \"col\")) %&gt;%\n  adorn_percentages(\"all\") %&gt;%\n  adorn_pct_formatting(digits = 1) %&gt;%\n  adorn_ns %&gt;%\n  adorn_title\n\nknitr::kable(total_tb2) \n\n\n\n\nknow_end\n\n\n\n\n\nknow_begin\nno\nyes\nTotal\n\n\nno\n31.4% (27)\n33.7% (29)\n65.1% (56)\n\n\nyes\n7.0% (6)\n27.9% (24)\n34.9% (30)\n\n\nTotal\n38.4% (33)\n61.6% (53)\n100.0% (86)\n\n\n\n\n\n\n\nThe contingency table using the datasummary_crosstab() from the modelsummary package:\n\nmodelsummary::datasummary_crosstab(know_begin ~ know_end, \n                     statistic = 1 ~ 1 + N + Percent(), \n                     data = asthma)\n\n\n\nknow_begin\n\nno\nyes\nAll\n\n\n\nno\nN\n27\n29\n56\n\n\n\n%\n31.4\n33.7\n65.1\n\n\nyes\nN\n6\n24\n30\n\n\n\n%\n7.0\n27.9\n34.9\n\n\nAll\nN\n33\n53\n86\n\n\n\n%\n38.4\n61.6\n100.0\n\n\n\n\n\n\n\n\n\n\nThe proportion of children who knew to manage asthma at the beginning is (6+24)/86= 30/86 = 0.349 or 34.9%. The proportion of children who knew to mange asthma at the end is (29+24)/86 = 53/86 = 0.616 or 61.6%.\n\n\n\n\n\n\nAssumption\n\n\n\nThe basic assumption of the test is that the sum of the discordant cells should be larger than 25 (that is fulfilled in our example)."
  },
  {
    "objectID": "mcnemar.html#run-mcnemars-test",
    "href": "mcnemar.html#run-mcnemars-test",
    "title": "14  McNemar’s test",
    "section": "\n14.5 Run McNemar’s test",
    "text": "14.5 Run McNemar’s test\nFinally, we run the McNemar’s test:\n\n\n\n\n\n\nMcNemar’s test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nmcnemar.test(tb3)\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  tb3\nMcNemar's chi-squared = 13.829, df = 1, p-value = 0.0002003\n\n\n\n\n\nmcnemar_test(tb3)\n\n# A tibble: 1 × 6\n      n statistic    df      p p.signif method      \n* &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;       \n1    86      13.8     1 0.0002 ***      McNemar test\n\n\n\n\n\n\n\nThe proportion of children who knew to manage asthma at the end (61.6%) is significant larger compared with the proportion of children who knew to manage asthma at the beginning (34.9%) (p-value &lt;0.001)."
  },
  {
    "objectID": "mcnemar.html#exact-binomial-test",
    "href": "mcnemar.html#exact-binomial-test",
    "title": "14  McNemar’s test",
    "section": "\n14.6 Exact binomial test",
    "text": "14.6 Exact binomial test\nExact binomial test for 2x2 table when the sum of the discordant cells are less than 25:\n\nmcnemar.exact(tb3)\n\n\n    Exact McNemar test (with central confidence intervals)\n\ndata:  tb3\nb = 29, c = 6, p-value = 0.0001168\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  1.971783 14.238838\nsample estimates:\nodds ratio \n  4.833333"
  }
]