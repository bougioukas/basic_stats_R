[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Practical Statistics in Medicine with R",
    "section": "",
    "text": "Preface\n\nWhom is this textbook for?\nThis textbook is based on my notes from a series of lectures given for a few years at the Aristotle University of Thessaloniki, Greece. The textbook can be used as support material for practical labs on basic statistics using R at any level from beginner to advanced. It can also be used as a support for self-teaching.\nI have paid particular attention to the form of the book, which I think should aid understanding the most common statistical tests using Base R and pipe-friendly functions, coherent with the ‘tidyverse’ design philosophy. The {ggplot2} package and many ggplot2 extensions are the preferred tools of choice for constructing data visualizations in the textbook.\n\n\n\n\n\n\nTip\n\n\n\nThis textbook assumes that the reader has a basic knowledge of R, tidyverse and introductory statistics. If you want to familiarize yourself with this programming language and statistics, we recommend the books below:\n\nUsing R for Introductory Statistics\nR for Data Science\nggplot2: Elegant Graphics for Data Analysis\n\n\n\n \n\n\nR via RStudio and RStudio Projects\nThroughout this textbook we will use R via RStudio. Both programs can be downloaded from  posit. It is also recommended to work with RStudio Projects. This enables to organize our files and switch between different projects without getting the data, scripts, or output files all mixed up. Everything gets read in or saved to the right folder/directory.\nFor our purpose, it is sufficient to consider a simple RStudio Project folder that contains the following subfolders (Figure 1).:\n\ndata: data files of any kind, such as .csv, .xlsx, .txt, etc.\nfigures: plots, diagrams, and other graphs\n\n\n\n\nFigure 1: The folder structure of a minimal RStudio project.\n\n\n\n\nReproducibility and License\nAll sections of this textbook are reproducible as they were made using  Quarto® which is an open-source scientific and technical publishing system built on Pandoc.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\nThe online version of the textbook is free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License."
  },
  {
    "objectID": "introduction.html#the-discipline-of-statistics",
    "href": "introduction.html#the-discipline-of-statistics",
    "title": "1  Introduction",
    "section": "1.1 The discipline of statistics",
    "text": "1.1 The discipline of statistics\nStatistics is an empirical or practical method for collecting, organizing, summarizing, and presenting data, and for making inferences about the population from which the data are drawn.\nThe discipline of traditional (frequentist) statistics includes two main branches (Figure 1.1):\n\ndescriptive statistics that includes measures of frequency and measures of location and dispersion. It also includes a description of the general shape of the distribution of the data.\ninferential statistics that aims at generalizing conclusions made on a sample to a whole population. It includes estimation and hypothesis testing.\n\n\n\n\n\n\nflowchart LR\n  \n    A[Traditional &lt;br/&gt; Statistics]--- B[Descriptive statistics]\n    A --- C[Inferential statistics]\n    B --- D[Measures of frequency: &lt;br/&gt; e.g., frequency, percentage.]\n    B --- E[Measures of location &lt;br/&gt; and dispersion: &lt;br/&gt; e.g., mean, standard deviation.]\n    C --- H[Estimation]\n    C --- I[Hypothesis Testing]\n    style A color:#980000, stroke:#333,stroke-width:4px\n    \n\n\nFigure 1.1: The discipline of statistics and its two branches, descriptive statistics and inferential statistics"
  },
  {
    "objectID": "introduction.html#data-and-variables",
    "href": "introduction.html#data-and-variables",
    "title": "1  Introduction",
    "section": "1.2 Data and Variables",
    "text": "1.2 Data and Variables\n\nBiomedical Data\nBiomedical data have unique features compared with data in other domains. The data may include administrative health data, biomarker data, biometric data (for example, from wearable technologies) and imaging, and may originate from many different sources, including Electronic Health Records (EHRs), surveillance systems, clinical registries, biobanks, the internet (e.g. social media) and patient self-reports.\nBiomedical data can be transformed into information. This information can become knowledge if the researchers and clinicians understand it (Figure 1.2).\n\n\n\nFigure 1.2: From data to knowledge.\n\n\nThere are three main data structures: structured data, unstructured data, and semi-structured data.\nStructured data is generally tabular data that is represented by columns and rows in a database.\nSemi-Structured data is a form of structured data that does not obey the tabular structure, yet does have some structural properties. Emails, for example, are semi-structured by sender, recipient,subject, date, time etc. and they are also organized into folders, like Inbox, Sent, Trash, etc.\nUnstructured data usually open text (such as social media posts), images, videos, etc., that have no predetermined organization or design.\nIn this textbook we use data organized in a structured format (spreadsheets). In statistics, tabular data refers to data that are organized in a table with rows and columns. A row is a observation (or record), which corresponds to the statistical unit of the dataset. The columns are the variables (or characteristics) of interest.\n \n\n\nVariables\nA variable is a quantity or characteristic that is free to vary, or take on different values. To gain information on variables and their associations, it is necessary to design and conduct scientific experiments.\nResearchers design experiments to test if changes to one or more variables are associated with changes to another variable of interest. For example, if researchers hypothesize that a new therapy is more effective than the usual care for migraine pain, they could design an experiment to test this hypothesis; participants should randomly assigned to one of two groups: the experimental group receiving the new treatment that is being tested, and the control group receiving an conventional treatment. In this experiment, the type of treatment each participant received (i.e., new treatment vs. conventional treatment) is the independent variable (IV), while the pain relief is the dependent variable (DV) or the outcome variable.\n\n\n\n\n\n\nIndependent Vs Dependent variables\n\n\n\nAn independent variable is the variable that is changed or controlled in a scientific experiment to test the effects on another variable.\nA dependent (outcome) variable is the variable being tested in a scientific experiment and is affected by the independent variable(s) of interest."
  },
  {
    "objectID": "introduction.html#types-of-data",
    "href": "introduction.html#types-of-data",
    "title": "1  Introduction",
    "section": "1.3 Types of Data",
    "text": "1.3 Types of Data\nData in variables can be either categorical or numerical (otherwise known as qualitative and quantitative) in nature (Figure 1.3).\n\n\n\n\n\nflowchart TB\n    A[Data in variables]---&gt; B[Categorical data]\n    A[Data in variables]---&gt; C[Numerical data]\n    B ---&gt; E[Nominal&lt;br&gt;e.g. blood type A, B, AB, O]\n    B ---&gt; F[Ordinal&lt;br&gt;e.g. degree of pain&lt;br&gt;minimal/moderate/&lt;br&gt;severe/unbearable]\n    C ---&gt; G[Discrete&lt;br&gt;e.g. number of children]\n    C ---&gt; H[Numerical&lt;br&gt;e.g. height, blood pressure]\n    \n   style E color:#980000, stroke:#333,stroke-width:4px\n   style F color:#980000, stroke:#333,stroke-width:4px\n   style G color:#980000, stroke:#333,stroke-width:4px\n   style H color:#980000, stroke:#333,stroke-width:4px\n\n\nFigure 1.3: Broad classification of the different types of data with examples.\n\n\n\n\n \n\n\n\n\n\n\nNote\n\n\n\nThe degree of measurement accuracy and the type of data are both important factors in the decision to perform a statistical analysis.\n\n\n \n\nCategorical Data\nA. Nominal Data\nNominal data can be labeled creating distinct unordered categories. They are not measured but simply counted. They can be either binary such as dead/alive; cured/not cured or have more than two categories, for example, blood group A, B, AB, O; type I, type II or gestational diabetes; eye color (e.g., brown, blue, green, gray).\n\n\n\n\n\n\nNumerical representation of categories are just codes\n\n\n\nWe can denote dead/alive as 1/0 for health status and denote A/B/AB/O as 1/2/3/4 for blood type. Unlike numerical data, the numbers representing different categories do not have mathematical meaning (they are just codes).\n\n\n \nΒ. Ordinal Data\nWhen the categories can be ordered, the data are of ordinal type. For example, patients may classify their degree of pain as minimal, moderate, severe, or unbearable. In this case, there is a natural order of the values, since moderate pain is more intense than minimal and less than severe pain.\n\n\n\n\n\n\nCollapsion of categories leads to a loss of information\n\n\n\nOrdinal data are often transformed into binary data to simplify analysis, presentation and interpretation, which may result in a considerable loss of information.\n\n\n \n\n\nNumerical Data\nA. Discrete Data\nDiscrete data can take only a finite number of values (usually integers) in a range, for example, the number of children in a family or the number of days missed from work. Other examples are often counts per unit of time such as the number of deaths in a hospital per year, the number of visits to the general practitioner in a year, or the number of epileptic seizures a patient has per month. In dentistry, a common measure is the number of decayed, filled or missing teeth.\n\n\n\n\n\n\nDiscrete Vs Ordinal data\n\n\n\nIn practice discrete data are often treated in statistical analyses as if they were ordinal data. Although this may be acceptable, we do not take the most out of our data.\n\n\n \nΒ. Continuous Data\nContinuous data are numbers (usually with units) that can take any value within a given range. However, in practice, they are restricted by the accuracy of the measuring instrument. Height, weight, blood pressure, cholesterol level, body temperature, body mass index (BMI) are just few examples of variables that take continuous values.\n\n\n\n\n\n\nCategorization of numerical data leads to a loss of information\n\n\n\nContinuous data are often categorized creating categorical variables. For example, the BMI, which is a continuous variable, is usually converted into an ordinal variable with four categories (underweight, normal, overweight and obese). However, dividing continuous variables into categories leads to a loss of information."
  },
  {
    "objectID": "descriptive.html#packages-we-need",
    "href": "descriptive.html#packages-we-need",
    "title": "2  Descriptive statistics",
    "section": "2.1 Packages we need",
    "text": "2.1 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(EnvStats)\nlibrary(modeest)\nlibrary(scales)\nlibrary(questionr)\nlibrary(MESS)\nlibrary(ggsci)\nlibrary(ggdist)\nlibrary(ggrain)\nlibrary(patchwork)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "descriptive.html#importing-the-data",
    "href": "descriptive.html#importing-the-data",
    "title": "2  Descriptive statistics",
    "section": "2.2 Importing the data",
    "text": "2.2 Importing the data\nWe will use the dataset named arrhythmia which is a .xlsx file. It is supposed that we work with RStudio Projects and the dataset is stored in the subfolder with the name “data” inside the RStudio Project folder. If this is the case, we can read the data using a relative path with the following code:\n\n\nNOTE: The path of a file/directory is its location/address in the file system. There are two kinds of paths: absolute path such as “C:/My_name/../my_project/data/arrhythmia.xlsx” and relative path such as “data/arrhythmia.xlsx”.\n \nThe function here() allows us to navigate throughout each of the subfolders and files within a given RStudio Project using relative paths .\n\nlibrary(readxl)\narrhythmia &lt;- read_excel(here(\"data\", \"arrhythmia.xlsx\"))\n\n\n\n\n\n\n\nFigure 2.1: Table with raw data of arrhythmia data set.\n\n\n\nWe take a look at the data:\n\nglimpse(arrhythmia)\n\nRows: 428\nColumns: 8\n$ age     &lt;dbl&gt; 75, 56, 54, 55, NA, 40, 49, 44, 50, 62, 45, 54, 30, 44, 47, 47…\n$ sex     &lt;chr&gt; \"male\", \"female\", \"male\", \"male\", \"male\", \"female\", \"female\", …\n$ height  &lt;dbl&gt; 190, 165, 172, 175, 190, 160, 162, 168, 167, 170, 165, 172, 17…\n$ weight  &lt;dbl&gt; 80, 64, 95, 94, 80, 52, 54, 56, 67, 72, 86, 58, 73, 88, 48, 59…\n$ QRS     &lt;dbl&gt; 91, 81, 138, 115, 88, 77, 78, 84, 89, 152, 77, 78, 133, 77, 75…\n$ HR      &lt;dbl&gt; 63, 53, 75, 71, 75, 70, 67, 64, 63, 70, 72, 73, 56, 72, 76, 67…\n$ bmi     &lt;dbl&gt; 22.2, 23.5, 32.1, 30.7, 22.2, 20.3, 20.6, 19.8, 24.0, 24.9, 31…\n$ bmi_cat &lt;chr&gt; \"normal\", \"normal\", \"obese\", \"obese\", \"normal\", \"normal\", \"nor…\n\n\nAdditionally, we can get some basic summary measures for each variable:\n\nsummary(arrhythmia)\n\n      age            sex                height        weight     \n Min.   :18.00   Length:428         Min.   :146   Min.   : 18.0  \n 1st Qu.:38.00   Class :character   1st Qu.:160   1st Qu.: 60.0  \n Median :48.00   Mode  :character   Median :165   Median : 70.0  \n Mean   :48.67                      Mean   :165   Mean   : 70.1  \n 3rd Qu.:59.00                      3rd Qu.:170   3rd Qu.: 80.0  \n Max.   :83.00                      Max.   :190   Max.   :176.0  \n NA's   :3                                                       \n      QRS               HR              bmi          bmi_cat         \n Min.   : 55.00   Min.   : 44.00   Min.   : 5.20   Length:428        \n 1st Qu.: 80.00   1st Qu.: 65.00   1st Qu.:22.90   Class :character  \n Median : 87.00   Median : 72.00   Median :25.40   Mode  :character  \n Mean   : 91.79   Mean   : 73.55   Mean   :25.72                     \n 3rd Qu.: 96.00   3rd Qu.: 80.00   3rd Qu.:28.10                     \n Max.   :178.00   Max.   :152.00   Max.   :61.60                     \n                                                                     \n\n\nThe data set arrhythmia has 428 patients (rows) and includes 8 variables (columns) as follows:\n\nage: age (yrs)\nsex: sex (male, female)\nheight: height (cm)\nweight: weight (kg)\nQRS: mean duration of QRS (ms) \nHR: heart rate (beats/min)\nbmi\nbmi_cat (4 levels: underweight, normal, overweight, obese)\n\nWe might have noticed that the categorical variables sex and bmi_cat are recognized of character &lt;chr&gt; type. We can use the factor() function inside the mutate() to convert the variables to factors as follows:\n\narrhythmia &lt;- arrhythmia %&gt;% \n  mutate(sex = factor(sex),\n1         bmi_cat = factor(bmi_cat, levels = c(\"underweight\", \"normal\",\n                                              \"overweight\", \"obese\")))\n\n\n1\n\nbmi_cat is an ordered variable so the order of the levels has to be specified explicitly in the factor() function.\n\n\n\n\nLet’s look at the data again with the glipmse() function:\n\nglimpse(arrhythmia)\n\nRows: 428\nColumns: 8\n$ age     &lt;dbl&gt; 75, 56, 54, 55, NA, 40, 49, 44, 50, 62, 45, 54, 30, 44, 47, 47…\n$ sex     &lt;fct&gt; male, female, male, male, male, female, female, male, female, …\n$ height  &lt;dbl&gt; 190, 165, 172, 175, 190, 160, 162, 168, 167, 170, 165, 172, 17…\n$ weight  &lt;dbl&gt; 80, 64, 95, 94, 80, 52, 54, 56, 67, 72, 86, 58, 73, 88, 48, 59…\n$ QRS     &lt;dbl&gt; 91, 81, 138, 115, 88, 77, 78, 84, 89, 152, 77, 78, 133, 77, 75…\n$ HR      &lt;dbl&gt; 63, 53, 75, 71, 75, 70, 67, 64, 63, 70, 72, 73, 56, 72, 76, 67…\n$ bmi     &lt;dbl&gt; 22.2, 23.5, 32.1, 30.7, 22.2, 20.3, 20.6, 19.8, 24.0, 24.9, 31…\n$ bmi_cat &lt;fct&gt; normal, normal, obese, obese, normal, normal, normal, normal, …\n\n\nNow, both variables, sex and bmi_cat, have become factors with levels."
  },
  {
    "objectID": "descriptive.html#summarizing-categorical-data-frequency-statistics",
    "href": "descriptive.html#summarizing-categorical-data-frequency-statistics",
    "title": "2  Descriptive statistics",
    "section": "2.3 Summarizing Categorical Data (Frequency Statistics)",
    "text": "2.3 Summarizing Categorical Data (Frequency Statistics)\nThe first step to analyze categorical data is to count the different types of labels and calculate the frequencies. The set of frequencies of all the possible categories is called the frequency distribution of the variable. Additionally, we can express the frequencies as proportions of the total sample size (relative frequencies, %).\nWe can generate a frequency table for the sex variable using the freq() function from the {questionr} package:\n\nfreq(arrhythmia$sex, cum = T, total = T, valid = F)\n\n         n     %  %cum\nfemale 237  55.4  55.4\nmale   191  44.6 100.0\nTotal  428 100.0 100.0\n\n\nThe table shows the number of patients (n) in each category (absolute frequency), the percentage (%) contribution of each category to the total (relative frequency), and the commutative percentage (%cum). Of note, the percentages add up to 100%.\nSimilarly, we can create the frequency table for the bmi_cat variable:\n\nfreq(arrhythmia$bmi_cat, cum = T, total = T, valid = F)\n\n              n     %  %cum\nunderweight  11   2.6   2.6\nnormal      192  44.9  47.4\noverweight  167  39.0  86.4\nobese        58  13.6 100.0\nTotal       428 100.0 100.0\n\n\n \nWe can also sort the BMI categories in a decreasing order of frequencies:\n\nfreq(arrhythmia$bmi_cat, cum = T, total = T, valid = F, sort = \"dec\")\n\n              n     %  %cum\nnormal      192  44.9  44.9\noverweight  167  39.0  83.9\nobese        58  13.6  97.4\nunderweight  11   2.6 100.0\nTotal       428 100.0 100.0\n\n\nIn the above table we observe that a large proportion of patients are overweight (167 out of 428, 39.0%).\n \nIn addition to tabulating each variable separately, we might be interested in whether the distribution of patients across each sex is different for each BMI category.\n\ntab &lt;- table(arrhythmia$sex, arrhythmia$bmi_cat)\nrprop(tab, percent = T, total = F, n = T)\n\n        \n         underweight normal overweight obese  n  \n  female   3.0%       48.5%  32.1%      16.5% 237\n  male     2.1%       40.3%  47.6%       9.9% 191\n\n\nWe can see that the percentage of overweight male patients (47.6%) is higher than overweight female patients (32.1%). In contrast, the percentage of obese male patients (9.9%) is lower than obese female patients (16.5%)."
  },
  {
    "objectID": "descriptive.html#displaying-categorical-data",
    "href": "descriptive.html#displaying-categorical-data",
    "title": "2  Descriptive statistics",
    "section": "2.4 Displaying Categorical Data",
    "text": "2.4 Displaying Categorical Data\nWhile frequency tables are extremely useful, the best way to investigate a dataset is to plot it. For categorical variables, such as sex and bmi_cat, it is straightforward to present the number in each category, usually indicating the frequency and percentage of the total number of patients. When shown graphically this is called a bar plot.\nA. Simple Bar Plot\nA simple bar plot is an easy way to make comparisons across categories. Figure 2.2 shows the BMI categories for 428 patients. Along the horizontal axis (x-axis) are the different BMI categories whilst on the vertical axis (y-axis) is the percentage (%). The height of each bar represents the percentage of the total patients in that category. For example, it can be seen that the percentage of overweight participants is 39% (167/428).\n\n# create a data frame with ordered BMI categories and their counts\ndat1 &lt;- arrhythmia %&gt;%\n  count(bmi_cat) %&gt;% \n  mutate(pct = round_percent(n, 1))\n\n# plot the data\nggplot(dat1, aes(x = bmi_cat, y = pct)) +\n  geom_col(width=0.65, fill = \"steelblue4\") +\n  geom_text(aes(label=paste0(pct, \"%\")),\n            vjust=1.6, color = \"white\", size = 3) +\n  labs(x = \"BMI category\", y = \"Percent\",\n       caption = \"Number of patients: 428\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  theme_minimal(base_size = 12)\n\n\n\n\nFigure 2.2: Bar plot showing the BMI category distribution for 428 patients.\n\n\n\n\n\n\n\n\n\n\nBasic Properties of a Simple Bar plot\n\n\n\n\nAll bars should have equal width and should have equal space between them.\nThe height of bar is equivalent to the data they represent.\nThe bars must be plotted against a common zero-valued baseline.\n\n\n\n \nB. Side-by-side and Grouped Bar Plots\nIf the data are further classified into whether the patient was male or female then it becomes impossible to present this information to a single bar plot. In this case, we can present the data as a side-by-side bar plot (Figure 2.3) or, even better, as a grouped bar plot to make the visual comparisons easier (Figure 2.4).\n\n# create a data frame with ordered BMI categories and their counts by sex\ndat2 &lt;- arrhythmia %&gt;%\n  count(bmi_cat, sex) %&gt;% \n  group_by(sex) %&gt;% \n  mutate(pct = round_percent(n, 1)) %&gt;% \n  ungroup()\n\nggplot(dat2) + \n  geom_col(aes(bmi_cat, pct, fill = sex), width=0.7, position = \"dodge\") +\n  geom_text(aes(bmi_cat, pct, label = paste0(pct, \"%\"), \n                group = sex), color = \"white\", size = 3,vjust=1.2,\n            position = position_dodge(width = .9)) +\n  labs(x = \"BMI category\", y = \"Percent\",\n       caption = \"female: n=237, male: n=191\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  scale_fill_jco() +\n  theme_minimal(base_size = 12) +\n  theme(legend.position=\"none\",\n        axis.text.x = element_text(angle = 45, hjust = 1)) +\n  facet_wrap(~sex, ncol = 2)\n\n\n\n\nFigure 2.3: Side-by-side bar plot showing by BMI category and sex.\n\n\n\n\n\nggplot(dat2) + \n  geom_col(aes(bmi_cat, pct, fill = sex), width = 0.8, position = \"dodge\") +\n  geom_text(aes(bmi_cat, pct, label = paste0(pct, \"%\"), \n                group = sex), color = \"white\", size = 3,vjust=1.2,\n            position = position_dodge(width = .9)) +\n  labs(x = \"BMI category\", y = \"Percent\",\n       caption = \"female: n=237, male: n=191\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  scale_fill_jco() +\n  theme_minimal(base_size = 12)\n\n\n\n\nFigure 2.4: Grouped bar plot showing 428 patients by BMI category and sex.\n\n\n\n\n \nC. Stacked Bar Plot\nUnlike side-by-side or grouped bar plots, stacked bar plots segment their bars. A 100% Stack Bar Plot shows the percentage-of-the-whole of each group. This makes it easier to see if relative differences exist between quantities in each group (Figure 2.5).\n\n# create a data frame with ordered BMI categories and their counts by sex\ndat3 &lt;- arrhythmia %&gt;% \n  group_by(sex) %&gt;% \n  count(bmi_cat) %&gt;% \n  mutate(pct = round_percent(n, 2)) %&gt;% \n  ungroup()\n\nggplot(dat3, aes(x = sex, y = pct, fill = forcats::fct_rev(bmi_cat)))+\n  geom_bar(stat = \"identity\", width = 0.8)+\n  geom_text(aes(label = paste0(round(pct, 1), \"%\"), y = pct), \n            position = position_stack(vjust = 0.5)) +\n  coord_flip()+\n  scale_fill_simpsons() +\n  scale_y_continuous(labels = scales::percent_format(scale = 1))+\n  labs(x = \"Sex\", y = \"Percent\", fill = \"BMI category\") +\n  theme_minimal(base_size = 12)\n\n\n\n\nFigure 2.5: A horizontal 100% stacked bar plot showing the distribution of BMI stratified by sex.\n\n\n\n\n\n\n\n\n\n\nStacked bar plots tend to become confusing when the variable has many levels\n\n\n\nOne issue to consider when using stacked bar plots is the number of variable levels: when dealing with many categories, stacked bar plots tend to become rather confusing."
  },
  {
    "objectID": "descriptive.html#summarizing-numerical-data",
    "href": "descriptive.html#summarizing-numerical-data",
    "title": "2  Descriptive statistics",
    "section": "2.5 Summarizing Numerical Data",
    "text": "2.5 Summarizing Numerical Data\nSummary measures are single numerical values that summarize a large number of values. Numeric data are described with two main types of summary measures (Table 2.1):\n\nmeasures of central location (where the center of the distribution of the values in a variable is located)\nmeasures of dispersion (how widely the values are spread above and below the central value)\n\n\n\nTable 2.1: Common summary measures of central location and dispersion\n\n\n\n\n\n\nMeasures of central location\nMeasures of dispersion\n\n\n\n\n\nmean\nmedian\nmode\n\n\nvariance\nstandard deviation\nrange (minimum, maximum)\ninterquartile range (1st and 3rd quartiles)"
  },
  {
    "objectID": "descriptive.html#summary-statistics",
    "href": "descriptive.html#summary-statistics",
    "title": "2  Descriptive statistics",
    "section": "2.6 Summary statistics",
    "text": "2.6 Summary statistics\n\nMeasures of central location\nA. Sample Mean or Average\n\n\nAdvantages of mean\n\nIt uses all the data values in the calculation.\nIt is algebraically defined and thus mathematically manageable.\n\nDisadvantages of mean\n\nIt is highly affected by the presence of a few abnormally high or abnormally low values (outliers), so it is not an appropriate average for highly skewed (asymmetrical) distributions.\nIt can not be determined easily by inspection of the data.\n\nLet \\(x_1, x_2,...,x_{n-1}, x_n\\) be a set of n measurements. The arithmetic sample mean or average, \\(\\bar{x}\\), is the sum of the observations divided by their number n, thus:\n\\[\n\\bar{x}= \\frac{x_1 + x_2 + ... + x_{n-1} + x_n}{n} = \\frac{1}{n}\\sum_{i=1}^{n}x_{i}\n\\] where \\(x_{i}\\) represents the individual sample values and \\({\\sum_{i=1}^{n}x_{i}}\\) their sum.\nLet’s calculate the sample mean of age variable in our dataset:\n\n\n\n\n\n\nSample mean of age\n\n\n\n\nBase Rdplyr\n\n\n\n1mean(arrhythmia$age, na.rm = TRUE)\n\n\n1\n\nIf some of the values in a vector are missing (NA), then the mean of the vector can not be defined. The argument na.rm = TRUE removes the missing values and the mean is calculated using the remaining values.\n\n\n\n\n[1] 48.67059\n\n\n\n\n\narrhythmia %&gt;% \n  dplyr::summarise(mean = mean(age, na.rm = TRUE))\n\n# A tibble: 1 × 1\n   mean\n  &lt;dbl&gt;\n1  48.7\n\n\n\n\n\n\n\nB. Median of the sample\nThe sample median, md, is an alternative measure of location, which is less sensitive to outliers. The median is calculated by first sorting the observed values (i.e. arranging them in an ascending/descending order) and selecting the middle one. If the sample size n is odd, the median is the number at the middle of the ordered observations. If the sample size is even, the median is the average of the two middle numbers.\n\n\nAdvantage of sample median\n\nIt is not affected by outliers.\n\nDisadvantage of sample median\n\nIt does not take into account the precise value of each observation and hence does not use all the information available in the data.\n\nTherefore, the sample median, md, of n observations is:\n\nthe \\(\\frac{n+1}{2}\\)th ordered value, \\(md=x_{\\frac{n+1}{2}}\\), if n is odd.\nthe average of the \\(\\frac{n}{2}\\)th and \\(\\frac{n+1}{2}\\)th ordered values, \\(md=\\frac{1}{2}(x_{\\frac{n}{2}}+x_{\\frac{n+1}{2}})\\), if n is even.\n\n\n\n\n\n\n\nSample median of age\n\n\n\n\nBase Rdplyr\n\n\n\nmedian(arrhythmia$age, na.rm = TRUE)\n\n[1] 48\n\n\n\n\n\narrhythmia %&gt;% \n  dplyr::summarise(median = median(age, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  median\n   &lt;dbl&gt;\n1     48\n\n\n\n\n\n\n\nC. Mode of the sample\nA third measure of location is the mode. This is the value that occurs most frequently in a set of data values. Note that some dataset do not have a mode because each value occurs only once.\n\n\nNOTE: When a distribution has to modes (peaks) is called Bimodal distribution. This can be caused by mixing two populations together. For example, height might appear to have a bimodal distribution if men and women are included in the study.\nBase R does not provide a function for calculating the mode of a numeric variable. However, we can download the package called {modeest} and use the mlv() function specifying the method as \"mfv\". This method returns the most frequent value(s):\n\nmlv(arrhythmia$age, method = \"mfv\", na.rm = TRUE)\n\n[1] 47\n\n\n\n\nMeasures of Dispersion\nA. Sample Variance\nSample variance, \\(s^2\\), is a measure of spread of the data. It is calculated by taking the sum of the squared deviations from the sample mean and dividing by \\(n-1\\):\n\\[variance = s^2 = \\frac{\\sum\\limits_{i=1}^n (x -\\bar{x})^2}{n-1}\\]\n\n\n\n\n\n\nSample variance of age\n\n\n\n\nBase Rdplyr\n\n\n\nvar(arrhythmia$age, na.rm = TRUE)\n\n[1] 199.4243\n\n\n\n\n\narrhythmia %&gt;% \n  dplyr::summarise(variance = var(age, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  variance\n     &lt;dbl&gt;\n1     199.\n\n\n\n\n\n\n\nThe variance is expressed in square units, so it is not suitable measure for describing variability of data.\nB. Standard deviation of the sample\nStandard deviation (denoted as sd or s) of a data set is the square root of the sample variance:\n\\[sd= s = \\sqrt{s^2} = \\sqrt\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^2}{n-1}\\]\n\n\n\n\n\n\nStandard deviation of age\n\n\n\n\nBase Rdplyr\n\n\n\nsd(arrhythmia$age, na.rm = TRUE)\n\n[1] 14.12177\n\n\n\n\n\narrhythmia %&gt;% \n  dplyr::summarise(standard_deviation = sd(age, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  standard_deviation\n               &lt;dbl&gt;\n1               14.1\n\n\n\n\n\n\n\nStandard deviation is expressed in the same units as the original values.\nC. Range of the sample\nThe Range is the difference between the minimum (lowest) and maximum (highest) values. In R, the range() function returns a vector containing the minimum and maximum values:\n\n\nOne disadvantage of using range as a measure of dispersion is its sensitivity to outliers.\n\nrange(arrhythmia$age, na.rm = TRUE)\n\n[1] 18 83\n\n\nThe difference between the two values, 83 - 18, is:\n\ndiff(range(arrhythmia$age, na.rm = TRUE))\n\n[1] 65\n\n\nD. Inter-quartile range of the sample\n\nIQR(arrhythmia$age, na.rm = TRUE)\n\n[1] 21\n\n\n\nquantile(arrhythmia$age, prob=c(0.25, 0.75), na.rm = T, type=1)\n\n25% 75% \n 38  59 \n\n\nLet’s calculate the summary statistics for the age variable in our dataset.\n\n\n\n\n\n\nSummary statistics: Variable age\n\n\n\n\ndplyrdlookrpsych\n\n\n\narrhythmia %&gt;%\n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(age)),\n    min = min(age, na.rm = TRUE),\n    q1 = quantile(age, 0.25, na.rm = TRUE),\n    median = quantile(age, 0.5, na.rm = TRUE),\n    q3 = quantile(age, 0.75, na.rm = TRUE),\n    max = max(age, na.rm = TRUE),\n    mean = mean(age, na.rm = TRUE),\n    sd = sd(age, na.rm = TRUE),\n    skewness = EnvStats::skewness(age, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(age, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 11\n      n    na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1   428     3    18    38     48    59    83  48.7  14.1   0.0557   -0.605\n\n\n\n\n\narrhythmia %&gt;% \n  dlookr::describe(age) %&gt;% \n  select(described_variables, n, na, mean, sd, p25, p50, p75, skewness, kurtosis)\n\nRegistered S3 method overwritten by 'httr':\n  method         from  \n  print.response rmutil\n\n\n# A tibble: 1 × 10\n  described_variables     n    na  mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 age                   425     3  48.7  14.1    38    48    59   0.0557\n# ℹ 1 more variable: kurtosis &lt;dbl&gt;\n\n\n\n\n\npsych::describe(arrhythmia$age)\n\nRegistered S3 method overwritten by 'psych':\n  method         from  \n  plot.residuals rmutil\n\n\n   vars   n  mean    sd median trimmed   mad min max range skew kurtosis   se\nX1    1 425 48.67 14.12     48   48.61 16.31  18  83    65 0.06    -0.62 0.69\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary statistics: Variable QRS\n\n\n\n\ndplyrdlookrpsych\n\n\n\narrhythmia %&gt;%\n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(QRS)),\n    min = min(QRS, na.rm = TRUE),\n    q1 = quantile(QRS, 0.25, na.rm = TRUE),\n    median = quantile(QRS, 0.5, na.rm = TRUE),\n    q3 = quantile(QRS, 0.75, na.rm = TRUE),\n    max = max(QRS, na.rm = TRUE),\n    mean = mean(QRS, na.rm = TRUE),\n    sd = sd(QRS, na.rm = TRUE),\n    skewness = EnvStats::skewness(QRS, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(QRS, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 11\n      n    na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1   428     0    55    80     87    96   178  91.8  19.1     1.88     3.94\n\n\n\n\n\narrhythmia %&gt;% \n  dlookr::describe(QRS) %&gt;% \n  select(described_variables, n, na, mean, sd, p25, p50, p75, skewness, kurtosis)\n\n# A tibble: 1 × 10\n  described_variables     n    na  mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 QRS                   428     0  91.8  19.1    80    87    96     1.88\n# ℹ 1 more variable: kurtosis &lt;dbl&gt;\n\n\n\n\n\npsych::describe(arrhythmia$QRS)\n\n   vars   n  mean    sd median trimmed   mad min max range skew kurtosis   se\nX1    1 428 91.79 19.14     87   88.48 11.86  55 178   123 1.86     3.85 0.93\n\n\n\n\n\n\n\n \nB. Summary statistics by group\nNext, we are interested in calculating the summary statistics of the age variable for males and females, separately.\n\n\n\n\n\n\nSummary statistics: age stratified by sex\n\n\n\n\ndplyrdlookrpsych\n\n\n\nsummary_age_sex &lt;- arrhythmia %&gt;%\n  group_by(sex) %&gt;% \n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(age)),\n    min = min(age, na.rm = TRUE),\n    q1 = quantile(age, 0.25, na.rm = TRUE),\n    median = quantile(age, 0.5, na.rm = TRUE),\n    q3 = quantile(age, 0.75, na.rm = TRUE),\n    max = max(age, na.rm = TRUE),\n    mean = mean(age, na.rm = TRUE),\n    sd = sd(age, na.rm = TRUE),\n    skewness = EnvStats::skewness(age, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(age, na.rm = TRUE)\n  ) %&gt;% \n  ungroup()\n\nsummary_age_sex\n\n# A tibble: 2 × 12\n  sex       n    na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 fema…   237     2    19    36     47  58.5    83  47.6  14.5   0.163    -0.723\n2 male    191     1    18    41     49  59      80  49.9  13.5  -0.0630   -0.343\n\n\n\n\n\narrhythmia %&gt;% \n  group_by(sex) %&gt;% \n  dlookr::describe(age) %&gt;% \n  select(described_variables, sex, n, na, mean, sd, p25, p50, p75, skewness, kurtosis)\n\n# A tibble: 2 × 11\n  described_variables sex        n    na  mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;fct&gt;  &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 age                 female   235     2  47.6  14.5    36    47  58.5   0.163 \n2 age                 male     190     1  49.9  13.5    41    49  59    -0.0630\n# ℹ 1 more variable: kurtosis &lt;dbl&gt;\n\n\n\n\n\npsych::describeBy(arrhythmia$age, group = arrhythmia$sex)\n\n\n Descriptive statistics by group \ngroup: female\n   vars   n  mean    sd median trimmed   mad min max range skew kurtosis   se\nX1    1 235 47.64 14.53     47    47.4 16.31  19  83    64 0.16    -0.75 0.95\n------------------------------------------------------------ \ngroup: male\n   vars   n  mean    sd median trimmed   mad min max range  skew kurtosis   se\nX1    1 190 49.94 13.53     49   50.05 13.34  18  80    62 -0.06    -0.39 0.98\n\n\n\n\n\n\n\nIf we want to save our descriptive statistics, calculated in R, we can use the write_xlsx() function from {writexl} package. In the example below, we are saving the summary_age_sex table to a .xlsx file in the data folder of our RStudio Project:\n\nlibrary(writexl)\nwrite_xlsx(summary_age_sex, here(\"data\", \"summary_age_sex.xlsx\"))\n\n \n\n\n\n\n\n\nSummary statistics: QRS stratified by sex\n\n\n\n\ndplyrdlookrpsych\n\n\n\narrhythmia %&gt;%\n  group_by(sex) %&gt;% \n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(QRS)),\n    min = min(QRS, na.rm = TRUE),\n    q1 = quantile(QRS, 0.25, na.rm = TRUE),\n    median = quantile(QRS, 0.5, na.rm = TRUE),\n    q3 = quantile(QRS, 0.75, na.rm = TRUE),\n    max = max(QRS, na.rm = TRUE),\n    mean = mean(QRS, na.rm = TRUE),\n    sd = sd(QRS, na.rm = TRUE),\n    skewness = EnvStats::skewness(QRS, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(QRS, na.rm = TRUE)\n  ) %&gt;% \n  ungroup()\n\n# A tibble: 2 × 12\n  sex       n    na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 fema…   237     0    55    77     82   90    163  86.4  17.2     2.22     5.65\n2 male    191     0    71    87     92  102.   178  98.5  19.4     1.92     3.65\n\n\n\n\n\narrhythmia %&gt;% \n  group_by(sex) %&gt;% \n  dlookr::describe(QRS) %&gt;% \n  select(described_variables, sex, n, na, mean, sd, p25, p50, p75, skewness, kurtosis)\n\n# A tibble: 2 × 11\n  described_variables sex        n    na  mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;fct&gt;  &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 QRS                 female   237     0  86.4  17.2    77    82   90      2.22\n2 QRS                 male     191     0  98.5  19.4    87    92  102.     1.92\n# ℹ 1 more variable: kurtosis &lt;dbl&gt;\n\n\n\n\n\npsych::describeBy(arrhythmia$QRS, group = arrhythmia$sex)\n\n\n Descriptive statistics by group \ngroup: female\n   vars   n mean    sd median trimmed mad min max range skew kurtosis   se\nX1    1 237 86.4 17.18     82   83.23 8.9  55 163   108 2.19     5.44 1.12\n------------------------------------------------------------ \ngroup: male\n   vars   n  mean    sd median trimmed   mad min max range skew kurtosis  se\nX1    1 191 98.48 19.37     92   94.74 10.38  71 178   107 1.89     3.45 1.4\n\n\n\n\n\n\n\n\n\n\n\n\n\nReporting summary statistics for numerical data\n\n\n\nA. Mean (sd) for data with symmetric distribution. A distribution, or dataset, is symmetric if its left and right sides are mirror images.\nB. Median (Q1, Q3) for data with skewed (or asymmetrical) distribution."
  },
  {
    "objectID": "descriptive.html#displaying-numerical-data",
    "href": "descriptive.html#displaying-numerical-data",
    "title": "2  Descriptive statistics",
    "section": "2.7 Displaying Numerical Data",
    "text": "2.7 Displaying Numerical Data\nA. Histogram / Density plot\nThe most common way of presenting a frequency distribution of a continuous variable is a histogram. Histograms (Figure 2.6) depict the distribution of the data as a series of bars without space between them. Each bar typically covers a range of numeric values called a bin; a bar’s height indicates the frequency of observations with a value within the corresponding bin.\n\n# Histogram of age\narrhythmia %&gt;% \n  ggplot(aes(x = age)) +\n1  geom_histogram(binwidth = 8, fill = \"steelblue4\",\n                 color = \"#8fb4d9\", alpha = 0.6) +  \n  theme_minimal(base_size = 14) +\n  labs(title = \"Histogram: age\", y = \"Frequency\")\n\n# Histogram of QRS\narrhythmia %&gt;% \n  ggplot(aes(x = QRS)) +\n  geom_histogram(binwidth = 8, fill = \"steelblue4\",      \n                 color = \"#8fb4d9\", alpha = 0.6) +\n  theme_minimal(base_size = 14) +\n  labs(title = \"Histogram: QRS\", y = \"Frequency\")\n\n\n\n\n\n1\n\nThe exact visual appearance depends on the choice of the binwidth argument. Try different bin widths to verify that the resulting histogram represents the underlying data accurately.\n\n\n\n\n\n\n\n\n\n(a) Histogram of age for the 425 patients.\n\n\n\n\n\n\n\n\n\n(b) Histogram of QRS for the 428 patients.\n\n\n\n\nFigure 2.6: Distributions of (a) age and (b) QRS variables.\n\n\n\nA histogram gives information about:\n\nHow the data are distributed (symmetrical or asymmetrical) and if there are any outliers.\nWhere the peak (or peaks) of the distribution is.\nThe amount of variability in the data.\n\nDensity plot is also used to present the distribution of a continuous variable and it is considered a variation of the histogram allowing for smoother distributions1 (Figure 2.7). In this case, geom_density() function is used for displaying the distribution.1 Density curves are usually scaled such that the area under the curve equals one.\n\n# density plot of age\narrhythmia %&gt;% \n  ggplot(aes(x = age)) +\n  geom_density(fill=\"steelblue4\", color=\"#8fb4d9\", \n               adjust = 1.5, alpha=0.6) +\n  theme_minimal(base_size = 14) +\n  labs(title = \"Density Plot: age\", y = \"Density\")\n\n# density plot of QRS\narrhythmia %&gt;% \n  ggplot(aes(x = QRS)) +\n  geom_density(fill=\"steelblue4\", color=\"#8fb4d9\", \n               adjust = 1.5, alpha=0.6) +\n  theme_minimal(base_size = 14) +\n  labs(title = \"Density Plot: QRS\", y = \"Density\")\n\n\n\n\n\n\n\n(a) Density plot of age for the 425 patients.\n\n\n\n\n\n\n\n\n\n(b) Density plot of QRS for the 428 patients.\n\n\n\n\nFigure 2.7: Density plot of (a) age and (b) QRS variables.\n\n\n\n \nB. Box Plot\nBox plots can be used for displaying location and dispersion for continuous data, particularly when comparing distributions between many groups (Figure 2.8). This type of graph uses boxes and lines to depict the distributions. Box limits indicate the range of the central 50% of the data, with a horizontal line in the box corresponding to the median. Whiskers extend from each box to capture the range of the remaining data. Data points that are outside the whiskers are represented as dots on the graph and considered potential outliers.22 An outlier is an observation that is significantly distant from the main body the data. We say any value outside of the following interval is an outlier: \\[(Q_1 - 1.5 \\times IQR, \\ Q_3 + 1.5 \\times IQR)\\]\n\n# box plot of age stratified by sex\narrhythmia %&gt;% \n  ggplot(aes(x = sex, y = age, fill = sex)) +\n  geom_boxplot(alpha = 0.6, width = 0.5) +\n  theme_minimal(base_size = 14) +\n  labs(title = \"Grouped Box Plot: age by sex\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n# box plot of QRS stratified by sex\narrhythmia %&gt;% \n  ggplot(aes(x = sex, y = QRS, fill = sex)) +\n  geom_boxplot(alpha = 0.6, width = 0.5) +\n  theme_minimal(base_size = 14) +\n  labs(title = \"Grouped Box Plot: QRS by sex\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n(a) Box plot of age stratified by sex (female: 235; male = 190).\n\n\n\n\n\n\n\n\n\n(b) Box plot of QRS stratified by sex (female: 237; male = 191).\n\n\n\n\nFigure 2.8: Box plot of (a) age and (b) QRS variables stratified by sex.\n\n\n\nIn Figure 2.8 a, box plots of age are approximately symmetric about the median for females and males. On the contrary, in Figure 2.8 b, both distributions of QRS data are positively skewed; the box plots show the medians closer to the lower quartiles (q25) and we observe many outliers at the upper range of the data for females and males.\n \nC. Raincloud Plot\nThere are many variations of the box plot. For example, there is a way to combine raw data (dots), probability density, and key summary statistics such as median, and relevant intervals of a range of likely values for the population parameter, in an appealing and flexible format with minimal redundancy, using the raincloud plot (Figure 2.9):\n\nggdistggrain\n\n\n\n# raincloud plot of age stratified by sex\narrhythmia %&gt;% \n  ggplot(aes(x = sex, y = age, fill = sex)) +\n  stat_slab(aes(thickness = stat(pdf*n)), \n                scale = 0.5) +\n  stat_dotsinterval(side = \"bottom\", \n                    scale = 0.5, \n                    slab_size = 0.2) +\n  theme_minimal(base_size = 14) +\n  labs(title = \"Grouped Raincloud Plots: age by sex\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n# raincloud plot of QRS stratified by sex\narrhythmia %&gt;% \n  ggplot(aes(x = sex, y = QRS, fill = sex)) +\n  stat_slab(aes(thickness = stat(pdf*n)), \n                scale = 0.5) +\n  stat_dotsinterval(side = \"bottom\", \n                    scale = 0.5, \n                    slab_size = 0.2) +\n  theme_minimal(base_size = 14) +\n  labs(title = \"Grouped Raincloud Plots: QRS by sex\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n(a) Raincloud of age stratified by sex (female: 235; male = 190).\n\n\n\n\n\n\n\n\n\n(b) Raincloud of QRS stratified by sex (female: 237; male = 191).\n\n\n\n\nFigure 2.9: Raincloud plot of (a) age and (b) QRS variables stratified by sex.\n\n\n\n\n\n\nggplot(arrhythmia, aes(sex, age, fill = sex)) +\n  geom_rain(likert= TRUE,\n            point.args = list(alpha = .3)) +\n  theme_minimal(base_size = 14) +\n  labs(title = \"Grouped Raincloud Plots: age by sex\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n\nggplot(arrhythmia, aes(sex, QRS, fill = sex)) +\n  geom_rain(likert= TRUE,\n            point.args = list(alpha = .3)) +\n  theme_minimal(base_size = 14) +\n  labs(title = \"Grouped Raincloud Plots: QRS by sex\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n(a) Raincloud of age stratified by sex (female: 235; male = 190).\n\n\n\n\n\n\n\n\n\n(b) Raincloud of QRS stratified by sex (female: 237; male = 191).\n\n\n\n\nFigure 2.10: Raincloud plot of (a) age and (b) QRS variables stratified by sex."
  },
  {
    "objectID": "normal.html#packages-we-need",
    "href": "normal.html#packages-we-need",
    "title": "3  Normal distribution",
    "section": "\n3.1 Packages we need",
    "text": "3.1 Packages we need\nWe need to load the following packages:\n\nlibrary(stevemisc)\nlibrary(ggpubr)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "normal.html#the-shape-of-a-normal-distribution",
    "href": "normal.html#the-shape-of-a-normal-distribution",
    "title": "3  Normal distribution",
    "section": "\n3.2 The shape of a normal distribution",
    "text": "3.2 The shape of a normal distribution\nA normal distribution is a symmetric “bell-shaped” probability distribution where most of the observed data are clustered around a central location. Data farther from the central location occur less frequently (Figure 3.1).\n\n\nNormal distribution, technically a probability density function, is a distribution defined by two parameters, mean \\(\\mu\\) and variance \\(\\sigma^2\\). The mean, \\(\\mu\\), is a “location parameter”, which defines the central tendency. The variance, \\(\\sigma^2\\), is the “scale parameter”, which defines the width and height of the distribution. It’s formally given as:\n\\[ f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}} \\]\nwhere \\(\\pi \\approx 3.142\\) and \\(e \\approx 2.718\\).\n\nx &lt;- seq(-4, 4, length=200)                                                    \ndf &lt;- data.frame(x)                                                            \n\nggplot(df, aes(x)) +                                                           \n  stat_function(fun = dnorm) +                                                 \n  scale_x_continuous(breaks = c(-3, -2, -1, 0, 1, 2, 3),                       \n                     labels = expression(-3*sigma, -2*sigma, -1*sigma,         \n                                         mu, 1*sigma, 2*sigma, 3*sigma)) +     \n  labs(x = \"Variable\",\n       y = \"Probability density\") +\n  theme(text = element_text(size = 16))                                    \n\n\n\nFigure 3.1: The area underneath a Normal Distribution"
  },
  {
    "objectID": "normal.html#the-properties-of-a-normal-distribution",
    "href": "normal.html#the-properties-of-a-normal-distribution",
    "title": "3  Normal distribution",
    "section": "\n3.3 The properties of a normal distribution",
    "text": "3.3 The properties of a normal distribution\n\nnormal_dist(\"#522d80\",\"#00868B\") + \n  labs(y = \"Probability density\", \n       x = \"Variable\") +\n  scale_x_continuous(breaks = c(-3, -2.58, -1.96, -1, \n                              0, 1, 1.96, 2.58, 3),\n                   labels = expression(-3*sigma, -2.58*sigma, -1.96*sigma, -1*sigma, \n                                       mu, 1*sigma, 1.96*sigma, 2.58*sigma, 3*sigma)) +\n  theme(text = element_text(size = 20), \n        axis.text.x = element_text(size = 12))\n\n\n\nFigure 3.2: The area underneath a Normal Distribution\n\n\n\n\nThe Normal distribution has the properties summarized as follows:\n\nBell shaped and symmetrical around the mean. Shape statistics, skewness and excess kurtosis are zero.\nThe peak of the curve lies above the mean.\nAny position along the horizontal axis (x-axis) can be expressed as a number of standard deviations from the mean.\nAll three measures of central location mean, median, and mode are the same.\nThe empirical rule (also called the “68-95-99 rule”). Much of the area (68%) of the distribution is between -1 \\(\\sigma\\) below the mean and +1 \\(\\sigma\\) above the mean, the large majority (95%) between -1.96 \\(\\sigma\\) below the mean and +1.96 \\(\\sigma\\) above the mean (often used as a reference range), and almost all (99%) between -2.58 \\(\\sigma\\) below the mean and +2.58 \\(\\sigma\\) above the mean. The total area under the curve equals to 1 (or 100%), almost -3 \\(\\sigma\\) below the mean and +3 \\(\\sigma\\) above the mean."
  },
  {
    "objectID": "normal.html#shape-statistics-and-normality",
    "href": "normal.html#shape-statistics-and-normality",
    "title": "3  Normal distribution",
    "section": "\n3.4 Shape statistics and normality",
    "text": "3.4 Shape statistics and normality\nThere are two shape statistics that can indicate deviation from normality: skewness and excess kurtosis.\nA. Skewness\nSkewness is usually described as a measure of a distribution’s symmetry – or lack of symmetry. Skewness values that are negative indicate a tail to the left (Figure 3.3 a), zero value indicate a symmetric distribution (Figure 3.3 b), while values that are positive indicate a tail to the right (Figure 3.3 c).\n\n\n\n\n\n\nNormal distribution and skewness\n\n\n\nThe skewness for a normal distribution is zero. In practice, approximate bell-shaped curves have skewness values between −1 and +1. Values from −1 to −3 or from +1 to +3 indicate that the distribution is tending away from symmetry with &gt;1 indicating moderate skewness and &gt;2 indicating severe skewness. Any values below−3 or above +3 are a good indication that the distribution is not symmetric, therefore, the variable can not be normally distributed.\n\n\n\nShow the code# create a data frame\nx &lt;- seq(0, 1, length=200)\ny1 &lt;- dbeta(x, 7, 2)\ny2 &lt;- dbeta(x, 7, 7)\ny3 &lt;- dbeta(x, 2, 7)\ndf1 &lt;- data.frame(x, y1, y2, y3)\n\n# left skewed distribution\nggplot(df1, aes(x, y1)) +\n  geom_line(color=\"green\", linewidth = 1.0) +\n  geom_segment(aes(x = 0.7, y = 0, xend = 0.7,  yend = 1.98),\n               color = \"black\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.67, y = 2.1, label = 'mean', size = 8, color = \"black\") +\n  geom_segment(aes(x = 0.78, y = 0, xend = 0.78,  yend = 2.78), \n               color = \"blue\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.75, y = 2.9, label = 'median', size = 8, color = \"blue\") +\n  geom_segment(aes(x = 0.86, y = 0, xend = 0.86,  yend = 3.17), \n               color = \"orange\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.81, y = 3.13, label = 'mode', size = 8, color = \"orange\") +\n  theme_minimal(base_size = 18) +\n  coord_cartesian(expand = FALSE, xlim = c(0, NA), ylim = c(0, NA)) +\n  labs(title = \"Left skewed distribution\",\n       x = \"Variable\",\n       y = \"Probability density\")\n\n# symmetric distribution\nggplot(df1, aes(x, y2)) +\n  geom_line(color=\"green\", linewidth = 1.0) +\n  geom_segment(aes(x = 0.49, y = 0, xend = 0.49,  yend = 2.89), \n               color = \"orange\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.5, y = 2.46, label = 'mode', size = 8, color = \"orange\") +\n  geom_segment(aes(x = 0.5, y = 0, xend = 0.5,  yend = 2.9), \n               color = \"black\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.5, y = 2.90, label = 'mean', size = 8, color = \"black\") +\n  geom_segment(aes(x = 0.51, y = 0, xend = 0.51,  yend = 2.89), \n               color = \"blue\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.5, y = 2.66, label = 'median', size = 8, color = \"blue\") +\n  theme_minimal(base_size = 18) +\n  coord_cartesian(expand = FALSE, xlim = c(0, NA), ylim = c(0, NA)) +\n  labs(title = \"Symmetric distribution\",\n       x = \"Variable\",\n       y = \"Probability density\")\n\n# right skewed distribution\nggplot(df1, aes(x, y3)) +\n  geom_line(color=\"green\", linewidth = 1.0) +\n  geom_segment(aes(x = 0.15, y = 0, xend = 0.15,  yend = 3.17), \n               color = \"orange\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.22, y = 3.10, label = 'mode', size = 8, color = \"orange\") +\n  geom_segment(aes(x = 0.25, y = 0, xend = 0.25,  yend = 2.5), \n               color = \"blue\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.3, y = 2.55, label = 'median', size = 8, color = \"blue\") +\n  geom_segment(aes(x = 0.3, y = 0, xend = 0.3,  yend = 1.9), \n               color = \"black\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.34, y = 1.96, label = 'mean', size = 8, color = \"black\") +\n  theme_minimal(base_size = 18) +\n  coord_cartesian(expand = FALSE, xlim = c(0, NA), ylim = c(0, NA)) +\n  labs(title = \"Right skewed distribution\",\n       x = \"Variable\",\n       y = \"Probability density\")\n\n\n\n\n\n(a) Left skewed distribution (negatively skewed). The mean and the meadian are too left to the mode.\n\n\n\n\n\n\n(b) Symmetric distribution (zero skewness). The mean, median and mode are the same.\n\n\n\n\n\n\n(c) Right skewed distribution (positively skewed). The mean and median are to the right of the mode.\n\n\n\n\nFigure 3.3: Types of distribution according to the summetry.\n\n\n\nB. Excess kurtosis\nThe other way that distributions can deviate from normality is kurtosis. The excess kurtosis1 parameter is a measure of the combined weight of the tails relative to the rest of the distribution. Kurtosis is associated indirect with the peak of the distribution (if the peak of the distribution is too high/sharp or too low compared to a “normal” distribution).1 Excess kurtosis is commonly used because for a normal distribution is equal to zero, while the kurtosis is equal to 3.\nDistributions with negative excess kurtosis are called platykurtic (Figure 3.4 a). If the measure of excess kurtosis is zero the distribution is mesokurtic (Figure 3.4 b). Finally, distributions with positive excess kurtosis are called leptokurtic (Figure 3.4 c).\n\n\n\n\n\n\nNormal distribution and excess kurtosis\n\n\n\nThe excess kurtosis for a normal distribution is zero. In practice, approximate normal distributions have excess kurtosis values between −1 and +1. Values from −1 to −3 or from +1 to +3 indicate that the distribution is tending away from a mesokurtic distribution. Any values below −3 or above +3 are a good indication that the distribution is not mesokurtic, therefore, the variable can not be normally distributed.\n\n\n\nShow the code# create a data frame\nx &lt;- seq(-6, 6, length=200)\ny1 &lt;- dnorm(x)\ny2 &lt;- dnorm(x, sd= 2)\ny3 &lt;- dnorm(x, sd= 0.5)\ndf2 &lt;- data.frame(x, y1, y2, y3)\n\n# platykurtic distribution\nggplot(df2, aes(x, y1)) +\n  geom_line(color=\"black\", linewidth = 0.8, linetype = \"dashed\") +\n  geom_line(aes(x, y2), color=\"deeppink\", linewidth = 0.8) +\n  annotate('text', x = 2.0, y = 0.3, label = 'normal', size = 7, color = \"black\") +\n  annotate('text', x = 4.2, y = 0.1, label = 'platykurtic', size = 8, color = \"deeppink\") +\n  theme_minimal(base_size = 18) +\n  labs(title = \"Platykurtic distribution\",\n       x = \"Variable\",\n       y = \"Probability density\")\n\n# mesokurtic distribution\nggplot(df2, aes(x, y1)) +\n  geom_line(color=\"deeppink\", linewidth = 0.8) +\n  annotate('text', x = 0, y = 0.20, label = 'normal is a', size = 7, color = \"black\") +\n  theme_minimal(base_size = 18) +\n  annotate('text', x = 0, y = 0.15, label = 'mesokurtic distribution', size = 7, color = \"deeppink\") +\n  labs(title = \"Mesokurtic distribution\",\n       x = \"Variable\",\n       y = \"Probability density\")\n\n# leptokurtic distribution\nggplot(df2, aes(x, y1)) +\n  geom_line(color=\"black\", linewidth = 0.8, linetype = \"dashed\") +\n  geom_line(aes(x, y3), color=\"deeppink\", linewidth = 0.8) +\n  annotate('text', x = 2.1, y = 0.25, label = 'normal', size = 7, color = \"black\") +\n  annotate('text', x = 1.9, y = 0.75, label = 'leptokurtic', size = 8, color = \"deeppink\") +\n  theme_minimal(base_size = 18) +\n  labs(title = \"Leptokurtic distribution\",\n       x = \"Variable\",\n       y = \"Probability density\")\n\n\n\n\n\n(a) Platykurtic distribution (negative excess kurtosis).\n\n\n\n\n\n\n(b) Mesokurtic distribution (zero excess kurtosis).\n\n\n\n\n\n\n(c) Leptokurtic distribution (positive excess kurtosis).\n\n\n\n\nFigure 3.4: Types of distribution according to the summetry."
  },
  {
    "objectID": "normal.html#normal-q-q-plots",
    "href": "normal.html#normal-q-q-plots",
    "title": "3  Normal distribution",
    "section": "\n3.5 Normal Q-Q plots",
    "text": "3.5 Normal Q-Q plots\nThe normal Q–Q plot, or normal quantile-quantile plot, provides an easy way to visually check whether or not a data set is normally distributed. The values in the plot are the quantiles2 of the variable distribution (sample quantiles) plotted against the quantiles of a standard normal distribution (theoretical quantiles). If the points fall close to a straight line at a 45-degree angle, then the data are normally distributed (although the ends of the Q-Q plot often deviate from the straight line).2 Quantiles are values that split sorted data or a probability distribution into equal parts. The most commonly used quantiles have special names. Quartiles: Three quartiles (Q1, median, Q3) split the data into four parts. Percentiles: 99 percentiles split the data into 100 parts.\n\n\n\n\n\n\nNormal q-q plot of age\n\n\n\n\n\nBase R\nggpubr\n\n\n\n\nqqnorm(arrhythmia$age, pch = 1, frame = FALSE)\nqqline(arrhythmia$age, col = \"steelblue\", lwd = 2)\n\n\n\nFigure 3.5: Q-Q plot of age.\n\n\n\n\n\n\n\narrhythmia %&gt;%\n  ggqqplot(\"age\", conf.int = F)\n\n\n\nFigure 3.6: Q-Q plot of age.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormal q-q plot of QRS\n\n\n\n\n\nBase R\nggpubr\n\n\n\n\nqqnorm(arrhythmia$QRS, pch = 1, frame = FALSE)\nqqline(arrhythmia$QRS, col = \"steelblue\", lwd = 2)\n\n\n\nFigure 3.7: Normal Q-Q plot of QRS.\n\n\n\n\n\n\n\narrhythmia %&gt;%\n  ggqqplot(\"QRS\", conf.int = F)\n\n\n\nFigure 3.8: Normal Q-Q plot of QRS."
  },
  {
    "objectID": "inference.html#hypothesis-testsing",
    "href": "inference.html#hypothesis-testsing",
    "title": "4  Foundations for statistical inference",
    "section": "4.1 Hypothesis Testsing",
    "text": "4.1 Hypothesis Testsing\nHypothesis testing is a method of deciding whether the data are consistent with the null hypothesis. The calculation of the p-value is an important part of the procedure. Given a study with a single outcome measure and a statistical test, hypothesis testing can be summarized in five steps.\n\n\n\n\n\n\nSteps of Hypothesis Testsing\n\n\n\nStep 1: State the null hypothesis, \\(H_{0}\\), and alternative hypothesis, \\(H_{1}\\), based on the research question.\n\nNOTE: We decide a non-directional \\(H_{1}\\) (also known as two-sided hypothesis) whether we test for effects in both directions (most common), otherwise a directional (also known as one-sided) hypothesis.\n\nStep 2: Set the level of significance, α (usually 0.05).\nStep 3: Identify the appropriate test statistic and check the assumptions. Calculate the test statistic using the data.\n\nNOTE: There are two basic types of statistical tests and they are described as parametric and non-parametric. The parametric tests (e.g., t-test, ANOVA), make certain assumptions about the distribution of the unknown parameter of interest and thus the test statistic is valid under these assumptions. For non-parametric tests (e.g., Mann-Whitney U test, Kruskal-Wallis test), there are no such assumptions. Most nonparametric tests use some way of ranking the measurements. Non-parametric tests are about 95% as powerful as parametric tests.\n\nStep 4: Decide whether or not the result is statistically significant.\n\nThe p-value is the probability of obtaining the observed results, or something more extreme, if the null hypothesis is true.\n\nUsing the known distribution of the test statistic and according to the result of our statistical test, we calculate the corresponding p-value. Then we compare the p-value to the significance level α:\n\nIf p − value &lt; α, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ α, do not reject the null hypothesis, \\(H_{0}\\).\n\nThe Table 4.1 demonstrates how to interpret the strength of the evidence. However, always keep in mind the size of the study being considered.\n\n\nTable 4.1: ?(caption)\n\n\n\n\n(a) Strength of the evidence against \\(H_{0}\\).\n\n\np-value\nInterpretation\n\n\n\n\n\\(p \\geq{0.10}\\)\nNo evidence to reject \\(H_{0}\\)\n\n\n\\(0.05\\leq p &lt; 0.10\\)\nWeak evidence to reject \\(H_{0}\\)\n\n\n\\(0.01\\leq p &lt; 0.05\\)\nEvidence to reject \\(H_{0}\\)\n\n\n\\(0.001\\leq p &lt; 0.01\\)\nStrong evidence to reject \\(H_{0}\\)\n\n\n\\(p &lt; 0.001\\)\nVery strong evidence to reject \\(H_{0}\\)\n\n\n\n\n\n\nStep 5: Interpret the results."
  },
  {
    "objectID": "inference.html#type-of-errors-in-hypothesis-testing",
    "href": "inference.html#type-of-errors-in-hypothesis-testing",
    "title": "4  Foundations for statistical inference",
    "section": "4.2 Type of Errors in Hypothesis Testing",
    "text": "4.2 Type of Errors in Hypothesis Testing\nIn the framework of hypothesis testing there are two types of errors: Type I error and type II error (Table 4.2).\nType I error: we reject the null hypothesis when it is true (false positive), and conclude that there is an effect when, in reality, there is none. The maximum chance (probability) of making a Type I error is denoted by α (alpha). This is the significance level of the test; we reject the null hypothesis if our p-value is less than the significance level, i.e. if p &lt; a.\nType II error: we do not reject the null hypothesis when it is false (false negative), and conclude that there is no effect when one really exists. The chance of making a Type II error is denoted by β (beta); its compliment, (1 - β), is the power of the test.\n\n\nTable 4.2: Types of error in hypothesis testing.\n\n\n\n\n\n\n\n\n\n\nIn population \\(H_0\\) is\n\n\n\n\n\n\n\nTrue\nFalse\n\n\nDecision based onthe sample\nDo Not Reject \\(H_0\\)\nCorrect decision:\\(1 - \\alpha\\)\nType II error (\\(\\beta\\))\n\n\n\nReject \\(H_0\\)\nType I error (\\(\\alpha\\))\nCorrect decision:\\(1 - \\beta\\) (power of the test)\n\n\n\n\n \nThe power (\\(1 - \\beta\\)), therefore, is the probability of (correctly) rejecting the null hypothesis when it is false; i.e. it is the chance (usually expressed as a percentage) of detecting, as statistically significant, a real treatment effect of a given size. Table 4.3 presents the main factors that can influence the power in a study.\n\n\nTable 4.3: Factors Influencing Power.\n\n\n\n\n\n\nFactor\nInfluence on study’s power\n\n\n\n\nEffect Size  (e.g., mean difference, risk ratio)\nAs effect size increases, power tends to increase (a larger effect size is easier to be detected by the statistical test, leading to a greater probability of a statistically significant result).\n\n\nSample Size\nAs the sample size goes up, power generally goes up (this factor is the most easily manipulated by researchers).\n\n\nStandard deviation\nAs variability decreases, power tends to increase (variability can be reduced by controlling extraneous variables such as inclusion and exclusion criteria defining the sample in a study).\n\n\nSignificance level α\nAs α goes up, power goes up (it would be easier to find statistical significance with a larger α, e.g. α=0.1, compared to a smaller α, e.g. α=0.05)."
  },
  {
    "objectID": "student_t_test.html#research-question-and-hypothesis-testing",
    "href": "student_t_test.html#research-question-and-hypothesis-testing",
    "title": "5  Two-sample t-test (Student’s t-test)",
    "section": "5.1 Research question and Hypothesis Testing",
    "text": "5.1 Research question and Hypothesis Testing\nWe consider the data in depression dataset. In an experiment designed to test the effectiveness of paroxetine for treating bipolar depression, the participants were randomly assigned into two groups (paroxetine Vs placebo). The researchers used the Hamilton Depression Rating Scale (HDRS) to measure the depression state of the participants and wanted to find out if the HDRS score is different in paroxetine group as compared to placebo group at the end of the experiment. The significance level \\(\\alpha\\) was set to 0.05.\nNote: A score of 0–7 in HDRS is generally accepted to be within the normal range, while a score of 20 or higher indicates at least moderate severity.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\\(H_0\\): the means of HDRS in the two groups are equal (\\(\\mu_{1} = \\mu_{2}\\))\n\\(H_1\\): the means of HDRS in the two groups are not equal (\\(\\mu_{1} \\neq \\mu_{2}\\))"
  },
  {
    "objectID": "student_t_test.html#packages-we-need",
    "href": "student_t_test.html#packages-we-need",
    "title": "5  Two-sample t-test (Student’s t-test)",
    "section": "5.2 Packages we need",
    "text": "5.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(PupillometryR)\nlibrary(gtsummary)\nlibrary(ggpubr)\nlibrary(sjstats)\nlibrary(sjPlot)\nlibrary(ggsci)\nlibrary(report)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "student_t_test.html#preraring-the-data",
    "href": "student_t_test.html#preraring-the-data",
    "title": "5  Two-sample t-test (Student’s t-test)",
    "section": "5.3 Preraring the data",
    "text": "5.3 Preraring the data\nWe import the data depression in R:\n\nlibrary(readxl)\ndepression &lt;- read_excel(here(\"data\", \"depression.xlsx\"))\n\n\n\n\n\n\n\nFigure 5.1: Table with data from “depression” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(depression)\n\nRows: 76\nColumns: 2\n$ intervention &lt;chr&gt; \"placebo\", \"placebo\", \"placebo\", \"placebo\", \"placebo\", \"p…\n$ HDRS         &lt;dbl&gt; 19, 21, 28, 22, 22, 28, 23, 17, 19, 20, 26, 23, 23, 22, 1…\n\n\nThe data set depression has 76 patients (rows) and includes two variables (columns). The numeric (&lt;dbl&gt;) HDRS variable and the character (&lt;chr&gt;) intervention variable which should be converted to a factor (&lt;fct&gt;) variable using the factor() function as follows:\n\ndepression &lt;- depression %&gt;% \n  mutate(intervention = factor(intervention))\nglimpse(depression)\n\nRows: 76\nColumns: 2\n$ intervention &lt;fct&gt; placebo, placebo, placebo, placebo, placebo, placebo, pla…\n$ HDRS         &lt;dbl&gt; 19, 21, 28, 22, 22, 28, 23, 17, 19, 20, 26, 23, 23, 22, 1…"
  },
  {
    "objectID": "student_t_test.html#assumptions",
    "href": "student_t_test.html#assumptions",
    "title": "5  Two-sample t-test (Student’s t-test)",
    "section": "5.4 Assumptions",
    "text": "5.4 Assumptions\n\n\n\n\n\n\nCheck if the following assumptions are satisfied\n\n\n\n\nThe data are normally distributed in both groups\nThe data in both groups have similar variance (also named as homogeneity of variance or homoscedasticity)\n\n\n\nA. Explore the characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\nGraphs\nWe can visualize the distribution of HDRS for the two groups:\n\nset.seed(123)\nggplot(depression, aes(x=intervention, y=HDRS)) + \n  geom_flat_violin(aes(fill = intervention), scale = \"count\") +\n  geom_boxplot(width = 0.14, outlier.shape = NA, alpha = 0.5) +\n  geom_point(position = position_jitter(width = 0.05), \n             size = 1.2, alpha = 0.6) +\n  scale_fill_jco() +\n  theme_classic(base_size = 14) +  \n  labs(title = \"Grouped Raincloud Plot: HDRS by intervention\") +\n  theme(legend.position=\"none\")\n\n\n\n\nFigure 5.2: Raincloud plot of HDRS variable stratified by intervention.\n\n\n\n\nThe above figure shows that the data are close to symmetry and the assumption of a normal distribution is reasonable.\n\ndepression %&gt;%\n  ggqqplot(\"HDRS\", color = \"intervention\", conf.int = F) +\n  scale_color_jco() +\n  facet_wrap(~ intervention) + \n  theme(legend.position = \"none\")\n\n\n\n\nFigure 5.3: Normality Q-Q plot for HDRS for paroxetine and placebo.\n\n\n\n\n \nSummary statistics\nThe HDRS summary statistics for each group are:\n\n\n\n\n\n\nSummary statistics by group\n\n\n\n\ndplyrdlookr\n\n\n\nHDRS_summary &lt;- depression %&gt;%\n  group_by(intervention) %&gt;%\n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(HDRS)),\n    min = min(HDRS, na.rm = TRUE),\n    q1 = quantile(HDRS, 0.25, na.rm = TRUE),\n    median = quantile(HDRS, 0.5, na.rm = TRUE),\n    q3 = quantile(HDRS, 0.75, na.rm = TRUE),\n    max = max(HDRS, na.rm = TRUE),\n    mean = mean(HDRS, na.rm = TRUE),\n    sd = sd(HDRS, na.rm = TRUE),\n    skewness = EnvStats::skewness(HDRS, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(HDRS, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\nHDRS_summary\n\n# A tibble: 2 × 12\n  intervention     n    na   min    q1 median    q3   max  mean    sd skewness\n  &lt;fct&gt;        &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 paroxetine      33     0    13    18     21    22    27  20.3  3.65  0.00167\n2 placebo         43     0    14    19     21    24    28  21.5  3.41  0.0276 \n# ℹ 1 more variable: kurtosis &lt;dbl&gt;\n\n\n\n\n\ndepression %&gt;% \n  group_by(intervention) %&gt;% \n  dlookr::describe(HDRS) %&gt;% \n  select(described_variables,  intervention, n, na, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup()\n\n# A tibble: 2 × 11\n  described_variables intervention     n    na  mean    sd   p25   p50   p75\n  &lt;chr&gt;               &lt;fct&gt;        &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 HDRS                paroxetine      33     0  20.3  3.65    18    21    22\n2 HDRS                placebo         43     0  21.5  3.41    19    21    24\n# ℹ 2 more variables: skewness &lt;dbl&gt;, kurtosis &lt;dbl&gt;\n\n\n\n\n\n\n\nThe means are close to medians (20.3 vs 21 and 21.5 vs 21). The skewness is approximately zero (symmetric distribution) and the (excess) kurtosis falls into the acceptable range of [-1, 1] indicating approximately normal distributions for both groups.\n \nNormality test\n\n\nHypothesis testing for Shapiro-Wilk test for normality  \\(H_{0}\\): the data came from a normally distributed population.  \\(H_{1}\\): the data tested are not normally distributed. \n\nIf p − value &lt; 0.05, reject the null hypothesis, \\(H_{0}\\). \nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\).\n\nThe Shapiro-Wilk test for normality for each group is:\n\ndepression %&gt;%\n  group_by(intervention) %&gt;%\n  shapiro_test(HDRS) %&gt;% \n  ungroup()\n\n# A tibble: 2 × 4\n  intervention variable statistic     p\n  &lt;fct&gt;        &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 paroxetine   HDRS         0.976 0.670\n2 placebo      HDRS         0.979 0.614\n\n\nThe tests of normality suggest that the data for the HDRS in both groups are normally distributed (p=0.67 &gt;0.05 and p=0.61 &gt;0.05, respectively).\n\n\n\n\n\n\nImportant\n\n\n\nNormality tests often are not helpful guides\n\nFor small sample sizes, the Shapiro-Wilk test (and other normality tests) has little power to reject the null hypothesis (under-powered test).\nIf the sample size is large normality tests may detect even trivial deviations from the normal distribution (over-powered test).\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe decision about normality of data should be based on a careful consideration of all available information such as graphs (histograms, Q-Q plots), summary and shape measures and statistical tests.\n\n\n \nB. Levene’s test for equality of variances\n\n\nHypothesis testing for Levene’s test for equality of variances \\(H_{0}\\): the variances of data in two groups are equal \\(H_{1}\\): the variances of data in two groups are not equal\n\nIf p − value &lt; 0.05, reject the null hypothesis, \\(H_{0}\\) \nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\)\n\nThe Levene’s test for equality of variances is:\n\ndepression %&gt;% \n  levene_test(HDRS ~ intervention)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1    74     0.176 0.676\n\n\nSince the p-value = 0.676 &gt;0.05, the null hypothesis that the variances of HDRs in two groups are equal is not rejected."
  },
  {
    "objectID": "student_t_test.html#run-the-t-test",
    "href": "student_t_test.html#run-the-t-test",
    "title": "5  Two-sample t-test (Student’s t-test)",
    "section": "5.5 Run the t-test",
    "text": "5.5 Run the t-test\nWe will perform a pooled variance t-test (Student’s t-test) to test the null hypothesis that the mean HDRS score is the same for both groups (paroxetine and placebo).\n\n\nThe formula of the test is given by the t-statistic as follows:\n\\[t = \\frac{\\bar{x}_{1} - \\bar{x}_{2}}{s_{p} \\cdot \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}}}}\\]\nwhere \\(s_{p}\\) is an estimate of the pooled standard deviation of the two groups which is calculated by the following equation:\n\\[s_{p} = \\sqrt{\\frac{(n_{1}-1)s_{1}^2 + (n_{2}-1)s_{2}^2}{n_{1}+ n_{2}-2}}\\]\n\n\n\n\n\n\nStudent’s t-test\n\n\n\n\nBase Rrstatix\n\n\n\n1t.test(HDRS ~ intervention, var.equal = T, data=depression)\n\n\n1\n\nIf we reject the null hypothesis of Levene’s test, we have to type var.equal = F (or type nothing as this is the default), so the Welch’s t-test is applied.\n\n\n\n\n\n    Two Sample t-test\n\ndata:  HDRS by intervention\nt = -1.4185, df = 74, p-value = 0.1602\nalternative hypothesis: true difference in means between group paroxetine and group placebo is not equal to 0\n95 percent confidence interval:\n -2.777498  0.467420\nsample estimates:\nmean in group paroxetine    mean in group placebo \n                20.33333                 21.48837 \n\n\n\n\n\ndepression %&gt;% \n    t_test(HDRS ~ intervention, var.equal = T, detailed = T)                    \n\n# A tibble: 1 × 15\n  estimate estimate1 estimate2 .y.   group1   group2    n1    n2 statistic     p\n*    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1    -1.16      20.3      21.5 HDRS  paroxet… place…    33    43     -1.42  0.16\n# ℹ 5 more variables: df &lt;dbl&gt;, conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt;, method &lt;chr&gt;,\n#   alternative &lt;chr&gt;\n\n\n\n\n\n\n\nThe p-value = 0.16 is greater than 0.05. There is no evidence of a significant difference in mean HDRS between the two groups (failed to reject \\(H_0\\)). The difference between means (20.33 - 21.49) equals to -1.16 units of the HDRS and note that the 95% confidence interval of the difference in means (-2.78 to 0.47) includes the hypothesized null value of 0. Based on these results, there is not evidence that paroxetine is effective as a treatment for bipolar depression.\nNote that the paroxetine group (\\(n_1=33\\)) has df (degrees of freedom) = 33-1 = 32 and the placebo sample (\\(n_2= 43\\)) has df = 43-1 = 42 , so we have df = 32 + 42 = 74 in total. Another way of thinking of this is that the complete sample size is 76, and we have estimated two parameters from the data (the two means), so we have df = 76-2 = 74 .\n\n\nNOTE: The Student t-test does not have any restrictions on \\(n_1\\) and \\(n_2\\) —they can be equal or unequal. However, equal samples are preferred because this maximizes the power to detect a specified difference."
  },
  {
    "objectID": "student_t_test.html#present-the-results",
    "href": "student_t_test.html#present-the-results",
    "title": "5  Two-sample t-test (Student’s t-test)",
    "section": "5.6 Present the results",
    "text": "5.6 Present the results\nSummary table\nIt is common practice to report the mean (sd) for each group in summary tables.\n\n\nShow the code\ndepression %&gt;% \n  tbl_summary(\n    by = intervention, \n    statistic = HDRS ~ \"{mean} ({sd})\", \n    digits = list(everything() ~ 1),\n    label = list(HDRS ~ \"HDRS score\"), \n    missing = c(\"no\")) %&gt;% \n    add_difference(test.args = all_tests(\"t.test\") ~ list(var.equal = TRUE),\n                   estimate_fun = HDRS ~ function(x) style_sigfig(x, digits = 2),\n                   pvalue_fun = function(x) style_pvalue(x, digits = 2)) %&gt;% \n  add_n()\n\n\n\n\n\n\n\n\n\nCharacteristic\nN\nparoxetine, N = 331\nplacebo, N = 431\nDifference2\n95% CI2,3\np-value2\n\n\n\n\nHDRS score\n76\n20.3 (3.7)\n21.5 (3.4)\n-1.2\n-2.8, 0.47\n0.16\n\n\n\n1 Mean (SD)\n\n\n2 Two Sample t-test\n\n\n3 CI = Confidence Interval\n\n\n\n\n\n\n\n\nReport the results\nThere is also a specific package with the name {report} that may be useful in reporting the results of the t-test:\n\nreport_results &lt;- t.test(depression$HDRS ~ depression$intervention, var.equal = T) \nreport(report_results)\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Two Sample t-test testing the difference of depression$HDRS by\ndepression$intervention (mean in group paroxetine = 20.33, mean in group\nplacebo = 21.49) suggests that the effect is negative, statistically not\nsignificant, and small (difference = -1.16, 95% CI [-2.78, 0.47], t(74) =\n-1.42, p = 0.160; Cohen's d = -0.33, 95% CI [-0.78, 0.13])\n\n\n  We can use the above information to write up a final report:\n\n\n\n\n\n\nFinal report\n\n\n\nThere is not evidence that HDRS score is significantly different in paroxetine group, mean = 20.3 (sd = 3.7), as compared to placebo group, 21.5 (3.4), (mean difference= -1.2 units, 95% CI = -2.8 to 0.47, p = 0.16 &gt;0.05)."
  },
  {
    "objectID": "wmw_test.html#research-question-and-hypothesis-testing",
    "href": "wmw_test.html#research-question-and-hypothesis-testing",
    "title": "6  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n6.1 Research question and Hypothesis Testing",
    "text": "6.1 Research question and Hypothesis Testing\nWe consider the data in thromboglobulin dataset that contains the urinary \\(\\beta\\) thromboglobulin excretion (pg/ml) measured in 12 non-diabetic patients and 12 diabetic patients. The researchers used \\(\\alpha\\) = 0.05 significance level to test if the distribution of urinary \\(\\beta\\) thromboglobulin (b_TG) differs in the two groups.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): the distribution of urinary \\(\\beta\\) thromboglobulin is the same in the two groups\n\n\\(H_1\\): the distribution of urinary \\(\\beta\\) thromboglobulin is different in the two groups\n\n\n\n\n\nNOTE: The null hypothesis is that the observations from one group do not tend to have higher or lower ranks than observations from the other group. This test does not compare the medians of the data as is commonly thought, it tests the whole distribution. However, if the distributions of the two groups have similar shapes and spreads (i.e., differing only in location), the WMW test can address (in most cases) whether there are differences in the medians between the two groups.(ref.)"
  },
  {
    "objectID": "wmw_test.html#packages-we-need",
    "href": "wmw_test.html#packages-we-need",
    "title": "6  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n6.2 Packages we need",
    "text": "6.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(exactRankTests)\nlibrary(PupillometryR)\nlibrary(gtsummary)\nlibrary(ggpubr)\nlibrary(sjstats)\nlibrary(sjPlot)\nlibrary(ggsci)\nlibrary(report)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "wmw_test.html#preraring-the-data",
    "href": "wmw_test.html#preraring-the-data",
    "title": "6  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n6.3 Preraring the data",
    "text": "6.3 Preraring the data\nWe import the data thromboglobulin in R:\n\nlibrary(readxl)\ntg &lt;- read_excel(here(\"data\", \"thromboglobulin.xlsx\"))\n\n\n\n\nFigure 6.1: Table with data from “depression” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(tg)\n\nRows: 24\nColumns: 2\n$ status &lt;chr&gt; \"non_diabetic\", \"non_diabetic\", \"non_diabetic\", \"non_diabetic\",…\n$ b_TG   &lt;dbl&gt; 4.1, 6.3, 7.8, 8.5, 8.9, 10.4, 11.5, 12.0, 13.8, 17.6, 24.3, 37…\n\n\nThe data set tg has 24 patients (rows) and includes two variables (columns). The numeric (&lt;dbl&gt;) b_TG variable and the character (&lt;chr&gt;) status variable which should be converted to a factor (&lt;fct&gt;) variable using the factor() function as follows:\n\ntg &lt;- tg %&gt;% \n  mutate(status = factor(status))\nglimpse(tg)\n\nRows: 24\nColumns: 2\n$ status &lt;fct&gt; non_diabetic, non_diabetic, non_diabetic, non_diabetic, non_dia…\n$ b_TG   &lt;dbl&gt; 4.1, 6.3, 7.8, 8.5, 8.9, 10.4, 11.5, 12.0, 13.8, 17.6, 24.3, 37…"
  },
  {
    "objectID": "wmw_test.html#explore-the-characteristics-of-distribution-for-each-group-and-check-for-normality",
    "href": "wmw_test.html#explore-the-characteristics-of-distribution-for-each-group-and-check-for-normality",
    "title": "6  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n6.4 Explore the characteristics of distribution for each group and check for normality",
    "text": "6.4 Explore the characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\n \nGraph\nWe can visualize the distribution of b_TG for the two groups:\n\nset.seed(123)\nggplot(tg, aes(x=status, y=b_TG)) + \n  geom_flat_violin(aes(fill = status), scale = \"count\") +\n  geom_boxplot(width = 0.14, outlier.shape = NA, alpha = 0.5) +\n  geom_point(position = position_jitter(width = 0.05), \n             size = 1.2, alpha = 0.6) +\n  ggsci::scale_fill_jco() +\n  theme_classic(base_size = 14) +\n  theme(legend.position=\"none\", \n        axis.text = element_text(size = 14))\n\n\n\nFigure 6.2: Rain cloud plot.\n\n\n\n\nThe above figure shows that the data in both groups are positively skewed and they have similar shaped distributions.\n\ntg %&gt;%\n  ggqqplot(\"b_TG\", color = \"status\", conf.int = F) +\n  scale_color_jco() +\n  facet_wrap(~ status) + \n  theme(legend.position = \"none\")\n\n\n\nFigure 6.3: Normality Q-Q plot for HDRS for paroxetine and placebo.\n\n\n\n\n \nSummary statistics\nThe b_TG summary statistics for each group are:\n\n\n\n\n\n\nSummary statistics by group\n\n\n\n\n\ndplyr\ndlookr\n\n\n\n\ntg_summary &lt;- tg %&gt;%\n  group_by(status) %&gt;%\n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(b_TG)),\n    min = min(b_TG, na.rm = TRUE),\n    q1 = quantile(b_TG, 0.25, na.rm = TRUE),\n    median = quantile(b_TG, 0.5, na.rm = TRUE),\n    q3 = quantile(b_TG, 0.75, na.rm = TRUE),\n    max = max(b_TG, na.rm = TRUE),\n    mean = mean(b_TG, na.rm = TRUE),\n    sd = sd(b_TG, na.rm = TRUE),\n    skewness = EnvStats::skewness(b_TG, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(b_TG, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\ntg_summary\n\n# A tibble: 2 × 12\n  status           n    na   min    q1 median    q3   max  mean    sd skewness\n  &lt;fct&gt;        &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 diabetic        12     0  23.8 27.2    29.2  34.8  46.2  31.8  7.17     1.05\n2 non_diabetic    12     0   4.1  8.32   11.0  14.8  37.2  13.5  9.19     1.81\n# ℹ 1 more variable: kurtosis &lt;dbl&gt;\n\n\n\n\n\ntg %&gt;% \n  group_by(status) %&gt;% \n  dlookr::describe(b_TG) %&gt;% \n  select(described_variables,  status, n, na, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup()\n\n# A tibble: 2 × 11\n  described_variables status      n    na  mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;fct&gt;   &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 b_TG                diabet…    12     0  31.8  7.17 27.2   29.2  34.8     1.05\n2 b_TG                non_di…    12     0  13.5  9.19  8.32  11.0  14.8     1.81\n# ℹ 1 more variable: kurtosis &lt;dbl&gt;\n\n\n\n\n\n\n\nThe means are not very close to the medians (31.8 vs 29.2 and 13.5 vs 11.0). Moreover, both the skewness (1.81) and the (excess) kurtosis (3.47) for the non-diabetic group falls outside of the acceptable range of [-1, 1] indicating right-skewed and leptokurtic distribution.\n \nNormality test\nThe Shapiro-Wilk test for normality for each group is:\n\ntg %&gt;%\n  group_by(status) %&gt;%\n  shapiro_test(b_TG) %&gt;% \n  ungroup()\n\n# A tibble: 2 × 4\n  status       variable statistic      p\n  &lt;fct&gt;        &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 diabetic     b_TG         0.886 0.105 \n2 non_diabetic b_TG         0.817 0.0148\n\n\nWe can see that the data for the non-diabetic group is not normally distributed (p=0.015 &lt;0.05) according to the Shapiro-Wilk test."
  },
  {
    "objectID": "wmw_test.html#run-the-wilcoxon-mann-whitney-test",
    "href": "wmw_test.html#run-the-wilcoxon-mann-whitney-test",
    "title": "6  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n6.5 Run the Wilcoxon-Mann-Whitney test",
    "text": "6.5 Run the Wilcoxon-Mann-Whitney test\nThe difference in location between two distributions with similar shapes (Figure 6.2) can be tested using the Wilcoxon-Mann-Whitney (WMW) test:\n\n\n\n\n\n\nWilcoxon-Mann-Whitney test\n\n\n\n\n\nBase R\nexactRankTests\nrstatix\n\n\n\n\nwilcox.test(b_TG ~ status, conf.int = T, data = tg)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  b_TG by status\nW = 134, p-value = 0.0001028\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n 13.7 24.0\nsample estimates:\ndifference in location \n                 18.95 \n\n\nHistorical Note: As you can see, in R the Mann-Whitney test is calculated with the wilcox.test() function and it is called Wilcoxon rank-sum test. What is the reason for this? Henry Mann and Donald Whitney (1947) reported in their article that the test was first proposed by Frank Wilcoxon (1945) and they gave their version for the test. So the right would be to call this test Wilcoxon-Mann-Whitney (WMW) test.\n\n\nAlthough a small number of ties should not have a serious impact on our results, in case of ties we can use the wilcox.exact() function from the package {exactRankTests}:\n\nwilcox.exact(b_TG ~ status, conf.int = T, data = tg)\n\n\n    Exact Wilcoxon rank sum test\n\ndata:  b_TG by status\nW = 134, p-value = 0.0001028\nalternative hypothesis: true mu is not equal to 0\n95 percent confidence interval:\n 13.7 24.0\nsample estimates:\ndifference in location \n                 18.95 \n\n\n\n\n\ntg %&gt;%\n  wilcox_test(b_TG ~ status, detailed = T)\n\n# A tibble: 1 × 12\n  estimate .y.   group1  group2    n1    n2 statistic       p conf.low conf.high\n*    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     19.0 b_TG  diabet… non_d…    12    12       134 1.03e-4     13.7        24\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\n\n\n\n\n\nThe result (the median of the difference1 = 18.95, 95%CI: 13.7 to 24) is significant (p &lt;0.001) and we reject the null hypothesis.1 Note: the estimator for the difference in location parameters does not estimate the difference in medians (a common misconception) but rather the median of the difference between the two samples.\nIn general, however, WMW test is regarded as a test of comparing the difference in ranks between the two groups as follows:\n\nmwu(tg, b_TG, status, out = \"browser\")\n\n\nMann-Whitney U-Test\n\n\n\n\n\n\n\n\n\n\n\n\nGroups\nN\nMean Rank\nMann-Whitney U\nWilcoxon W\nZ\nEffect Size\np-value\n\n\ndiabetic\nnon_diabetic\n12\n12\n17.67\n7.33\n212\n134\n3.580\n0.731\n0.000"
  },
  {
    "objectID": "wmw_test.html#present-the-results",
    "href": "wmw_test.html#present-the-results",
    "title": "6  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n6.6 Present the results",
    "text": "6.6 Present the results\nSummary table\nIt is common practice to report the median (IQR) for each group in summary tables.\n\nShow the codetg %&gt;% \n  tbl_summary(\n    by = status, \n    statistic = b_TG ~ \"{median} ({p25}, {p75})\", \n    digits = list(everything() ~ 1),\n    label = list(b_TG ~ \"b_TG \\n(pg/ml)\"), \n    missing = c(\"no\")) %&gt;% \n  add_p(test = b_TG ~ \"wilcox.test\") %&gt;% \n  add_n()\n\n\n\n\n\n\nCharacteristic\nN\n\ndiabetic, N = 121\n\n\nnon_diabetic, N = 121\n\n\np-value2\n\n\n\nb_TG (pg/ml)\n24\n29.2 (27.1, 34.8)\n10.9 (8.3, 14.8)\n&lt;0.001\n\n\n\n\n1 Median (IQR)\n\n\n\n2 Wilcoxon rank sum exact test\n\n\n\n\n\n\nReport the results\nThere is also a specific package with the name {report} that may be useful in reporting the results of WMW test:\n\nreport_results &lt;- wilcox.test(tg$b_TG ~ tg$status) \nreport(report_results)\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Wilcoxon rank sum exact test testing the difference in ranks between\ntg$b_TG and tg$status suggests that the effect is positive, statistically\nsignificant, and very large (W = 134.00, p &lt; .001; r (rank biserial) = 0.86,\n95% CI [0.68, 0.94])\n\n\n \nWe can use the information to write up a final report:\n\n\n\n\n\n\nFinal report\n\n\n\nThere is evidence that the urinary \\(\\beta\\) thromboglobulin excretion is higher in diabetic group, median = 29.2 (IQR: 27.1, 34.8) pg/ml, as compared to non-diabetic group, 10.9 (8.3, 14.8) pg/ml. The WMW test suggests that there is a significant difference in mean ranks between the two groups (17.67 Vs 7.33, p &lt;0.001)."
  },
  {
    "objectID": "paired_t_test.html#research-question",
    "href": "paired_t_test.html#research-question",
    "title": "7  Paired t-test",
    "section": "\n7.1 Research question",
    "text": "7.1 Research question\nThe dataset weight contains the birth and discharge weight of 25 newborns. We might ask if the mean difference of the weight in birth and in discharge equals to zero or not. If the differences between the pairs of measurements are normally distributed, a paired t-test is the most appropriate statistical test.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): the mean difference of weight equals to zero (\\(\\mu_{d} = 0\\))\n\n\\(H_1\\): the mean difference of weight does not equal to zero (\\(\\mu_{d} \\neq 0\\))"
  },
  {
    "objectID": "paired_t_test.html#packages-we-need",
    "href": "paired_t_test.html#packages-we-need",
    "title": "7  Paired t-test",
    "section": "\n7.2 Packages we need",
    "text": "7.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(gtsummary)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "paired_t_test.html#preraring-the-data",
    "href": "paired_t_test.html#preraring-the-data",
    "title": "7  Paired t-test",
    "section": "\n7.3 Preraring the data",
    "text": "7.3 Preraring the data\nWe import the data weight in R:\n\nlibrary(readxl)\nweight &lt;- read_excel(here(\"data\", \"weight.xlsx\"))\n\nWe calculate the differences using the mutate() function:\n\nweight &lt;- weight %&gt;%\n  mutate(dif_weight = birth_weight - discharge_weight)\n\n\n\n\nFigure 7.1: Table with data from “weight” file.\n\n\n\nWe inspect the data:\n\nglimpse(weight) \n\nRows: 25\nColumns: 3\n$ birth_weight     &lt;dbl&gt; 3250, 2680, 2960, 3420, 3210, 2740, 3250, 3170, 2970,…\n$ discharge_weight &lt;dbl&gt; 3220, 2640, 2940, 3350, 3140, 2730, 3220, 3150, 2890,…\n$ dif_weight       &lt;dbl&gt; 30, 40, 20, 70, 70, 10, 30, 20, 80, 103, 84, 42, -15,…"
  },
  {
    "objectID": "paired_t_test.html#assumptions",
    "href": "paired_t_test.html#assumptions",
    "title": "7  Paired t-test",
    "section": "\n7.4 Assumptions",
    "text": "7.4 Assumptions\n\n\n\n\n\n\nCheck if the following assumption is satisfied\n\n\n\nThe differences are normally distributed."
  },
  {
    "objectID": "paired_t_test.html#explore-the-characteristics-of-distribution-of-differences",
    "href": "paired_t_test.html#explore-the-characteristics-of-distribution-of-differences",
    "title": "7  Paired t-test",
    "section": "\n7.5 Explore the characteristics of distribution of differences",
    "text": "7.5 Explore the characteristics of distribution of differences\nThe distribution of the differences can be explored with appropriate plots and summary statistics.\nGraph\nWe can explore the distribution of differences visually for symmetry with a density plot (a smoothed version of the histogram):\n\nweight %&gt;%\n  ggplot(aes(x = dif_weight)) +\n  geom_density(fill = \"#76B7B2\", color=\"black\", alpha = 0.2) +\n  geom_vline(aes(xintercept=mean(dif_weight)),\n             color=\"blue\", linetype=\"dashed\", size=1.4) +\n  geom_vline(aes(xintercept=median(dif_weight)),\n             color=\"red\", linetype=\"dashed\", size=1.2) +\n  labs(x = \"Weight difference\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\")\n\n\n\nFigure 7.2: Density plot of the weight differences.\n\n\n\n\nThe above figure shows that the data are following an approximately symmetrical distribution. Note that the arethmetic mean (blue vertical dashed line) is very close to the median (red vertical dashed line) of the data.\nSummary statistics\nSummary statistics can also be calculated for the variables.\n\n\n\n\n\n\nSummary statistics\n\n\n\n\n\ndplyr\ndlookr\n\n\n\nWe can utilize the across function to obtain the results across the three variables simultaneously:\n\nsummary_weight &lt;- weight %&gt;%\n  dplyr::summarise(across(\n    .cols = c(dif_weight, birth_weight, discharge_weight), \n    .fns = list(\n      n = ~n(),\n      na = ~sum(is.na(.)),\n      min = ~min(., na.rm = TRUE),\n      q1 = ~quantile(., 0.25, na.rm = TRUE),\n      median = ~quantile(., 0.5, na.rm = TRUE),\n      q3 = ~quantile(., 0.75, na.rm = TRUE),\n      max = ~max(., na.rm = TRUE),\n      mean = ~mean(., na.rm = TRUE),\n      sd = ~sd(., na.rm = TRUE),\n      skewness = ~EnvStats::skewness(., na.rm = TRUE),\n      kurtosis= ~EnvStats::kurtosis(., na.rm = TRUE)\n    ),\n    .names = \"{col}_{fn}\")\n    )\n\n# present the results\nsummary_weight &lt;- summary_weight %&gt;% \n  mutate(across(everything(), round, 2)) %&gt;%   # round to 3 decimal places\n  pivot_longer(1:33, names_to = \"Stats\", values_to = \"Values\")  # long format\n\nsummary_weight\n\n# A tibble: 33 × 2\n   Stats               Values\n   &lt;chr&gt;                &lt;dbl&gt;\n 1 dif_weight_n         25   \n 2 dif_weight_na         0   \n 3 dif_weight_min      -30   \n 4 dif_weight_q1        20   \n 5 dif_weight_median    40   \n 6 dif_weight_q3        70   \n 7 dif_weight_max      103   \n 8 dif_weight_mean      39.6 \n 9 dif_weight_sd        32.3 \n10 dif_weight_skewness  -0.16\n# ℹ 23 more rows\n\n\n\n\n\nweight %&gt;% \n  dlookr::describe(dif_weight, birth_weight, discharge_weight) %&gt;% \n  select(described_variables, n, na, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup()\n\n# A tibble: 3 × 10\n  described_variables     n    na   mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 dif_weight             25     0   39.6  32.3    20    40    70   -0.157\n2 birth_weight           25     0 3076.  248.   2960  3150  3210   -0.291\n3 discharge_weight       25     0 3036.  248.   2880  3100  3220   -0.219\n# ℹ 1 more variable: kurtosis &lt;dbl&gt;\n\n\n\n\n\n\n\nAs it was previously mentioned, the mean of the differences (39.64) is close to median (40). Moreover, both the skewness and the kurtosis are approximately zero indicating a symmetric and mesokurtic distribution for the weight differences.\nNormality test\nAdditionally, we can check the statistical test for normality of the differences.\n\n weight %&gt;%\n    shapiro_test(dif_weight)\n\n# A tibble: 1 × 3\n  variable   statistic     p\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1 dif_weight     0.974 0.742\n\n\nThe Shapiro-Wilk test suggests that the weight differences are normally distributed (p=0.74 &gt; 0.05)."
  },
  {
    "objectID": "paired_t_test.html#run-the-paired-t-test",
    "href": "paired_t_test.html#run-the-paired-t-test",
    "title": "7  Paired t-test",
    "section": "\n7.6 Run the paired t-test",
    "text": "7.6 Run the paired t-test\nWe will perform a paired t-test to test the null hypothesis that the mean differences of weight equals to zero.\n\n\n\n\n\n\nPaired t-test\n\n\n\n\n\nBase R (1st way)\nBase R (2nd way)\nrstatix\n\n\n\nOur data are in a wide format. However, we are going to use only the dif_weight variable, inside the t.test():\n\nt.test(weight$dif_weight)\n\n\n    One Sample t-test\n\ndata:  weight$dif_weight\nt = 6.1428, df = 24, p-value = 2.401e-06\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 26.32139 52.95861\nsample estimates:\nmean of x \n    39.64 \n\n\n\n\n\nt.test(weight$birth_weight, weight$discharge_weight, paired = T)\n\n\n    Paired t-test\n\ndata:  weight$birth_weight and weight$discharge_weight\nt = 6.1428, df = 24, p-value = 2.401e-06\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 26.32139 52.95861\nsample estimates:\nmean difference \n          39.64 \n\n\n\n\n\nweight %&gt;% \n  t_test(dif_weight ~ 1, detailed = T)\n\n# A tibble: 1 × 12\n  estimate .y.    group1 group2     n statistic       p    df conf.low conf.high\n*    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     39.6 dif_w… 1      null …    25      6.14 2.40e-6    24     26.3      53.0\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;"
  },
  {
    "objectID": "paired_t_test.html#present-the-results-in-a-summary-table",
    "href": "paired_t_test.html#present-the-results-in-a-summary-table",
    "title": "7  Paired t-test",
    "section": "\n7.7 Present the results in a summary table",
    "text": "7.7 Present the results in a summary table\n\nShow the codetb1 &lt;- weight %&gt;% \n  mutate(id = row_number()) %&gt;% \n  select(-dif_weight) %&gt;% \n  pivot_longer(!id, names_to = \"group\", values_to = \"weights\")\n\ntb1 %&gt;% \n  tbl_summary(by = group, include = -id,\n            label = list(weights ~ \"weights (grams)\"),\n            statistic =  weights ~ \"{mean} ({sd})\") %&gt;%\n  add_difference(test = weights ~ \"paired.t.test\", group = id,\n                 estimate_fun = weights ~ function(x) style_sigfig(x, digits = 3))\n\n\n\n\n\n\nCharacteristic\n\nbirth_weight, N = 251\n\n\ndischarge_weight, N = 251\n\n\nDifference2\n\n\n95% CI2,3\n\n\np-value2\n\n\n\nweights (grams)\n3,076 (248)\n3,036 (248)\n39.6\n26.3, 53.0\n&lt;0.001\n\n\n\n\n1 Mean (SD)\n\n\n\n2 Paired t-test\n\n\n\n3 CI = Confidence Interval\n\n\n\n\n\n\nThere was a significant reduction in weight (mean change = 39.6 g, sd = 32.31) after the discharge (p-value &lt;0.001 that is lower than 0.05; reject \\(H_0\\)). Note that the 95% confidence interval (26.3 to 53.0) doesn’t include the null hypothesized value of zero. However, is this reduction of clinical importance?1 sd for the change is useful information for meta-analytic techniques (see Cochrane Handbook for Systematic Reviews of Interventions)"
  },
  {
    "objectID": "wilcoxon_test.html#research-question",
    "href": "wilcoxon_test.html#research-question",
    "title": "8  Wilcoxon Signed-Rank test",
    "section": "\n8.1 Research question",
    "text": "8.1 Research question\nThe dataset eyes contains thickness of the cornea (in microns) in patients with one eye affected by glaucoma; the other eye is unaffected. We investigate if there is evidence for difference in corneal thickness in affected and unaffected eyes.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\\(H_0\\): The distribution of the differences in thickness of the cornea is symmetrical about zero\n\\(H_1\\): The distribution of the differences in thickness of the cornea is not symmetrical about zero\n\n\n\nNOTE: If we are testing the null hypothesis that the median of the paired rank differences is zero, then the paired rank differences must all come from a symmetrical distribution. Note that we do not have to assume that the distributions of the original populations are symmetrical."
  },
  {
    "objectID": "wilcoxon_test.html#packages-we-need",
    "href": "wilcoxon_test.html#packages-we-need",
    "title": "8  Wilcoxon Signed-Rank test",
    "section": "\n8.2 Packages we need",
    "text": "8.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(PupillometryR)\nlibrary(gtsummary)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "wilcoxon_test.html#preraring-the-data",
    "href": "wilcoxon_test.html#preraring-the-data",
    "title": "8  Wilcoxon Signed-Rank test",
    "section": "\n8.3 Preraring the data",
    "text": "8.3 Preraring the data\nWe import the data eyes in R:\n\nlibrary(readxl)\neyes &lt;- read_excel(here(\"data\", \"eyes.xlsx\"))\n\nWe calculate the differences using the function mutate():\n\neyes &lt;- eyes %&gt;%\n  mutate(dif_thickness = affected_eye - unaffected_eye)\n\n\n\n\nFigure 8.1: Table with data from “eyes” file.\n\n\n\nWe inspect the data:\n\nglimpse(eyes) \n\nRows: 8\nColumns: 3\n$ affected_eye   &lt;dbl&gt; 488, 478, 480, 426, 440, 410, 458, 460\n$ unaffected_eye &lt;dbl&gt; 484, 478, 492, 444, 436, 398, 464, 476\n$ dif_thickness  &lt;dbl&gt; 4, 0, -12, -18, 4, 12, -6, -16"
  },
  {
    "objectID": "wilcoxon_test.html#explore-the-characteristics-of-distribution-of-differences",
    "href": "wilcoxon_test.html#explore-the-characteristics-of-distribution-of-differences",
    "title": "8  Wilcoxon Signed-Rank test",
    "section": "\n8.4 Explore the characteristics of distribution of differences",
    "text": "8.4 Explore the characteristics of distribution of differences\nThe distributions of differences can be explored with appropriate plots and summary statistics.\nGraph\nWe can explore the data visually for symmetry with a density plot.\n\neyes %&gt;%\n  ggplot(aes(x = dif_thickness)) +\n  geom_density(fill = \"#76B7B2\", color=\"black\", alpha = 0.2) +\n  geom_vline(aes(xintercept=mean(dif_thickness)),\n            color=\"blue\", linetype=\"dashed\", size=1.2) +\n  geom_vline(aes(xintercept=median(dif_thickness)),\n            color=\"red\", linetype=\"dashed\", size=1.2) +\n  labs(x = \"Differences of thickness (micron)\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\")\n\n\n\nFigure 8.2: Density plot of the thickness differences.\n\n\n\n\nSummary statistics\nSummary statistics can also be calculated for the variables.\n\n\n\n\n\n\nSummary statistics\n\n\n\n\n\ndplyr\ndlookr\n\n\n\nWe can utilize the across() function to obtain the results across the three variables simultaneously:\n\nsummary_eyes &lt;- eyes %&gt;%\n  dplyr::summarise(across(\n    .cols = c(dif_thickness, affected_eye, unaffected_eye), \n    .fns = list(\n      n = ~n(),\n      na = ~sum(is.na(.)),\n      min = ~min(., na.rm = TRUE),\n      q1 = ~quantile(., 0.25, na.rm = TRUE),\n      median = ~quantile(., 0.5, na.rm = TRUE),\n      q3 = ~quantile(., 0.75, na.rm = TRUE),\n      max = ~max(., na.rm = TRUE),\n      mean = ~mean(., na.rm = TRUE),\n      sd = ~sd(., na.rm = TRUE),\n      skewness = ~EnvStats::skewness(., na.rm = TRUE),\n      kurtosis= ~EnvStats::kurtosis(., na.rm = TRUE)\n    ),\n    .names = \"{col}_{fn}\")\n    )\n\n# present the results\nsummary_eyes &lt;- summary_eyes %&gt;% \n  mutate(across(everything(), round, 2)) %&gt;%   # round to 3 decimal places\n  pivot_longer(1:33, names_to = \"Stats\", values_to = \"Values\")  # long format\n\nsummary_eyes\n\n# A tibble: 33 × 2\n   Stats                  Values\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 dif_thickness_n          8   \n 2 dif_thickness_na         0   \n 3 dif_thickness_min      -18   \n 4 dif_thickness_q1       -13   \n 5 dif_thickness_median    -3   \n 6 dif_thickness_q3         4   \n 7 dif_thickness_max       12   \n 8 dif_thickness_mean      -4   \n 9 dif_thickness_sd        10.7 \n10 dif_thickness_skewness   0.03\n# ℹ 23 more rows\n\n\n\n\n\neyes %&gt;% \n  dlookr::describe(dif_thickness, affected_eye, unaffected_eye) %&gt;% \n  select(described_variables, n, na, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup()\n\n# A tibble: 3 × 10\n  described_variables     n    na  mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 dif_thickness           8     0    -4  10.7  -13     -3    4    0.0295\n2 affected_eye            8     0   455  27.7  436.   459  478.  -0.493 \n3 unaffected_eye          8     0   459  31.3  442    470  480.  -1.11  \n# ℹ 1 more variable: kurtosis &lt;dbl&gt;\n\n\n\n\n\n\n\nThe differences seems to come from a population with a symmetrical distribution and the skewness is close to zero (0.03). However, the (excess) kurtosis equals to -1.37 (platykurtic) and the sample size is small. Therefore, the data may not follow the normal distribution.\nNormality test\nWe can use Shapiro-Wilk test to check for normality of the differences.\n\n eyes %&gt;%\n    shapiro_test(dif_thickness)\n\n# A tibble: 1 × 3\n  variable      statistic     p\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;\n1 dif_thickness     0.944 0.651\n\n\nThe Shapiro-Wilk test suggests that the weight differences are normally distributed (p=0.65 &gt; 0.05). However, here, normality test is not helpful because of the small sample (the test is under-powered)."
  },
  {
    "objectID": "wilcoxon_test.html#run-the-wilcoxon-signed-rank-test",
    "href": "wilcoxon_test.html#run-the-wilcoxon-signed-rank-test",
    "title": "8  Wilcoxon Signed-Rank test",
    "section": "\n8.5 Run the Wilcoxon Signed-Rank test",
    "text": "8.5 Run the Wilcoxon Signed-Rank test\nThe differences between the two measurements can be tested using a rank test such as Wilcoxon Signed-Rank test.\n\n\n\n\n\n\nWilcoxon Signed-Rank test\n\n\n\n\n\nBase R (1st way)\nBase R (2nd way)\nrstatix\n\n\n\n\nwilcox.test(eyes$dif_thickness, conf.int = T)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  eyes$dif_thickness\nV = 7.5, p-value = 0.3088\nalternative hypothesis: true location is not equal to 0\n95 percent confidence interval:\n -16.999946   8.000014\nsample estimates:\n(pseudo)median \n     -5.992207 \n\n\n\n\n\nwilcox.test(eyes$affected_eye, eyes$unaffected_eye, conf.int = T, paired = TRUE)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  eyes$affected_eye and eyes$unaffected_eye\nV = 7.5, p-value = 0.3088\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -16.999946   8.000014\nsample estimates:\n(pseudo)median \n     -5.992207 \n\n\n\n\n\n eyes %&gt;%\n     wilcox_test(dif_thickness ~ 1)\n\n# A tibble: 1 × 6\n  .y.           group1 group2         n statistic     p\n* &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 dif_thickness 1      null model     8       7.5 0.309\n\n\n\n\n\n\n\nThe result is not significant (p = 0.31 &gt; 0.05). However, we can’t be certain that there is not difference in corneal thickness in affected and unaffected eyes because the sample size is very small."
  },
  {
    "objectID": "wilcoxon_test.html#present-the-results-in-a-summary-table",
    "href": "wilcoxon_test.html#present-the-results-in-a-summary-table",
    "title": "8  Wilcoxon Signed-Rank test",
    "section": "\n8.6 Present the results in a summary table",
    "text": "8.6 Present the results in a summary table\n\nShow the codetb2 &lt;- eyes %&gt;%\n  mutate(id = row_number()) %&gt;% \n  select(-dif_thickness) %&gt;% \n  pivot_longer(!id, names_to = \"groups\", values_to = \"thickness\")\n\ntb2 %&gt;% \n  tbl_summary(by = groups, include = -id,\n            label = list(thickness ~ \"thickness (microns)\"),\n            digits = list(everything() ~ 1)) %&gt;%\n  add_p(test = thickness ~ \"paired.wilcox.test\", group = id,\n                 estimate_fun = thickness ~ function(x) style_sigfig(x, digits = 3))\n\n\n\n\n\n\nCharacteristic\n\naffected_eye, N = 81\n\n\nunaffected_eye, N = 81\n\n\np-value2\n\n\n\nthickness (microns)\n459.0 (436.5, 478.5)\n470.0 (442.0, 479.5)\n0.3\n\n\n\n\n1 Median (IQR)\n\n\n\n2 Wilcoxon signed rank test with continuity correction\n\n\n\n\n\n\nThere is not evidence from this small study with patients of glaucoma that the thickness of the cornea in affected eyes, median = 459 \\(\\mu{m}\\) (IQR: 436.5, 478.5), differs from unaffected eyes 470 \\(\\mu{m}\\) (442, 479.5). The result (pseudomedian = -6, 95% CI: -16 to 8) is not significant (p=0.30 &gt;0.05)."
  },
  {
    "objectID": "anova.html#research-question-and-hypothesis-testing",
    "href": "anova.html#research-question-and-hypothesis-testing",
    "title": "9  One-way ANOVA test",
    "section": "\n9.1 Research question and Hypothesis Testing",
    "text": "9.1 Research question and Hypothesis Testing\nWe consider the data in dataDWL dataset. In this example we explore the variations between weight loss according to four different types of diet. The question that may be asked is: does the average weight loss differ according to the diet?\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): all group means are equal (the means of weight loss in the four diets are equal; \\(\\mu_{1} = \\mu_{2} = \\mu_{3} = \\mu_{4}\\))\n\n\\(H_1\\): at least one group mean differs from the others (there is at least one diet with mean weight loss different from the others)"
  },
  {
    "objectID": "anova.html#packages-we-need",
    "href": "anova.html#packages-we-need",
    "title": "9  One-way ANOVA test",
    "section": "\n9.2 Packages we need",
    "text": "9.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(PupillometryR)\nlibrary(gtsummary)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "anova.html#preraring-the-data",
    "href": "anova.html#preraring-the-data",
    "title": "9  One-way ANOVA test",
    "section": "\n9.3 Preraring the data",
    "text": "9.3 Preraring the data\nWe import the data dataDWL in R:\n\nlibrary(readxl)\ndataDWL &lt;- read_excel(here(\"data\", \"dataDWL.xlsx\"))\n\n\n\n\nFigure 9.1: Table with data from “dataDWL” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(dataDWL)\n\nRows: 60\nColumns: 2\n$ WeightLoss &lt;dbl&gt; 9.9, 9.6, 8.0, 4.9, 10.2, 9.0, 9.8, 10.8, 6.2, 8.3, 12.9, 1…\n$ Diet       &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\",…\n\n\nThe dataset dataDWL has 60 participants and includes two variables. The numeric (&lt;dbl&gt;) WeightLoss variable and the character (&lt;chr&gt;) Diet variable (with levels “A”, “B”, “C” and “D”) which should be converted to a factor variable using the factor() function as follows:\n\ndataDWL &lt;- dataDWL %&gt;% \n  mutate(Diet = factor(Diet))\nglimpse(dataDWL)\n\nRows: 60\nColumns: 2\n$ WeightLoss &lt;dbl&gt; 9.9, 9.6, 8.0, 4.9, 10.2, 9.0, 9.8, 10.8, 6.2, 8.3, 12.9, 1…\n$ Diet       &lt;fct&gt; A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, B, B, B, B, B,…"
  },
  {
    "objectID": "anova.html#assumptions",
    "href": "anova.html#assumptions",
    "title": "9  One-way ANOVA test",
    "section": "\n9.4 Assumptions",
    "text": "9.4 Assumptions\n\n\n\n\n\n\nCheck if the following assumptions are satisfied\n\n\n\n\nThe data are normally distributed in all groups\nThe data in all groups have similar variance (also named as homogeneity of variance or homoscedasticity)\n\n\n\nA. Explore the characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\nGraphs\nWe can visualize the distribution of WeightLoss for the four Diet groups:\n\nset.seed(123)\nggplot(dataDWL, aes(x=Diet, y=WeightLoss)) + \n  geom_flat_violin(aes(fill = Diet), scale = \"count\") +\n  geom_boxplot(width = 0.14, outlier.shape = NA, alpha = 0.5) +\n  geom_point(position = position_jitter(width = 0.05), \n             size = 1.2, alpha = 0.6) +\n  ggsci::scale_fill_jco() +\n  theme_classic(base_size = 14) +\n  theme(legend.position=\"none\", \n        axis.text = element_text(size = 14))\n\n\n\nFigure 9.2: Rain cloud plot.\n\n\n\n\nThe above figure shows that the data are close to symmetry and the assumption of a normal distribution is reasonable. Additionally, we can observe that the largest weight loss seems to have been achieved by the participants in C diet.\nSummary statistics\nThe WeightLoss summary statistics for each diet group are:\n\n\n\n\n\n\nSummary statistics by group\n\n\n\n\n\ndplyr\ndlookr\n\n\n\n\nDWL_summary &lt;- dataDWL %&gt;%\n  group_by(Diet) %&gt;%\n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(WeightLoss)),\n    min = min(WeightLoss, na.rm = TRUE),\n    q1 = quantile(WeightLoss, 0.25, na.rm = TRUE),\n    median = quantile(WeightLoss, 0.5, na.rm = TRUE),\n    q3 = quantile(WeightLoss, 0.75, na.rm = TRUE),\n    max = max(WeightLoss, na.rm = TRUE),\n    mean = mean(WeightLoss, na.rm = TRUE),\n    sd = sd(WeightLoss, na.rm = TRUE),\n    skewness = EnvStats::skewness(WeightLoss, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(WeightLoss, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\nDWL_summary\n\n# A tibble: 4 × 12\n  Diet      n    na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 A        15     0   4.9  8.15    9.6  10.5  12.9  9.18  2.30  -0.471    -0.302\n2 B        15     0   3.8  7.85    9.2  10.8  12.7  8.91  2.78  -0.467    -0.515\n3 C        15     0   8.7 10.8    12.2  13    15.1 12.1   1.79  -0.0451   -0.530\n4 D        15     0   5.8  9.5    10.5  11.8  13.7 10.5   2.23  -0.475     0.229\n\n\n\n\n\ndataDWL %&gt;% \n  group_by(Diet) %&gt;% \n  dlookr::describe(WeightLoss) %&gt;% \n  select(described_variables,  Diet, n, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup()\n\n# A tibble: 4 × 10\n  described_variables Diet      n  mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 WeightLoss          A        15  9.18  2.30  8.15   9.6  10.5  -0.471 \n2 WeightLoss          B        15  8.91  2.78  7.85   9.2  10.8  -0.467 \n3 WeightLoss          C        15 12.1   1.79 10.8   12.2  13    -0.0451\n4 WeightLoss          D        15 10.5   2.23  9.5   10.5  11.8  -0.475 \n# ℹ 1 more variable: kurtosis &lt;dbl&gt;\n\n\n\n\n\n\n\nThe means are close to medians and the standard deviations are also similar. Moreover, both skewness and (excess) kurtosis falls into the acceptable range of [-1, 1] indicating approximately normal distributions for all diet groups.\n \nNormality test\nThe Shapiro-Wilk test for normality for each diet group is:\n\ndataDWL %&gt;%\n  group_by(Diet) %&gt;%\n  shapiro_test(WeightLoss) %&gt;% \n  ungroup()\n\n# A tibble: 4 × 4\n  Diet  variable   statistic     p\n  &lt;fct&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1 A     WeightLoss     0.958 0.662\n2 B     WeightLoss     0.941 0.390\n3 C     WeightLoss     0.964 0.768\n4 D     WeightLoss     0.944 0.435\n\n\nThe tests of normality suggest that the data for the WeightLoss in all groups are normally distributed (p &gt; 0.05).\nB. Levene’s test for equality of variances\nThe Levene’s test for equality of variances is:\n\ndataDWL %&gt;% \n  levene_test(WeightLoss ~ Diet)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     3    56     0.600 0.617\n\n\nSince the p = 0.617 &gt; 0.05, the null hypothesis (\\(H_{0}\\): the variances of WeighLoss in four diet groups are equal) can not be rejected."
  },
  {
    "objectID": "anova.html#run-the-one-way-anova-test",
    "href": "anova.html#run-the-one-way-anova-test",
    "title": "9  One-way ANOVA test",
    "section": "\n9.5 Run the one-way ANOVA test",
    "text": "9.5 Run the one-way ANOVA test\nNow, we will perform an one-way ANOVA (with equal variances: Fisher’s classic ANOVA) to test the null hypothesis that the mean weight loss is the same for all the diet groups.\n\n\n\n\n\n\nOne-way ANOVA test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\n# Compute the analysis of variance\nanova_one_way &lt;- aov(WeightLoss ~ Diet, data = dataDWL)\n\n# Summary of the analysis\nsummary(anova_one_way)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nDiet         3  97.33   32.44   6.118 0.00113 **\nResiduals   56 296.99    5.30                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\ndataDWL %&gt;% \n  anova_test(WeightLoss ~ Diet, detailed = T)\n\nANOVA Table (type II tests)\n\n  Effect   SSn     SSd DFn DFd     F     p p&lt;.05   ges\n1   Diet 97.33 296.987   3  56 6.118 0.001     * 0.247\n\n\n\n\n\n\n\nThe statistic F=6.118 indicates the obtained F-statistic = (variation between sample means \\(/\\) variation within the samples). Note that we are comparing to an F-distribution (F-test). The degrees of freedom in the numerator (DFn) and the denominator (DFd) are 3 and 56, respectively (numarator: variation between sample means; denominator: variation within the samples).\nThe p=0.001 is lower than 0.05. There is at least one diet with mean weight loss which is different from the others means.\nFrom ANOVA table provided by the {rstatix} we can also calculate the generalized effect size (ges). The ges is the proportion of variability explained by the factor Diet (SSn) to total variability of the dependent variable (SSn + SSd), so:\n\\[\\ ges= 97.33 / (97.33 + 296.987) = 97.33 / 394.317 = 0.247\\] A ges of 0.247 (24.7%) means that 24.7% of the change in the weight loss can be accounted for the diet conditions.\n \nPresent the results in a summary table\nA summary table can also be presented:\n\nShow the codegt_sum4 &lt;- dataDWL %&gt;% \n  tbl_summary(\n    by = Diet, \n    statistic = WeightLoss ~ \"{mean} ({sd})\", \n    digits = list(everything() ~ 1),\n    label = list(WeightLoss ~ \"Weight Loss (kg)\"), \n    missing = c(\"no\")) %&gt;% \n  add_p(test = WeightLoss ~ \"aov\", purrr::partial(style_pvalue, digits = 2)) %&gt;% \n  as_gt() \n\ngt_sum4\n\n\n\n\n\n\nCharacteristic\n\nA, N = 151\n\n\nB, N = 151\n\n\nC, N = 151\n\n\nD, N = 151\n\n\np-value2\n\n\n\nWeight Loss (kg)\n9.2 (2.3)\n8.9 (2.8)\n12.1 (1.8)\n10.5 (2.2)\n0.001\n\n\n\n\n1 Mean (SD)\n\n\n\n2 One-way ANOVA"
  },
  {
    "objectID": "anova.html#post-hoc-tests",
    "href": "anova.html#post-hoc-tests",
    "title": "9  One-way ANOVA test",
    "section": "\n9.6 Post-hoc tests",
    "text": "9.6 Post-hoc tests\nA significant one-way ANOVA is generally followed up by post-hoc tests to perform multiple pairwise comparisons between groups:\n\n\n\n\n\n\nPost-hoc tests\n\n\n\n\n\nTukey test\nBonferroni\n\n\n\nIt is appropriate to use this test when one desires all the possible comparisons between a large set of means (e.g., 6 or more means) and the variances are supposed to be equal.\n\n# Pairwise comparisons\npwc_Tukey &lt;- dataDWL %&gt;% \n  tukey_hsd(WeightLoss ~ Diet)\n\npwc_Tukey \n\n# A tibble: 6 × 9\n  term  group1 group2 null.value estimate conf.low conf.high   p.adj\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 Diet  A      B               0   -0.273   -2.50      1.95  0.988  \n2 Diet  A      C               0    2.93     0.707     5.16  0.00513\n3 Diet  A      D               0    1.36    -0.867     3.59  0.377  \n4 Diet  B      C               0    3.21     0.980     5.43  0.0019 \n5 Diet  B      D               0    1.63    -0.593     3.86  0.222  \n6 Diet  C      D               0   -1.57    -3.80      0.653 0.252  \n# ℹ 1 more variable: p.adj.signif &lt;chr&gt;\n\n\nThe output contains the following columns of interest:\n\nestimate: estimate of the difference between means of the two groups\nconf.low, conf.high: the lower and the upper end point of the confidence interval at 95% (default)\np.adj: p-value after adjustment for the multiple comparisons.\n\n\n\nAlternatively, we can perform pairwise comparisons using pairwise t-test with the assumption of equal variances (pool.sd = TRUE) and calculate the adjusted p-values using Bonferroni correction:\n\npwc_Bonferroni &lt;- dataDWL %&gt;% \n  pairwise_t_test(\n    WeightLoss ~ Diet, pool.sd = TRUE,\n    p.adjust.method = \"bonferroni\"\n    )\npwc_Bonferroni \n\n# A tibble: 6 × 9\n  .y.        group1 group2    n1    n2        p p.signif   p.adj p.adj.signif\n* &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       \n1 WeightLoss A      B         15    15 0.746    ns       1       ns          \n2 WeightLoss A      C         15    15 0.000954 ***      0.00572 **          \n3 WeightLoss B      C         15    15 0.000344 ***      0.00206 **          \n4 WeightLoss A      D         15    15 0.111    ns       0.669   ns          \n5 WeightLoss B      D         15    15 0.0571   ns       0.343   ns          \n6 WeightLoss C      D         15    15 0.0666   ns       0.399   ns          \n\n\n\n\n\n\n\nPairwise comparisons were carried out using the method of Tukey (or Bonferroni) and the adjusted p-values were calculated.\nThe results in Tukey post hoc table show that the weight loss from diet C seems to be significantly larger than diet A (mean difference = 2.91 kg, 95%CI [0.71, 5.16], p=0.005 &lt;0.05) and diet B (mean difference = 3.21 kg, 95%CI [0.98, 5.43], p=0.002 &lt;0.05)."
  },
  {
    "objectID": "anova.html#welch-one-way-anova",
    "href": "anova.html#welch-one-way-anova",
    "title": "9  One-way ANOVA test",
    "section": "\n9.7 Welch one-way ANOVA",
    "text": "9.7 Welch one-way ANOVA\nIf the variance is different between the groups (unequal variances) then the degrees of freedom associated with the ANOVA test are calculated differently (Welch one-way ANOVA).\n\n# Welch one-way ANOVA test (not assuming equal variance)\n\ndataDWL %&gt;% \n  welch_anova_test(WeightLoss ~ Diet)\n\n# A tibble: 1 × 7\n  .y.            n statistic   DFn   DFd        p method     \n* &lt;chr&gt;      &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      \n1 WeightLoss    60      7.02     3  30.8 0.000989 Welch ANOVA\n\n\nIn this case, the Games-Howell post hoc test (or pairwise t-tests with no assumption of equal variances with Bonferroni correction) can be used to compare all possible combinations of group differences.\nGames-Howell post hoc test\n\n# Pairwise comparisons (Games-Howell)\n\npwc_GH &lt;- dataDWL %&gt;% \n  games_howell_test(WeightLoss ~ Diet)\n\npwc_GH\n\n# A tibble: 6 × 8\n  .y.        group1 group2 estimate conf.low conf.high p.adj p.adj.signif\n* &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 WeightLoss A      B        -0.273   -2.82      2.28  0.991 ns          \n2 WeightLoss A      C         2.93     0.872     4.99  0.003 **          \n3 WeightLoss A      D         1.36    -0.898     3.62  0.371 ns          \n4 WeightLoss B      C         3.21     0.849     5.56  0.005 **          \n5 WeightLoss B      D         1.63    -0.889     4.16  0.308 ns          \n6 WeightLoss C      D        -1.57    -3.60      0.452 0.17  ns"
  },
  {
    "objectID": "kruskal_wallis.html#research-question-and-hypothesis-testing",
    "href": "kruskal_wallis.html#research-question-and-hypothesis-testing",
    "title": "10  Kruskal-Wallis test",
    "section": "\n10.1 Research question and Hypothesis Testing",
    "text": "10.1 Research question and Hypothesis Testing\nWe consider the data in dataVO2 dataset. We wish to compare the VO2max in three different sports (runners, rowers, and triathletes).\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): the distribution of VO2max is the same in all groups (the medians of VO2max in the three sports are the same)\n\n\\(H_1\\): there is at least one group with VO2max distribution different from the others (there is at least one sport with median VO2max different from the others)\n\n\n\nNOTE: The Kruskal-Wallis test should be regarded as a test of dominance between distributions comparing the mean ranks. The null hypothesis is that the observations from one group do not tend to have a higher or lower ranking than observations from the other groups. This test does not test the medians of the data as is commonly thought, it tests the whole distribution. However, if the distributions of the two groups have similar shapes, the Kruskal-Wallis test can be used to determine whether there are differences in the medians in the two groups. In practice, we use the medians to present the results."
  },
  {
    "objectID": "kruskal_wallis.html#packages-we-need",
    "href": "kruskal_wallis.html#packages-we-need",
    "title": "10  Kruskal-Wallis test",
    "section": "\n10.2 Packages we need",
    "text": "10.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(PupillometryR)\nlibrary(gtsummary)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "kruskal_wallis.html#preraring-the-data",
    "href": "kruskal_wallis.html#preraring-the-data",
    "title": "10  Kruskal-Wallis test",
    "section": "\n10.3 Preraring the data",
    "text": "10.3 Preraring the data\nWe import the data dataVO2 in R:\n\nlibrary(readxl)\ndataVO2 &lt;- read_excel(here(\"data\", \"dataVO2.xlsx\"))\n\n\n\n\nFigure 10.1: Table with data from “dataVO2” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(dataVO2)\n\nRows: 30\nColumns: 2\n$ sport  &lt;chr&gt; \"runners\", \"runners\", \"runners\", \"runners\", \"runners\", \"runners…\n$ VO2max &lt;dbl&gt; 73.8, 79.9, 75.5, 72.5, 82.2, 78.3, 77.9, 76.5, 72.3, 80.2, 71.…\n\n\nThe dataset dataVO2 has 30 participants and two variables. The numeric VO2max variable and the sport variable (with levels “roweres”, “runners”, and “triathletes”) which should be converted to a factor variable using the factor() function as follows:\n\ndataVO2 &lt;- dataVO2 %&gt;% \n  mutate(sport = factor(sport))\n\nglimpse(dataVO2)\n\nRows: 30\nColumns: 2\n$ sport  &lt;fct&gt; runners, runners, runners, runners, runners, runners, runners, …\n$ VO2max &lt;dbl&gt; 73.8, 79.9, 75.5, 72.5, 82.2, 78.3, 77.9, 76.5, 72.3, 80.2, 71.…"
  },
  {
    "objectID": "kruskal_wallis.html#explore-the-characteristics-of-distribution-for-each-group-and-check-for-normality",
    "href": "kruskal_wallis.html#explore-the-characteristics-of-distribution-for-each-group-and-check-for-normality",
    "title": "10  Kruskal-Wallis test",
    "section": "\n10.4 Explore the characteristics of distribution for each group and check for normality",
    "text": "10.4 Explore the characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\nGraph\nWe can visualize the distribution of VO2max for the three sport groups:\n\nset.seed(123)\nggplot(dataVO2, aes(x=sport, y=VO2max)) + \n  geom_flat_violin(aes(fill = sport), scale = \"count\") +\n  geom_boxplot(width = 0.14, outlier.shape = NA, alpha = 0.5) +\n  geom_point(position = position_jitter(width = 0.05), \n             size = 1.2, alpha = 0.6) +\n  ggsci::scale_fill_jco() +\n  theme_classic(base_size = 14) +\n  theme(legend.position=\"none\", \n        axis.text = element_text(size = 14))\n\n\n\nFigure 10.2: Rain cloud plot.\n\n\n\n\nThe above figure shows that the data in triathletes group have some outliers. Additionally, we can observe that the runners group seems to have the largest VO2max.\nSummary statistics\nThe VO2max summary statistics for each sport group are:\n\n\n\n\n\n\nSummary statistics by group\n\n\n\n\n\ndplyr\ndlookr\n\n\n\n\nVO2_summary &lt;- dataVO2 %&gt;%\n  group_by(sport) %&gt;%\n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(VO2max)),\n    min = min(VO2max, na.rm = TRUE),\n    q1 = quantile(VO2max, 0.25, na.rm = TRUE),\n    median = quantile(VO2max, 0.5, na.rm = TRUE),\n    q3 = quantile(VO2max, 0.75, na.rm = TRUE),\n    max = max(VO2max, na.rm = TRUE),\n    mean = mean(VO2max, na.rm = TRUE),\n    sd = sd(VO2max, na.rm = TRUE),\n    skewness = EnvStats::skewness(VO2max, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(VO2max, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\nVO2_summary\n\n# A tibble: 3 × 12\n  sport     n    na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 rowe…    10     0  67.1  67.8   69.6  73.1  74.7  70.3  3.04  0.502      -1.53\n2 runn…    10     0  72.3  74.2   77.2  79.5  82.2  76.9  3.39 -0.00950    -1.16\n3 tria…    10     0  63.2  64     65.4  67.6  76.6  67.0  4.40  1.51        1.60\n\n\n\n\n\ndataVO2 %&gt;% \n  group_by(sport) %&gt;% \n  dlookr::describe(VO2max) %&gt;% \n  select(described_variables,  sport, n, na, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup()\n\n# A tibble: 3 × 11\n  described_variables sport       n    na  mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;fct&gt;   &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 VO2max              rowers     10     0  70.3  3.04  67.8  69.6  73.1  0.502  \n2 VO2max              runners    10     0  76.9  3.39  74.2  77.2  79.5 -0.00950\n3 VO2max              triath…    10     0  67.0  4.40  64    65.4  67.6  1.51   \n# ℹ 1 more variable: kurtosis &lt;dbl&gt;\n\n\n\n\n\n\n\nThe sample size is relative small (10 observations in each group). Moreover, the skewness (1.5) and the (excess) kurtosis (1.6) for the triathletes fall outside of the acceptable range of [-1, 1] indicating right-skewed and leptokurtic distribution.\nNormality test\nThe Shapiro-Wilk test for normality for each sport group is:\n\ndataVO2 %&gt;%\n  group_by(sport) %&gt;%\n  shapiro_test(VO2max) %&gt;% \n  ungroup()\n\n# A tibble: 3 × 4\n  sport       variable statistic      p\n  &lt;fct&gt;       &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 rowers      VO2max       0.865 0.0872\n2 runners     VO2max       0.954 0.712 \n3 triathletes VO2max       0.816 0.0229\n\n\nWe can see that the data for the triathletes is not normally distributed (p=0.023 &lt;0.05) according to the Shapiro-Wilk test.\nBy considering all of the information together (small samples, graphs, normality test) the overall decision is against of normality."
  },
  {
    "objectID": "kruskal_wallis.html#run-the-kruskal-wallis-test",
    "href": "kruskal_wallis.html#run-the-kruskal-wallis-test",
    "title": "10  Kruskal-Wallis test",
    "section": "\n10.5 Run the Kruskal-Wallis test",
    "text": "10.5 Run the Kruskal-Wallis test\nNow, we will perform a Kruskal-Wallis test to compare the VO2max in three sports.\n\n\n\n\n\n\nKruskal-Wallis test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nkruskal.test(VO2max ~ sport, data = dataVO2)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  VO2max by sport\nKruskal-Wallis chi-squared = 16.351, df = 2, p-value = 0.0002815\n\n\n\n\n\ndataVO2 %&gt;% \n  kruskal_test(VO2max ~ sport)\n\n# A tibble: 1 × 6\n  .y.        n statistic    df        p method        \n* &lt;chr&gt;  &lt;int&gt;     &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;         \n1 VO2max    30      16.4     2 0.000281 Kruskal-Wallis\n\n\n\n\n\n\n\nThe p-value (&lt;0.001) is lower than 0.05. There is at least one sport in which the VO2max is different from the others.\n \nPresent the results in a summary table\nA summary table can also be presented:\n\nShow the codegt_sum10 &lt;- dataVO2 %&gt;% \n  tbl_summary(\n    by = sport, \n    statistic = VO2max ~ \"{median} ({p25}, {p75})\", \n    digits = list(everything() ~ 1),\n    label = list(VO2max ~ \"VO2max (mL/kg/min)\"), \n    missing = c(\"no\")) %&gt;% \n  add_p(test = VO2max ~ \"kruskal.test\", purrr::partial(style_pvalue, digits = 2)) %&gt;%\n  as_gt() \n\ngt_sum10\n\n\n\n\n\n\nCharacteristic\n\nrowers, N = 101\n\n\nrunners, N = 101\n\n\ntriathletes, N = 101\n\n\np-value2\n\n\n\nVO2max (mL/kg/min)\n69.6 (67.8, 73.1)\n77.2 (74.2, 79.5)\n65.4 (64.0, 67.6)\n&lt;0.001\n\n\n\n\n1 Median (IQR)\n\n\n\n2 Kruskal-Wallis rank sum test"
  },
  {
    "objectID": "kruskal_wallis.html#post-hoc-tests",
    "href": "kruskal_wallis.html#post-hoc-tests",
    "title": "10  Kruskal-Wallis test",
    "section": "\n10.6 Post-hoc tests",
    "text": "10.6 Post-hoc tests\nA significant WMW is generally followed up by post-hoc tests to perform multiple pairwise comparisons between groups:\n\n\n\n\n\n\nPost-hoc tests\n\n\n\n\n\nDunn’s approach\nWMW with Bonferroni\n\n\n\n\n# Pairwise comparisons\npwc_Dunn &lt;- dataVO2 %&gt;% \n  dunn_test(VO2max ~ sport, p.adjust.method = \"bonferroni\")\n\npwc_Dunn \n\n# A tibble: 3 × 9\n  .y.    group1  group2         n1    n2 statistic        p   p.adj p.adj.signif\n* &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       \n1 VO2max rowers  runners        10    10      2.43  1.52e-2 4.57e-2 *           \n2 VO2max rowers  triathletes    10    10     -1.59  1.12e-1 3.37e-1 ns          \n3 VO2max runners triathletes    10    10     -4.01  5.96e-5 1.79e-4 ***         \n\n\n\n\nAlternatively, we can perform pairwise comparisons using pairwise WMW’s test and calculate the adjusted p-values using Bonferroni correction:\n\n# Pairwise comparisons\n\npwc_BW &lt;- dataVO2 %&gt;% \n  pairwise_wilcox_test(VO2max ~ sport, p.adjust.method = \"bonferroni\")\n\npwc_BW\n\n# A tibble: 3 × 9\n  .y.    group1  group2         n1    n2 statistic        p p.adj p.adj.signif\n* &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 VO2max rowers  runners        10    10       8.5 0.002    0.006 **          \n2 VO2max rowers  triathletes    10    10      80.5 0.023    0.07  ns          \n3 VO2max runners triathletes    10    10      93   0.000487 0.001 **          \n\n\n\n\n\n\n\nDunn’s pairwise comparisons were carried out using the method of Bonferroni and adjusting the p-values were calculated.\nThe runners’ VO2max (median= 77.2, IQR=[74.2, 79.5] mL/kg/min) seems to differ significantly (larger based on the medians) from rowers (69.6 [67.8, 73.1] mL/kg/min, p=0.046 &lt;0.05) and triathletes (65.4 [64.0, 67.6] mL/kg/min, p &lt;0.001)."
  },
  {
    "objectID": "correlation.html#research-question-and-hypothesis-testing",
    "href": "correlation.html#research-question-and-hypothesis-testing",
    "title": "11  Correlation",
    "section": "\n11.1 Research question and Hypothesis Testing",
    "text": "11.1 Research question and Hypothesis Testing\nWe consider the data in Birthweight dataset. Let’s say that we want to explore the association between weight (in Kg) and height (in cm) for a sample of 550 infants of 1 month age.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There is no association between the two numeric variables (they are independent)\n\n\\(H_1\\): There is association between the two numeric variables (they are dependent)"
  },
  {
    "objectID": "correlation.html#packages-we-need",
    "href": "correlation.html#packages-we-need",
    "title": "11  Correlation",
    "section": "\n11.2 Packages we need",
    "text": "11.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(XICOR)\nlibrary(ggExtra)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "correlation.html#preraring-the-data",
    "href": "correlation.html#preraring-the-data",
    "title": "11  Correlation",
    "section": "\n11.3 Preraring the data",
    "text": "11.3 Preraring the data\nWe import the data BirthWeight in R:\n\nlibrary(readxl)\nBirthWeight &lt;- read_excel(here(\"data\", \"BirthWeight.xlsx\"))\n\n\n\n\nFigure 11.1: Table with data from “BirthWeight” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(BirthWeight)\n\nRows: 550\nColumns: 7\n$ id        &lt;chr&gt; \"L001\", \"L003\", \"L004\", \"L005\", \"L006\", \"L007\", \"L008\", \"L00…\n$ weight    &lt;dbl&gt; 4.0, 4.6, 4.8, 3.9, 4.6, 3.6, 3.6, 4.5, 5.0, 3.7, 4.2, 4.4, …\n$ height    &lt;dbl&gt; 55.5, 57.0, 56.0, 56.0, 55.0, 51.5, 56.0, 57.0, 58.5, 52.0, …\n$ headc     &lt;dbl&gt; 37.5, 38.5, 38.5, 39.0, 39.5, 34.5, 38.0, 39.7, 39.0, 38.0, …\n$ gender    &lt;chr&gt; \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Femal…\n$ education &lt;chr&gt; \"tertiary\", \"tertiary\", \"year12\", \"tertiary\", \"year10\", \"ter…\n$ parity    &lt;chr&gt; \"2 or more siblings\", \"Singleton\", \"2 or more siblings\", \"On…\n\n\nThe data set BirthWeight has 550 infants of 1 month age (rows) and includes seven variables (columns). Both the weight and height are numeric (&lt;dbl&gt;) variables."
  },
  {
    "objectID": "correlation.html#plot-the-data",
    "href": "correlation.html#plot-the-data",
    "title": "11  Correlation",
    "section": "\n11.4 Plot the data",
    "text": "11.4 Plot the data\nA first step that is usually useful in studying the association between two numeric variables is to prepare a scatter plot of the data. The pattern made by the points plotted on the scatter plot usually suggests the basic nature and strength of the association between two variables.\n\np &lt;- ggplot(BirthWeight, aes(height, weight)) +\n  geom_point(color = \"blue\", size = 2) +\n  theme_minimal(base_size = 14)\n\nggMarginal(p, type = \"histogram\", \n           xparams = list(fill = 7),\n           yparams = list(fill = 3))\n\n\n\nFigure 11.2: Scatter plot of the association between height and weight in 550 infants of 1 month age.\n\n\n\n\nThe points in the scatter plot seem to be scattered around an invisible line. The scatter plot also shows that, in general, infants with high height tend to have high weight (positive association).\nAdditionally, the marginal histograms show that the data are approximately normally distributed (we have a large sample so the graphs are reliable) for both weight and height."
  },
  {
    "objectID": "correlation.html#correlation-between-two-numeric-variables",
    "href": "correlation.html#correlation-between-two-numeric-variables",
    "title": "11  Correlation",
    "section": "\n11.5 Correlation between two numeric variables",
    "text": "11.5 Correlation between two numeric variables\nCorrelation coefficients\nPearson’s coefficient measures linear correlation, while the Spearman’s and Kendall’s coefficients compare the ranks of data and measures monotonic associations. These coefficients are very powerful for detecting linear or monotonic associations, respectively. The new \\(ξ\\) correlation coefficient is more appropriate to measure the strength of non-monotonic associations.\nNote that the correlation between variables X and Y is equal to the correlation between variables Y and X so the order of the variables in the functions does not matter. The four correlations coefficients are:\n\n\n\n\n\n\nCorrelation coefficients\n\n\n\n\n\nPearson’s \\(r\\)\nSpearman’s \\(r_{s}\\)\nKendall’s \\(\\tau\\)\nCoefficient \\(ξ\\)\n\n\n\nThe Pearson’s correlation coefficient can be calculated for any dataset with two numeric variables. However, before we calculate the Pearson’s coefficient we should make sure that the following assumptions are met:\nAssumptions for Pearson’s \\(r\\) coefficient\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\nThere is a linear association between the two variables.\nFor a valid hypothesis testing and calculation of confidence intervals both variables should have an approximately normal distribution.\n\nAbsence of outliers in the data set.\n\nPearson’s \\(r\\) coefficient is a dimensionless quantity that takes a value in the range -1 to +1. A positive value indicates that both variables increase (or decrease) together while a negative coefficient indicates that one variable decreases as the other variable increases and vice versa. The stronger the correlation, the closer the correlation coefficient comes to ±1.\n\ncor(BirthWeight$height, BirthWeight$weight)\n\n[1] 0.7127154\n\n\n\n\nThe basic idea of Spearman’s rank correlation is that the ranks of X and Y are obtained by first separately ordering their values from small to large and then computing the correlation between the two sets of ranks. The strength of correlation is denoted by the coefficient of rank correlation, named Spearman’s rank correlation coefficient, \\(r_{s}\\).\nAssumptions for Spearman’s \\(r_{s}\\) coefficient\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\nThere is a monotonic association between the two variables. In a monotonic association, the variables tend to move in the same relative direction, but not necessarily at a constant rate. So all linear correlations are monotonic but the opposite is not always true, because we can have also monotonic non-linear associations.\n\nSpearman’s \\(rho\\) coefficient is dimensionless quantity that take value in the range -1 to +1. A positive correlation coefficient indicates that both variables increase (or decrease) in value together and a negative coefficient indicates that one variable decreases in value as the other variable increases and vice versa. The stronger the correlation, the closer the correlation coefficient comes to ±1.\n\ncor(BirthWeight$height, BirthWeight$weight, method = \"spearman\")\n\n[1] 0.7109399\n\n\n\n\nThe Kendall’s \\(\\tau\\) coefficient is the best alternative to Spearman’s \\(rho\\) correlation when the sample size is small and has many tied ranks. It is used to test the similarities in the ordering of data when it is ranked by quantities. Kendall’s correlation coefficient uses pairs of observations and determines the strength of association based on the patter on concordance and discordance between the pairs.\nAssumptions for Kendall’s \\(\\tau\\) coefficient\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\nThere is a monotonic association between the two variables. In a monotonic association, the variables tend to move in the same relative direction, but not necessarily at a constant rate. So all linear correlations are monotonic but the opposite is not always true, because we can have also monotonic non-linear associations.\n\nKendall’s \\(\\tau\\) coefficient is dimensionless quantity that takes value in the range -1 to +1. A positive correlation coefficient indicates that both variables increase (or decrease) in value together and a negative coefficient indicates that one variable decreases in value as the other variable increases and vice versa. The stronger the correlation, the closer the correlation coefficient comes to ±1.\n\ncor(BirthWeight$height, BirthWeight$weight, method = \"kendal\")\n\n[1] 0.5511955\n\n\n\n\nThe correlation coefficient \\(ξ\\) converges to a limit which has an easy interpretation as a measure of dependence. The limit ranges from 0 to 1. It is 1 if and only if Y is a measurable function of X and 0 if and only if X and Y are independent. Thus, \\(ξ\\) gives an actual measure of the strength of the association and it can be used for non-monotonic associations. However, for monotonic associations, it does not indicate the direction of the association.\nAssumptions for \\(ξ\\) coefficient\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\n\n\nxicor(BirthWeight$height, BirthWeight$weight)\n\n[1] 0.3379234\n\n\n\n\n\n\n\nCorrelation tests\nA correlation test is used to test whether the correlation (denoted ρ) between two numeric variables is significantly different from 0 or not in the population.\n\n\n\n\n\n\nHypothesis Testing for correlation coefficients\n\n\n\n\n\nPearson’s \\(r\\) test\nSpearman’s \\(r_{s}\\) test\nKendal’s \\(\\tau\\) test\nCoefficient \\(ξ\\) test\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\\(H_0\\): There is no linear association between the two numeric variables (they are independent, \\(ρ = 0\\))\n\n\\(H_1\\): There is linear association between the two numeric variables (they are dependent, \\(ρ \\neq 0\\))\n\n\ncor.test(BirthWeight$height, BirthWeight$weight) # the default method is \"pearson\"\n\n\n    Pearson's product-moment correlation\n\ndata:  BirthWeight$height and BirthWeight$weight\nt = 23.785, df = 548, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6689714 0.7515394\nsample estimates:\n      cor \n0.7127154 \n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\\(H_0\\): There is no monotonic association between the two numeric variables (they are independent)\n\n\\(H_1\\): There is monotonic association between the two numeric variables (they are dependent)\n\n\ncor.test(BirthWeight$height, BirthWeight$weight, method = \"spearman\")\n\nWarning in cor.test.default(BirthWeight$height, BirthWeight$weight, method =\n\"spearman\"): Cannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  BirthWeight$height and BirthWeight$weight\nS = 8015368, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.7109399 \n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\\(H_0\\): There is no monotonic association between the two numeric variables (they are independent)\n\n\\(H_1\\): There is monotonic association between the two numeric variables (they are dependent)\n\n\ncor.test(BirthWeight$height, BirthWeight$weight, method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  BirthWeight$height and BirthWeight$weight\nz = 18.34, p-value &lt; 2.2e-16\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.5511955 \n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\\(H_0\\): There is not association between the two numeric variables (they are independent)\n\n\\(H_1\\): There is association between the two numeric variables (they are dependent)\n\n\nxicor(BirthWeight$height, BirthWeight$weight, pvalue = TRUE)\n\n$xi\n[1] 0.3379234\n\n$sd\n[1] 0.02701652\n\n$pval\n[1] 0"
  },
  {
    "objectID": "chi_square.html#research-question-and-hypothesis-testing",
    "href": "chi_square.html#research-question-and-hypothesis-testing",
    "title": "12  Chi-sqaure test of independence",
    "section": "\n12.1 Research question and Hypothesis Testing",
    "text": "12.1 Research question and Hypothesis Testing\nWe will use the “Survival from Malignant Melanoma” dataset named “meldata”. The data consist of measurements made on patients with malignant melanoma, a type of skin cancer. Each patient had their tumor removed by surgery at the Department of Plastic Surgery, University Hospital of Odense, Denmark, between 1962 and 1977.\nSuppose we are interested in the association between tumor ulceration and death from melanoma.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There is no association between the two categorical variables (they are independent)\n\n\\(H_1\\): There is association between the two categorical variables (they are dependent)\n\n\n\nNOTE: In practice, the null hypothesis of independence, for our particular question, is no difference in the proportion of patients with ulcerated tumors who die compared with non-ulcerated tumors (\\(p_{ulcerated} = p_{non-ucerated}\\))."
  },
  {
    "objectID": "chi_square.html#packages-we-need",
    "href": "chi_square.html#packages-we-need",
    "title": "12  Chi-sqaure test of independence",
    "section": "\n12.2 Packages we need",
    "text": "12.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(ggsci)\nlibrary(patchwork)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "chi_square.html#preraring-the-data",
    "href": "chi_square.html#preraring-the-data",
    "title": "12  Chi-sqaure test of independence",
    "section": "\n12.3 Preraring the data",
    "text": "12.3 Preraring the data\nWe import the data meldata in R:\n\nlibrary(readxl)\nmeldata &lt;- read_excel(here(\"data\", \"meldata.xlsx\"))\n\n\n\n\nFigure 12.1: Table with data from “meldata” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(meldata)\n\nRows: 205\nColumns: 7\n$ time      &lt;dbl&gt; 10, 30, 35, 99, 185, 204, 210, 232, 232, 279, 295, 355, 386,…\n$ status    &lt;chr&gt; \"Alive\", \"Alive\", \"Alive\", \"Alive\", \"Died\", \"Died\", \"Died\", …\n$ sex       &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"F…\n$ age       &lt;dbl&gt; 76, 56, 41, 71, 52, 28, 77, 60, 49, 68, 53, 64, 68, 63, 14, …\n$ year      &lt;dbl&gt; 1972, 1968, 1977, 1968, 1965, 1971, 1972, 1974, 1968, 1971, …\n$ thickness &lt;dbl&gt; 6.76, 0.65, 1.34, 2.90, 12.08, 4.84, 5.16, 3.22, 12.88, 7.41…\n$ ulcer     &lt;chr&gt; \"Present\", \"Absent\", \"Absent\", \"Absent\", \"Present\", \"Present…\n\n\nThe data set meldata has 250 patients (rows) and includes seven variables (columns). We are interested in the character (&lt;chr&gt;) ulcer variable and the character (&lt;chr&gt;) status variable which should be converted to factor (&lt;fct&gt;) variables using the convert_as_factor() function as follows:\n\nmeldata &lt;- meldata %&gt;%\n  convert_as_factor(status, ulcer)\n\nglimpse(meldata)\n\nRows: 205\nColumns: 7\n$ time      &lt;dbl&gt; 10, 30, 35, 99, 185, 204, 210, 232, 232, 279, 295, 355, 386,…\n$ status    &lt;fct&gt; Alive, Alive, Alive, Alive, Died, Died, Died, Alive, Died, D…\n$ sex       &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"F…\n$ age       &lt;dbl&gt; 76, 56, 41, 71, 52, 28, 77, 60, 49, 68, 53, 64, 68, 63, 14, …\n$ year      &lt;dbl&gt; 1972, 1968, 1977, 1968, 1965, 1971, 1972, 1974, 1968, 1971, …\n$ thickness &lt;dbl&gt; 6.76, 0.65, 1.34, 2.90, 12.08, 4.84, 5.16, 3.22, 12.88, 7.41…\n$ ulcer     &lt;fct&gt; Present, Absent, Absent, Absent, Present, Present, Present, …"
  },
  {
    "objectID": "chi_square.html#plot-the-data",
    "href": "chi_square.html#plot-the-data",
    "title": "12  Chi-sqaure test of independence",
    "section": "\n12.4 Plot the data",
    "text": "12.4 Plot the data\nWe are interested in the association between tumor ulceration and death from melanoma. It is useful to plot the data as counts but also as percentages. It is percentages we are comparing, but we really want to know the absolute numbers as well.\n\np1 &lt;- meldata %&gt;%\n  ggplot(aes(x = ulcer, fill = status)) +\n  geom_bar(width = 0.7) +\n  scale_fill_jco() +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"bottom\")\n\n\np2 &lt;- meldata %&gt;%\n  ggplot(aes(x = ulcer, fill = status)) +\n  geom_bar(position = \"fill\", width = 0.7) +\n  scale_y_continuous(labels=scales::percent) +\n  scale_fill_jco() +\n  ylab(\"Percentage\") +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"bottom\") \n\np1 + p2 + \n  plot_layout(guides = \"collect\") & theme(legend.position = 'bottom')\n\n\n\nFigure 12.2: Bar plot.\n\n\n\n\nJust from the plot, death from melanoma in the ulcerated tumor group is around 40% and in the non-ulcerated group around 13%. The number of patients included in the study is not huge, however, this still looks like a real difference given its effect size."
  },
  {
    "objectID": "chi_square.html#contigency-table-and-expected-frequencies",
    "href": "chi_square.html#contigency-table-and-expected-frequencies",
    "title": "12  Chi-sqaure test of independence",
    "section": "\n12.5 Contigency table and Expected frequencies",
    "text": "12.5 Contigency table and Expected frequencies\nFirst, we will create a contingency 2x2 table (two categorical variables with exactly two levels each) with the frequencies using the Base R.\n\ntb1 &lt;- table(meldata$ulcer, meldata$status)\ntb1\n\n         \n          Alive Died\n  Absent     99   16\n  Present    49   41\n\n\nNext, we will also create a more informative table with row percentages and marginal totals.\n\n\n\n\n\n\nTable with row percentages and marginal totals\n\n\n\n\n\nfinalfit\nmodelsummary\n\n\n\nUsing the function summary_factorlist() which is included in finalfit package for obtaining row percentages and marginal totals:\n\nrow_tb1 &lt;- meldata %&gt;%\n  finalfit::summary_factorlist(dependent = \"status\", add_dependent_label = T,\n                     explanatory = \"ulcer\", add_col_totals = T,\n                     include_col_totals_percent = F,\n                     column = FALSE, total_col = TRUE)\n\nknitr::kable(row_tb1) \n\n\n\nDependent: status\n\nAlive\nDied\nTotal\n\n\n\nTotal N\n\n148\n57\n205\n\n\nulcer\nAbsent\n99 (86.1)\n16 (13.9)\n115 (100)\n\n\n\nPresent\n49 (54.4)\n41 (45.6)\n90 (100)\n\n\n\n\n\n\n\nThe contingency table using the datasummary_crosstab() from the modelsummary package:\n\nmodelsummary::datasummary_crosstab(ulcer ~ status, data = meldata)\n\n\n\nulcer\n\nAlive\nDied\nAll\n\n\n\nAbsent\nN\n99\n16\n115\n\n\n\n% row\n86.1\n13.9\n100.0\n\n\nPresent\nN\n49\n41\n90\n\n\n\n% row\n54.4\n45.6\n100.0\n\n\nAll\nN\n148\n57\n205\n\n\n\n% row\n72.2\n27.8\n100.0\n\n\n\n\n\n\n\n\n\n\nFrom the raw frequencies, there seems to be a large difference, as we noted in the plot we made above. The proportion of patients with ulcerated tumors who die equals to 45.6% compared with non-ulcerated tumors 13.9%."
  },
  {
    "objectID": "chi_square.html#assumptions",
    "href": "chi_square.html#assumptions",
    "title": "12  Chi-sqaure test of independence",
    "section": "\n12.6 Assumptions",
    "text": "12.6 Assumptions\n\n\n\n\n\n\nCheck if the following assumption is satisfied\n\n\n\nA commonly stated assumption of the chi-square test is the requirement to have an expected count of at least 5 in each cell of the 2x2 table.\nFor larger tables, all expected counts should be &gt; 1 and no more than 20% of all cells should have expected counts &lt; 5.\n\n\nWe can calculate the expected frequencies for each cell using the expected() function from {epitools} package:\n\nepitools::expected(tb1)\n\n         \n             Alive     Died\n  Absent  83.02439 31.97561\n  Present 64.97561 25.02439\n\n\nHere, as we observe the assumption is fulfilled."
  },
  {
    "objectID": "chi_square.html#run-pearsons-chi-square-test",
    "href": "chi_square.html#run-pearsons-chi-square-test",
    "title": "12  Chi-sqaure test of independence",
    "section": "\n12.7 Run Pearson’s chi-square test",
    "text": "12.7 Run Pearson’s chi-square test\nFinally, we run the chi-square test:\n\n\n\n\n\n\nchi-square test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nchisq.test(tb1)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tb1\nX-squared = 23.631, df = 1, p-value = 1.167e-06\n\n\n\n\n\nchisq_test(tb1)\n\n# A tibble: 1 × 6\n      n statistic          p    df method          p.signif\n* &lt;int&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;           &lt;chr&gt;   \n1   205      23.6 0.00000117     1 Chi-square test ****    \n\n\n\n\n\n\n\nThere is evidence for an association between the ulcer and status (reject \\(H_0\\)). The proportion of patients with ulcerated tumors who died (45.6%) is significant larger compared with non-ulcerated tumors (13.9%) (p&lt;0.001)."
  },
  {
    "objectID": "chi_square.html#risk-ratio-and-odds-ratio",
    "href": "chi_square.html#risk-ratio-and-odds-ratio",
    "title": "12  Chi-sqaure test of independence",
    "section": "\n12.8 Risk Ratio and Odds ratio",
    "text": "12.8 Risk Ratio and Odds ratio\nRisk ratio\nFrom the data in the following table\n\nepitools::table.margins(tb1)\n\n         \n          Alive Died Total\n  Absent     99   16   115\n  Present    49   41    90\n  Total     148   57   205\n\n\nwe can calculate the risk ratio by hand: \\[ Risk \\ Ratio = \\frac{\\frac{41}{90}}{\\frac{16}{115}} =\\frac{0.4556}{0.1391} = 3.27\\]\nThe risk ratio with the 95% CI using R:\n\nepitools::riskratio(tb1)$measure\n\n         risk ratio with 95% C.I.\n          estimate    lower    upper\n  Absent  1.000000       NA       NA\n  Present 3.274306 1.970852 5.439819\n\n\nThe risk of dying is 3.27 (95% CI: 1.97, 5.4) times higher for patients with ulcerated tumors compared to non-ulcerated tumors.\nOdds ratio\nWe can also calculate the odds ratio by hand: \\[ Odds \\ Ratio = \\frac{\\frac{41}{49}}{\\frac{16}{99}} =\\frac{0.837}{0.162} = 5.17\\] The odds ratio with the 95% CI using R:\n\nepitools::oddsratio(tb1, method = \"wald\")$measure\n\n         odds ratio with 95% C.I.\n          estimate    lower   upper\n  Absent  1.000000       NA      NA\n  Present 5.177296 2.645152 10.1334\n\n\nThe odds of dying is 5.17 (95% CI: 2.65, 10.13) times higher for patients with ulcerated tumors compared to non-ulcerated tumors patients.\nFinnaly, we can also reverse the odds ratio: \\[ \\frac{1}{OR} = \\frac{1}{5.17} = 0.193\\]\n\nepitools::oddsratio(tb1, method = \"wald\", rev = \"rows\")$measure\n\n         odds ratio with 95% C.I.\n          estimate      lower   upper\n  Present 1.000000         NA      NA\n  Absent  0.193151 0.09868354 0.37805\n\n\nThe non-ulcerated tumors patients has 0.193 (95% CI: 0.098, 0.378) times the odds (of dying) of the ulcerated tumors. This means that the non-ulcerated tumors patients has (0.193 - 1= -0.807) 80.7% lower odds of dying than ulcerated tumors."
  },
  {
    "objectID": "fisher_exact.html#research-question-and-hypothesis-testing",
    "href": "fisher_exact.html#research-question-and-hypothesis-testing",
    "title": "13  Fisher’s exact test",
    "section": "\n13.1 Research question and Hypothesis Testing",
    "text": "13.1 Research question and Hypothesis Testing\nWe consider the data in hemophilia dataset. In a survey there are two treatment regimens studied for controlling bleeding in 28 patients with hemophilia undergoing surgery. We want to investigate if there is an association between the treatment regimen (treatment A or B) and the bleeding complications (no or yes). The null hypothesis (\\(H_0\\)) is that the bleeding complications are independent from the treatment regimen, while the alternative (\\(H_1\\)) is that are dependent.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There is no association between the two categorical variables (they are independent)\n\n\\(H_1\\): There is association between the two categorical variables (they are dependent)\n\n\n\nNOTE: In practice, the null hypothesis of independence, for our particular question, is no difference in the proportion of patients with bleeding complications compared with patients with no bleeding complications (\\(p_{bleeding} = p_{no bleeding}\\))."
  },
  {
    "objectID": "fisher_exact.html#packages-we-need",
    "href": "fisher_exact.html#packages-we-need",
    "title": "13  Fisher’s exact test",
    "section": "\n13.2 Packages we need",
    "text": "13.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(ggsci)\nlibrary(patchwork)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "fisher_exact.html#preraring-the-data",
    "href": "fisher_exact.html#preraring-the-data",
    "title": "13  Fisher’s exact test",
    "section": "\n13.3 Preraring the data",
    "text": "13.3 Preraring the data\nWe import the data meldata in R:\n\nlibrary(readxl)\nhemophilia &lt;- read_excel(here(\"data\", \"hemophilia.xlsx\"))\n\n\n\n\nFigure 13.1: Table with data from “meldata” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(hemophilia)\n\nRows: 28\nColumns: 2\n$ treatment &lt;chr&gt; \"A\", \"A\", \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"A\", \"A\", \"B\", \"A\", …\n$ bleeding  &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\"…\n\n\nThe dataset hemophilia has 28 patients (rows) and includes 2 variables (columns), the character (&lt;chr&gt;) variable named treatment and the character (&lt;chr&gt;) variable named bleeding. Both of them should be converted to factor (&lt;fct&gt;) variables using the convert_as_factor() function as follows:\n\nhemophilia &lt;- hemophilia %&gt;%\n  convert_as_factor(treatment, bleeding)\n\nglimpse(hemophilia)\n\nRows: 28\nColumns: 2\n$ treatment &lt;fct&gt; A, A, A, B, A, B, B, A, A, A, B, A, B, B, A, A, B, B, B, A, …\n$ bleeding  &lt;fct&gt; no, no, no, yes, no, no, no, no, yes, no, no, no, yes, no, n…"
  },
  {
    "objectID": "fisher_exact.html#plot-the-data",
    "href": "fisher_exact.html#plot-the-data",
    "title": "13  Fisher’s exact test",
    "section": "\n13.4 Plot the data",
    "text": "13.4 Plot the data\nWe count the number of patients with bleeding in the two regimens. It is useful to plot this as counts but also as percentages and compare them.\n\np3 &lt;- hemophilia %&gt;%\n  ggplot(aes(x = treatment, fill = bleeding)) +\n  geom_bar(width = 0.7) +\n  scale_fill_jama() +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"bottom\")\n\n\np4 &lt;- hemophilia %&gt;%\n  ggplot(aes(x = treatment, fill = bleeding)) +\n  geom_bar(position = \"fill\", width = 0.7) +\n  scale_y_continuous(labels=scales::percent) +\n  scale_fill_jama() +\n  ylab(\"Percentage\") +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"bottom\") \n\np3 + p4 + \n  plot_layout(guides = \"collect\") & theme(legend.position = 'bottom')\n\n\n\nFigure 13.2: Bar plot.\n\n\n\n\nThe above bar plots with counts show graphically that the number of patients who had bleeding complications was similar in the two regimens. Note that the number of patients included in the study is small (n=28)."
  },
  {
    "objectID": "fisher_exact.html#contigency-table-and-expected-frequencies",
    "href": "fisher_exact.html#contigency-table-and-expected-frequencies",
    "title": "13  Fisher’s exact test",
    "section": "\n13.5 Contigency table and Expected frequencies",
    "text": "13.5 Contigency table and Expected frequencies\nFirst, we will create a contingency 2x2 table (two categorical variables with exactly two levels each) with the frequencies using the Base R.\n\ntb2 &lt;- table(hemophilia$treatment, hemophilia$bleeding)\ntb2\n\n   \n    no yes\n  A 13   2\n  B 10   3\n\n\nNext, we will also create a more informative table with row percentages and marginal totals.\n\n\n\n\n\n\nTable with row percentages and marginal totals\n\n\n\n\n\nfinalfit\nmodelsummary\n\n\n\nUsing the function summary_factorlist() which is included in finalfit package for obtaining row percentages and marginal totals:\n\nrow_tb2 &lt;- hemophilia %&gt;%\n  finalfit::summary_factorlist(dependent = \"bleeding\", add_dependent_label = T,\n                     explanatory = \"treatment\", add_col_totals = T,\n                     include_col_totals_percent = F,\n                     column = FALSE, total_col = TRUE)\n\nknitr::kable(row_tb2) \n\n\n\nDependent: bleeding\n\nno\nyes\nTotal\n\n\n\nTotal N\n\n23\n5\n28\n\n\ntreatment\nA\n13 (86.7)\n2 (13.3)\n15 (100)\n\n\n\nB\n10 (76.9)\n3 (23.1)\n13 (100)\n\n\n\n\n\n\n\nThe contingency table using the datasummary_crosstab() from the modelsummary package:\n\nmodelsummary::datasummary_crosstab(treatment ~ bleeding, data = hemophilia)\n\n\n\ntreatment\n\nno\nyes\nAll\n\n\n\nA\nN\n13\n2\n15\n\n\n\n% row\n86.7\n13.3\n100.0\n\n\nB\nN\n10\n3\n13\n\n\n\n% row\n76.9\n23.1\n100.0\n\n\nAll\nN\n23\n5\n28\n\n\n\n% row\n82.1\n17.9\n100.0\n\n\n\n\n\n\n\n\n\n\nFrom the row frequencies, there is not actually difference, as we noted in the plot we made above.\nNow, we will calculate the expected frequencies for each cell using the expected() function from {epitools} package:\n\nepitools::expected(tb2)\n\n   \n          no      yes\n  A 12.32143 2.678571\n  B 10.67857 2.321429\n\n\nIn the above table there are 2 cells (50%) with expected counts less than 5 (specifically 2.67 and 2.32), so the Chi-square test is not the appropriate one. In this case the Fisher’s exact test should be used instead."
  },
  {
    "objectID": "fisher_exact.html#run-fishers-exact-test",
    "href": "fisher_exact.html#run-fishers-exact-test",
    "title": "13  Fisher’s exact test",
    "section": "\n13.6 Run Fisher’s exact test",
    "text": "13.6 Run Fisher’s exact test\nFinally, we run the Fisher’s exact test:\n\n\n\n\n\n\nFisher’s exact test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nfisher.test(tb2)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  tb2\np-value = 0.6389\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.1807204 26.9478788\nsample estimates:\nodds ratio \n   1.90363 \n\n\n\n\n\nfisher_test(tb2)\n\n# A tibble: 1 × 3\n      n     p p.signif\n* &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   \n1    28 0.639 ns      \n\n\n\n\n\n\n\nThe p = 0.64 is higher than 0.05. There is absence of evidence for an association between the treatment regimens and bleeding complications (failed to reject \\(H_0\\))."
  },
  {
    "objectID": "fisher_exact.html#having-only-the-counts",
    "href": "fisher_exact.html#having-only-the-counts",
    "title": "13  Fisher’s exact test",
    "section": "\n13.7 Having only the counts",
    "text": "13.7 Having only the counts\nWhen we read an article which reports a chi-square or a fisher exact analysis we will see only the counts in a table without having the raw data of the categorical variables. In this instance, we can create the table using the matrix() function and run the tests. For our example of hemophilia we have the following table:\n\ndat &lt;- c(13, 10, 2, 3)\nmx &lt;- matrix(dat, nrow = 2, dimnames = list(c(\"A\", \"B\"), c(\"no\", \"yes\")))\nmx\n\n  no yes\nA 13   2\nB 10   3"
  },
  {
    "objectID": "mcnemar.html#research-question-and-hypothesis-testing",
    "href": "mcnemar.html#research-question-and-hypothesis-testing",
    "title": "14  McNemar’s test",
    "section": "\n14.1 Research question and Hypothesis Testing",
    "text": "14.1 Research question and Hypothesis Testing\nWe consider the data in asthma dataset. The dataset contains data from a survey of 86 children with asthma who attended a camp to learn how to self-manage their asthmatic episodes. The children were asked whether they knew (yes or not) how to manage their asthmatic episodes appropriately at both the start and completion of the camp.\nIn other words, was a significant change in children’s knowledge of asthma management between the beginning and completion of the health camp?\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There was no change in children’s knowledge of asthma management between the beginning and completion of the health camp\n\n\\(H_1\\): There was change in children’s knowledge of asthma management between the beginning and completion of the health camp"
  },
  {
    "objectID": "mcnemar.html#packages-we-need",
    "href": "mcnemar.html#packages-we-need",
    "title": "14  McNemar’s test",
    "section": "\n14.2 Packages we need",
    "text": "14.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(janitor)\nlibrary(modelsummary)\nlibrary(exact2x2)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "mcnemar.html#preraring-the-data",
    "href": "mcnemar.html#preraring-the-data",
    "title": "14  McNemar’s test",
    "section": "\n14.3 Preraring the data",
    "text": "14.3 Preraring the data\nWe import the data asthma in R:\n\nlibrary(readxl)\nasthma &lt;- read_excel(here(\"data\", \"asthma.xlsx\"))\n\n\n\n\nFigure 14.1: Table with data from “asthma” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(asthma)\n\nRows: 86\nColumns: 2\n$ know_begin &lt;chr&gt; \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"y…\n$ know_end   &lt;chr&gt; \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"…\n\n\nThe dataset asthma includes 86 children with asthma (rows) and 2 columns, the character (&lt;chr&gt;) know_begin and the character (&lt;chr&gt;) know_end. Therefore, we consider the dichotomous dependent variable asthma knowledge (yes/no) between two time points, know_begin and know_end.\nBoth measurements know_begin and know_end should be converted to factors (&lt;fct&gt;) using the convert_as_factor() function as follows:\n\nasthma &lt;- asthma %&gt;%\n  convert_as_factor(know_begin, know_end)\n\nglimpse(asthma)\n\nRows: 86\nColumns: 2\n$ know_begin &lt;fct&gt; yes, no, yes, no, no, no, yes, no, no, yes, no, no, yes, ye…\n$ know_end   &lt;fct&gt; yes, no, no, no, no, no, yes, yes, yes, yes, yes, no, yes, …"
  },
  {
    "objectID": "mcnemar.html#contigency-table",
    "href": "mcnemar.html#contigency-table",
    "title": "14  McNemar’s test",
    "section": "\n14.4 Contigency table",
    "text": "14.4 Contigency table\nWe can obtain the cross-tabulation table of the two measurements for the children’s knowledge of asthma:\n\ntb3 &lt;- table(know_begin = asthma$know_begin, know_end = asthma$know_end)\ntb3\n\n          know_end\nknow_begin no yes\n       no  27  29\n       yes  6  24\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThere is a basic difference between this table and the more common two-way table. In this case, the count represents the number of pairs, not the number of individuals.\n\n\nWe want to compare the proportion of children’s knowledge of asthma management at the beginning with the proportion of children’s knowledge of asthma management at the end. We can create a more informative table using the functions from janitor package for obtaining total percentages and marginal totals.\n\n\n\n\n\n\nTable with total percentages and marginal totals\n\n\n\n\n\njanitor\nmodelsummary\n\n\n\nWe can create an informative table using the functions from janitor package for obtaining total percentages and marginal totals:\n\ntotal_tb2 &lt;- asthma %&gt;%\n  tabyl(know_begin, know_end) %&gt;%\n  adorn_totals(c(\"row\", \"col\")) %&gt;%\n  adorn_percentages(\"all\") %&gt;%\n  adorn_pct_formatting(digits = 1) %&gt;%\n  adorn_ns %&gt;%\n  adorn_title\n\nknitr::kable(total_tb2) \n\n\n\n\nknow_end\n\n\n\n\n\nknow_begin\nno\nyes\nTotal\n\n\nno\n31.4% (27)\n33.7% (29)\n65.1% (56)\n\n\nyes\n7.0% (6)\n27.9% (24)\n34.9% (30)\n\n\nTotal\n38.4% (33)\n61.6% (53)\n100.0% (86)\n\n\n\n\n\n\n\nThe contingency table using the datasummary_crosstab() from the modelsummary package:\n\nmodelsummary::datasummary_crosstab(know_begin ~ know_end, \n                     statistic = 1 ~ 1 + N + Percent(), \n                     data = asthma)\n\n\n\nknow_begin\n\nno\nyes\nAll\n\n\n\nno\nN\n27\n29\n56\n\n\n\n%\n31.4\n33.7\n65.1\n\n\nyes\nN\n6\n24\n30\n\n\n\n%\n7.0\n27.9\n34.9\n\n\nAll\nN\n33\n53\n86\n\n\n\n%\n38.4\n61.6\n100.0\n\n\n\n\n\n\n\n\n\n\nThe proportion of children who knew to manage asthma at the beginning is (6+24)/86= 30/86 = 0.349 or 34.9%. The proportion of children who knew to mange asthma at the end is (29+24)/86 = 53/86 = 0.616 or 61.6%.\n\n\n\n\n\n\nAssumption\n\n\n\nThe basic assumption of the test is that the sum of the discordant cells should be larger than 25 (that is fulfilled in our example)."
  },
  {
    "objectID": "mcnemar.html#run-mcnemars-test",
    "href": "mcnemar.html#run-mcnemars-test",
    "title": "14  McNemar’s test",
    "section": "\n14.5 Run McNemar’s test",
    "text": "14.5 Run McNemar’s test\nFinally, we run the McNemar’s test:\n\n\n\n\n\n\nMcNemar’s test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nmcnemar.test(tb3)\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  tb3\nMcNemar's chi-squared = 13.829, df = 1, p-value = 0.0002003\n\n\n\n\n\nmcnemar_test(tb3)\n\n# A tibble: 1 × 6\n      n statistic    df      p p.signif method      \n* &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;       \n1    86      13.8     1 0.0002 ***      McNemar test\n\n\n\n\n\n\n\nThe proportion of children who knew to manage asthma at the end (61.6%) is significant larger compared with the proportion of children who knew to manage asthma at the beginning (34.9%) (p-value &lt;0.001)."
  },
  {
    "objectID": "mcnemar.html#exact-binomial-test",
    "href": "mcnemar.html#exact-binomial-test",
    "title": "14  McNemar’s test",
    "section": "\n14.6 Exact binomial test",
    "text": "14.6 Exact binomial test\nExact binomial test for 2x2 table when the sum of the discordant cells are less than 25:\n\nmcnemar.exact(tb3)\n\n\n    Exact McNemar test (with central confidence intervals)\n\ndata:  tb3\nb = 29, c = 6, p-value = 0.0001168\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  1.971783 14.238838\nsample estimates:\nodds ratio \n  4.833333"
  },
  {
    "objectID": "roc.html#research-question",
    "href": "roc.html#research-question",
    "title": "15  Receiver Operating Characteristic (ROC) curve",
    "section": "\n15.1 Research question",
    "text": "15.1 Research question\nWe want to compare two screening questionnaires for chronic obstructive pulmonary disease (COPD) among smokers aged &gt;45 years in the primary care setting:\n\n\nInternational Primary Care Airways Group (IPAG) questionnaire (Score: 0-38)\n\nCOPD Population Screener (COPDPS) questionnaire (Score: 0-10)\n\nThe diagnosis of COPD was based on spirometric criterion (FEV 1 /FVC &lt;0.7 following bronchodilation), clinical status (medical history, symptoms and physical examination), and exclusion of other diseases."
  },
  {
    "objectID": "roc.html#packages-we-need",
    "href": "roc.html#packages-we-need",
    "title": "15  Receiver Operating Characteristic (ROC) curve",
    "section": "\n15.2 Packages we need",
    "text": "15.2 Packages we need\nWe need to load the following packages:\n\nlibrary(pROC)\nlibrary(plotROC)\nlibrary(epiR)\nlibrary(ggsci)\nlibrary(here)\nlibrary(tidyverse)\n\n\nOther relative packages: OptimalCutpoints, cutpointr"
  },
  {
    "objectID": "roc.html#preraring-the-data",
    "href": "roc.html#preraring-the-data",
    "title": "15  Receiver Operating Characteristic (ROC) curve",
    "section": "\n15.3 Preraring the data",
    "text": "15.3 Preraring the data\nWe import the data copd in R:\n\nlibrary(readxl)\ndat &lt;- read_excel(here(\"data\", \"copd.xlsx\"))\n\n\n\n\nFigure 15.1: Table with data from “copd” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(dat)\n\nRows: 2,587\nColumns: 3\n$ IPAG      &lt;dbl&gt; 11, 15, 4, 7, 13, 15, 13, 14, 4, 21, 17, 11, 10, 17, 9, 18, …\n$ COPDPS    &lt;dbl&gt; 4, 3, 2, 2, 4, 4, 2, 2, 2, 6, 5, 2, 2, 4, 2, 4, 2, 5, 2, 3, …\n$ diagnosis &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …"
  },
  {
    "objectID": "roc.html#using-cut-off-points-and-the-roc-curve",
    "href": "roc.html#using-cut-off-points-and-the-roc-curve",
    "title": "15  Receiver Operating Characteristic (ROC) curve",
    "section": "\n15.4 Using cut-off points and the ROC curve",
    "text": "15.4 Using cut-off points and the ROC curve\nBased on previous studies, the cut-off points for a positive response are:\n\n≥ 17 for the IPAG questionnaire\n≥ 5 for the COPDPS questionnaire.\n\nWe can evaluate these cut-off values by calculating their associated measures of diagnostic accuracy (i.e Se, Sp, PPV, NPV).\n\ndat &lt;- dat %&gt;% \n  mutate(IPAG_cat = cut(IPAG, c(0, 17, 38), labels=c(\"-\",\"+\"), \n                        include.lowest = TRUE, right=FALSE),\n         COPDPS_cat = cut(COPDPS, c(0, 5, 10), labels=c(\"-\",\"+\"), \n                          include.lowest = TRUE, right=FALSE))\n\ndat &lt;- as.data.frame(dat)\n\n# we need to create a roc object for each questionnaire\nroc1 &lt;- roc(dat$diagnosis, dat$IPAG)\nroc2 &lt;- roc(dat$diagnosis, dat$COPDPS)\n\n\n\n\n\n\n\nNOTE\n\n\n\n\nFor screening purposes such as mammogram, the cut-off point can be selected to favor a higher sensitivity. Thus, a negative test result indicates the absent of the disease (SeNout; sensitive, negative, “rule out” the disease).\nFor confirmative diagnosis purposes, for example, when a chemotherapy is to initiated once the diagnosis is established, the cut-off point can be selected to favor a higher specificity. Thus, a positive test result indicates the presence of disease (SpPin; specificity, positive, “rule in” the disease).\n\n\n\nAdditionally, for a given diagnostic test, we can consider all cut-off points that give a unique pair of values for sensitivity and specificity. We can plot in a graph, which is known as a ROC curve, the sensitivity on the y-axis and 1-specificity (false positives) values on the x-axis for all these possible cut-off points of the diagnostic test. Then, the area under the ROC curve (AUC of ROC), also called the c-statistic, can be calculated which is a widely used measure of overall performance.\nIPAG questionnaire\nA. The use of a cut-off value: IPAG score ≥17\nFirst, we will find the counts of individuals in each of the four possible outcomes in a 2×2 table for the cut-off point of 17:\n\ntable(dat$IPAG_cat, dat$diagnosis)\n\n   \n       0    1\n  - 1670   70\n  +  644  203\n\n\nNext, we reformat the table as follows:\n\ntb1 &lt;- as.table(\n  rbind(c(203, 644), c(70, 1670))\n  )\n\ndimnames(tb1) &lt;- list(\n  Test = c(\"+\", \"_\"),\n  Outcome = c(\"+\", \"-\")\n)\n\ntb1\n\n    Outcome\nTest    +    -\n   +  203  644\n   _   70 1670\n\n\n\nresults_IPAG &lt;- epi.tests(tb1, digits = 3)\nresults_IPAG\n\n          Outcome +    Outcome -      Total\nTest +          203          644        847\nTest -           70         1670       1740\nTotal           273         2314       2587\n\nPoint estimates and 95% CIs:\n--------------------------------------------------------------\nApparent prevalence *                  0.327 (0.309, 0.346)\nTrue prevalence *                      0.106 (0.094, 0.118)\nSensitivity *                          0.744 (0.687, 0.794)\nSpecificity *                          0.722 (0.703, 0.740)\nPositive predictive value *            0.240 (0.211, 0.270)\nNegative predictive value *            0.960 (0.949, 0.969)\nPositive likelihood ratio              2.672 (2.428, 2.940)\nNegative likelihood ratio              0.355 (0.290, 0.436)\nFalse T+ proportion for true D- *      0.278 (0.260, 0.297)\nFalse T- proportion for true D+ *      0.256 (0.206, 0.313)\nFalse T+ proportion for T+ *           0.760 (0.730, 0.789)\nFalse T- proportion for T- *           0.040 (0.031, 0.051)\nCorrectly classified proportion *      0.724 (0.706, 0.741)\n--------------------------------------------------------------\n* Exact CIs\n\n\nThe results using the cut-off point of 17 give Se = 0.744 (0.687 - 0.794) and Sp = 0.722 (0.703 - 0.740). We observe that the probability of the absence of COPD given a negative test result is high NPV = 0.960 (95% CI: 0.949, 0.969) in this sample with smokers.\n \nB. The area under the ROC curve of IPAG questionnaire\nFirst, let’s get sensitivity and specificity if a cutoff point of 17 is used for the IPAG questionnaire and calculate the AUC (95% CI) of ROC. We will use these values to provide additional information about the data being displayed:\n\n# get the sensitivity\nse_IPAG &lt;- round(results_IPAG$detail$est[3], digits = 3)\n\n# get the specificity\nsp_IPAG &lt;- round(results_IPAG$detail$est[4], digits = 3)\n\n# get AUC IPAG and the 95%CI for this area\nauc_values1 &lt;- round(ci.auc(roc1), 3)\nci_l1 &lt;- auc_values1[1]\nauc1 &lt;- auc_values1[2]\nci_u1 &lt;- auc_values1[3]\n\n \nThe ability of the IPAG questionnaire to discriminate between individuals with and without COPD is shown graphically by the ROC curve in (Figure 15.2):\n\n# create the plot\ng1 &lt;- ggplot(dat, aes(d = diagnosis, m = IPAG)) + \n  geom_roc(n.cuts = 0, color = \"#0071BF\") +\n  theme(text = element_text(size = 14)) +\n  geom_abline(intercept = 0, slope = 1, linetype = 'dashed') +\n  geom_point(aes(x = 1-sp_IPAG, y = se_IPAG), color = \"black\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(x = \"1 - Specificity\", y = \"Sensitivity\")\n\n# add annotations to the plot\ng1 + annotate(\"text\", x=0.70, y=0.30, \n           label=paste(\"AUC IPAG = \", auc1, \n                       \" (95% CI = \", ci_l1, \" - \", ci_u1, \")\")) +\n  annotate(\"text\", x=0.18, y=0.78, \n           label=paste(\"Se =\", se_IPAG)) +\n  annotate(\"text\", x=0.18, y=0.74, \n           label=paste(\"1-Sp =\", 1-sp_IPAG))\n\n\n\nFigure 15.2: The ROC curve of IPAG questionnaire.\n\n\n\n\nThe AUC of IPAG questionnaire equals to 0.799 (95% CI: 0.769 - 0.829) which indicates a reasonable diagnostic test.\n \n\n\n\n\n\n\nPerfect and useless diagnostic tests\n\n\n\nA test which is perfect at discriminating between those with disease and those without disease has an AUC = 1 (i.e. the ROC curve gets to the upper left-hand corner).\nThe dashed diagonal line connecting (0,0) to (1,1) is the ROC curve corresponding to a test that is completely useless in diagnosis of a disease, AUC = 0.5 (i.e. individuals with and without the disease have equal “chances” of testing positive).\n\n\n \nCOPDPS questionnaire\nA. The use of a cut-off value: COPDPS score ≥5\nFirst, we will find the counts of individuals in each of the four possible outcomes in a 2×2 table for the cut-off point of 5:\n\ntable(dat$COPDPS_cat, dat$diagnosis)\n\n   \n       0    1\n  - 2089  121\n  +  225  152\n\n\nNext, we reformat the table as follows:\n\ntb2 &lt;- as.table(\n  rbind(c(152, 225), c(121, 2089))\n  )\n\ndimnames(tb2) &lt;- list(\n  Test = c(\"+\", \"_\"),\n  Outcome = c(\"+\", \"-\")\n)\n\ntb2\n\n    Outcome\nTest    +    -\n   +  152  225\n   _  121 2089\n\n\n\nresults_COPDPS &lt;- epi.tests(tb2, digits = 3)\nresults_COPDPS\n\n          Outcome +    Outcome -      Total\nTest +          152          225        377\nTest -          121         2089       2210\nTotal           273         2314       2587\n\nPoint estimates and 95% CIs:\n--------------------------------------------------------------\nApparent prevalence *                  0.146 (0.132, 0.160)\nTrue prevalence *                      0.106 (0.094, 0.118)\nSensitivity *                          0.557 (0.496, 0.617)\nSpecificity *                          0.903 (0.890, 0.915)\nPositive predictive value *            0.403 (0.353, 0.455)\nNegative predictive value *            0.945 (0.935, 0.954)\nPositive likelihood ratio              5.726 (4.864, 6.741)\nNegative likelihood ratio              0.491 (0.430, 0.561)\nFalse T+ proportion for true D- *      0.097 (0.085, 0.110)\nFalse T- proportion for true D+ *      0.443 (0.383, 0.504)\nFalse T+ proportion for T+ *           0.597 (0.545, 0.647)\nFalse T- proportion for T- *           0.055 (0.046, 0.065)\nCorrectly classified proportion *      0.866 (0.853, 0.879)\n--------------------------------------------------------------\n* Exact CIs\n\n\nThe results using the cut-off point of 5 give Se = 0.577 (0.496 - 0.617) and Sp = 0.903 (0.890 - 0.915).\n \nB. The area under the ROC curve of COPDPS questionnaire\nSimilarly, let’s get sensitivity and specificity if a cutoff point of 5 is used for the COPDPS questionnaire and calculate the AUC (95% CI) of ROC to provide additional information in the plot:\n\n# get the sensitivity\nse_COPDPS &lt;- round(results_COPDPS$detail$est[3], digits = 3)\n\n# get the specificity\nsp_COPDPS &lt;- round(results_COPDPS$detail$est[4], digits = 3)\n\n# get AUC COPDPS and the 95%CI for this area\nauc_values2 &lt;- round(ci.auc(roc2), 3)\nci_l2 &lt;- auc_values2[1]\nauc2 &lt;- auc_values2[2]\nci_u2 &lt;- auc_values2[3]\n\nThe ROC curve of IPAG questionnaire (Figure 15.3) follows:\n\n# create the plot\ng2 &lt;- ggplot(dat, aes(d = diagnosis, m = COPDPS)) + \n  geom_roc(n.cuts = 0, color = \"#EFC000\") +\n  theme(text = element_text(size = 14)) +\n  geom_abline(intercept = 0, slope = 1, linetype = 'dashed') +\n  geom_point(aes(x = 1-sp_COPDPS, y = se_COPDPS), color = \"black\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(x = \"1 - Specificity\", y = \"Sensitivity\")\n\n# add annotations to the plot\ng2 + annotate(\"text\", x=0.70, y=0.25, \n           label= paste(\"AUC COPDPS = \", auc2, \n                        \" (95% CI = \", ci_l2, \" - \", ci_u2, \")\")) +\n  annotate(\"text\", x=0.19, y=0.56, \n           label=paste(\"Se =\", se_COPDPS)) +\n  annotate(\"text\", x=0.19, y=0.52, \n           label=paste(\"1-Sp =\", 1-sp_COPDPS))\n\n\n\nFigure 15.3: The ROC curve of COPDPS questionnaire.\n\n\n\n\nThe AUC of COPDPS questionnaire equals to 0.791 (95% CI: 0.760 - 0.821) which is close to the value 0.799 of AUC of IPAG questionnaire."
  },
  {
    "objectID": "roc.html#comparing-roc-curves",
    "href": "roc.html#comparing-roc-curves",
    "title": "15  Receiver Operating Characteristic (ROC) curve",
    "section": "\n15.5 Comparing ROC Curves",
    "text": "15.5 Comparing ROC Curves\nA. Graphical comparison of ROC curves\nWe can plot the ROC curves for both questionnaires in the same graph and compare the area under the curves (Figure 15.4):\n\n# prepare the data\nlongdata &lt;- melt_roc(dat, \"diagnosis\", c(\"IPAG\", \"COPDPS\"))\nlongdata$name = factor(longdata$name, levels = c(\"IPAG\", \"COPDPS\"))\n\n# create the plot\ng &lt;- ggplot(longdata, aes(d = D, m = M, color = name)) + \n  geom_roc(n.cuts = 0) +\n  theme(text = element_text(size = 14),\n        legend.position=\"top\") +\n  geom_abline(intercept = 0, slope = 1, linetype = 'dashed') +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_color_jco() +\n  labs(x = \"1 - Specificity\", y = \"Sensitivity\", colour=\"Questionnaire\")\n\n# add annotations to the plot\ng + annotate(\"text\", x=0.70, y=0.35, color = \"#0071BF\",\n           label=paste(\"AUC IPAG = \", auc1, \n                       \" (95% CI = \", ci_l1, \" - \", ci_u1, \")\")) +\n  annotate(\"text\", x=0.70, y=0.28, color = \"#EFC000\",\n           label= paste(\"AUC COPDPS = \", auc2, \n                        \" (95% CI = \", ci_l2, \" - \", ci_u2, \")\"))\n\n\n\nFigure 15.4: Graphical comparison between IPAG and COPDPS ROC curves.\n\n\n\n\nThe AUC values obtained from the ROC curve were 0.799 (95% CI: 0.769 - 0.829) for the IPAG questionnaire and 0.791 (95% CI: 0.760 - 0.821) for the COPDPS questionnaire. Therefore, the two questionnaires have similar overall performance in the present sample.\n \nB. Compare AUCs using the DeLong’ s test\nThe DeLong’s test can be used for comparing 2 areas under the curve (AUCs).\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): there is no difference between two AUCs (\\(AUC_{IPAG} = AUC_{COPDPS}\\))\n\n\\(H_1\\): there is difference between two AUCs (\\(AUC_{IPAG} \\neq AUC_{COPDPS}\\))\n\n\n\n\nroc.test(roc1, roc2, method=c(\"delong\"))\n\n\n    DeLong's test for two correlated ROC curves\n\ndata:  roc1 and roc2\nZ = 0.67525, p-value = 0.4995\nalternative hypothesis: true difference in AUC is not equal to 0\n95 percent confidence interval:\n -0.01488242  0.03052696\nsample estimates:\nAUC of roc1 AUC of roc2 \n  0.7986393   0.7908170 \n\n\nThere was no significant difference in the AUC values with the two questionnaires (p = 0.45 &lt; 0.05)."
  }
]