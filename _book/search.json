[
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Practical Statistics in Medicine with R",
    "section": "License",
    "text": "License\nThis textbook is free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License."
  },
  {
    "objectID": "inference.html",
    "href": "inference.html",
    "title": "1  Foundations for statistical inference",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nUnderstand the hypothesis testing\nUnderstand the type of errors"
  },
  {
    "objectID": "inference.html#hypothesis-testsing",
    "href": "inference.html#hypothesis-testsing",
    "title": "1  Foundations for statistical inference",
    "section": "\n1.1 Hypothesis Testsing",
    "text": "1.1 Hypothesis Testsing\nHypothesis testing is a method of deciding whether the data are consistent with the null hypothesis. The calculation of the p-value is an important part of the procedure. Given a study with a single outcome measure and a statistical test, hypothesis testing can be summarized in five steps.\n\n\n\n\n\n\nSteps of Hypothesis Testsing\n\n\n\nStep 1: State the null hypothesis, \\(H_{0}\\), and alternative hypothesis, \\(H_{1}\\), based on the research question.\n\nNOTE: We decide a non-directional \\(H_{1}\\) (also known as two-sided hypothesis) whether we test for effects in both directions (most common), otherwise a directional (also known as one-sided) hypothesis.\n\nStep 2: Set the level of significance, α (usually 0.05).\nStep 3: Identify the appropriate test statistic and check the assumptions. Calculate the test statistic using the data.\n\nNOTE: There are two basic types of statistical tests and they are described as parametric and non-parametric. The parametric tests (e.g., t-test, ANOVA), make certain assumptions about the distribution of the unknown parameter of interest and thus the test statistic is valid under these assumptions. For non-parametric tests (e.g., Mann-Whitney U test, Kruskal-Wallis test), there are no such assumptions. Most nonparametric tests use some way of ranking the measurements. Non-parametric tests are about 95% as powerful as parametric tests.\n\nStep 4: Decide whether or not the result is statistically significant.\n\nThe p-value is the probability of obtaining the observed results, or something more extreme, if the null hypothesis is true.\n\nUsing the known distribution of the test statistic and according to the result of our statistical test, we calculate the corresponding p-value. Then we compare the p-value to the significance level α:\n\nIf p − value < α, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ α, do not reject the null hypothesis, \\(H_{0}\\).\n\nThe Table 1.1 demonstrates how to interpret the strength of the evidence. However, always keep in mind the size of the study being considered.\n\n\nTable 1.1: Strength of the evidence against \\(H_{0}\\).\n\np-value\nInterpretation\n\n\n\n\\(p \\geq{0.10}\\)\nNo evidence to reject \\(H_{0}\\)\n\n\n\n\\(0.05\\leq p < 0.10\\)\nWeak evidence to reject \\(H_{0}\\)\n\n\n\n\\(0.01\\leq p < 0.05\\)\nEvidence to reject \\(H_{0}\\)\n\n\n\n\\(0.001\\leq p < 0.01\\)\nStrong evidence to reject \\(H_{0}\\)\n\n\n\n\\(p < 0.001\\)\nVery strong evidence to reject \\(H_{0}\\)\n\n\n\n\n\nStep 5: Interpret the results."
  },
  {
    "objectID": "inference.html#type-of-errors-in-hypothesis-testing",
    "href": "inference.html#type-of-errors-in-hypothesis-testing",
    "title": "1  Foundations for statistical inference",
    "section": "\n1.2 Type of Errors in hypothesis testing",
    "text": "1.2 Type of Errors in hypothesis testing\nType I error: we reject the null hypothesis when it is true (false positive), and conclude that there is an effect when, in reality, there is none. The maximum chance (probability) of making a Type I error is denoted by α (alpha). This is the significance level of the test; we reject the null hypothesis if our p-value is less than the significance level, i.e. if p < a (Figure 1.1).\nType II error: we do not reject the null hypothesis when it is false (false negative), and conclude that there is no effect when one really exists. The chance of making a Type II error is denoted by β (beta); its compliment, (1 - β), is the power of the test. The power, therefore, is the probability of rejecting the null hypothesis when it is false; i.e. it is the chance (usually expressed as a percentage) of detecting, as statistically significant, a real treatment effect of a given size (Figure 1.1). Table 1.2 presents the main factors that can influence the power in a study.\n\n\n\n\nFigure 1.1: Types of error in hypothesis testing.\n\n\n\n\n\n\nTable 1.2: Factors Influencing Power.\n\n\n\n\n\nFactor\nInfluence on study’s power\n\n\n\nEffect Size\nAs effect size increases, power tends to increase.\n\n\nSample Size\nAs the sample size goes up, power generally goes up.\n\n\nStandard deviation\nAs variability decreases, power tends to increase.\n\n\nSignificance level α\nAs α goes up, power goes up."
  },
  {
    "objectID": "student_t_test.html",
    "href": "student_t_test.html",
    "title": "2  Two-sample t-test (Student’s t-test)",
    "section": "",
    "text": "Two sample t-test (Student’s t-test) can be used if we have two independent (unrelated) groups (e.g., males-females, treatment-non treatment) and one quantitative variable of interest.\nWhen we have finished this Chapter, we should be able to:"
  },
  {
    "objectID": "student_t_test.html#research-question-and-hypothesis-testing",
    "href": "student_t_test.html#research-question-and-hypothesis-testing",
    "title": "2  Two-sample t-test (Student’s t-test)",
    "section": "\n2.1 Research question and Hypothesis Testing",
    "text": "2.1 Research question and Hypothesis Testing\nWe consider the data in depression dataset. In an experiment designed to test the effectiveness of paroxetine for treating bipolar depression, the participants were randomly assigned into two groups (paroxetine Vs placebo). The researchers used the Hamilton Depression Rating Scale (HDRS) to measure the depression state of the participants and wanted to find out if the HDRS score is different in paroxetine group as compared to placebo group at the end of the experiment. The significance level \\(\\alpha\\) was set to 0.05.\nNote: A score of 0–7 in HDRS is generally accepted to be within the normal range, while a score of 20 or higher indicates at least moderate severity.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): the means of HDRS in the two groups are equal (\\(\\mu_{1} = \\mu_{2}\\))\n\n\\(H_1\\): the means of HDRS in the two groups are not equal (\\(\\mu_{1} \\neq \\mu_{2}\\))"
  },
  {
    "objectID": "student_t_test.html#packages-we-need",
    "href": "student_t_test.html#packages-we-need",
    "title": "2  Two-sample t-test (Student’s t-test)",
    "section": "\n2.2 Packages we need",
    "text": "2.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(PupillometryR)\nlibrary(gtsummary)\n\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "student_t_test.html#preraring-the-data",
    "href": "student_t_test.html#preraring-the-data",
    "title": "2  Two-sample t-test (Student’s t-test)",
    "section": "\n2.3 Preraring the data",
    "text": "2.3 Preraring the data\nWe import the data depression in R:\n\nlibrary(readxl)\ndepression <- read_excel(here(\"data\", \"depression.xlsx\"))\n\n\n\n\nFigure 2.1: Table with data from “depression” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(depression)\n\nRows: 76\nColumns: 2\n$ intervention <chr> \"placebo\", \"placebo\", \"placebo\", \"placebo\", \"placebo\", \"p…\n$ HDRS         <dbl> 19, 21, 28, 22, 22, 28, 23, 17, 19, 20, 26, 23, 23, 22, 1…\n\n\nThe data set depression has 76 patients (rows) and includes two variables (columns). The numeric (<dbl>) HDRS variable and the character (<chr>) intervention variable which should be converted to a factor (<fct>) variable using the factor() function as follows:\n\ndepression <- depression %>% \n  mutate(intervention = factor(intervention))\nglimpse(depression)\n\nRows: 76\nColumns: 2\n$ intervention <fct> placebo, placebo, placebo, placebo, placebo, placebo, pla…\n$ HDRS         <dbl> 19, 21, 28, 22, 22, 28, 23, 17, 19, 20, 26, 23, 23, 22, 1…"
  },
  {
    "objectID": "student_t_test.html#assumptions",
    "href": "student_t_test.html#assumptions",
    "title": "2  Two-sample t-test (Student’s t-test)",
    "section": "\n2.4 Assumptions",
    "text": "2.4 Assumptions\n\n\n\n\n\n\nCheck if the following assumptions are satisfied\n\n\n\n\nThe data are normally distributed in both groups\nThe data in both groups have similar variance (also named as homogeneity of variance or homoscedasticity)\n\n\n\nA. Explore the characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\nGraphs\nWe can visualize the distribution of HDRS for the two groups:\n\nset.seed(123)\nggplot(depression, aes(x=intervention, y=HDRS)) + \n  geom_flat_violin(aes(fill = intervention), scale = \"count\") +\n  geom_boxplot(width = 0.14, outlier.shape = NA, alpha = 0.5) +\n  geom_point(position = position_jitter(width = 0.05), \n             size = 1.2, alpha = 0.6) +\n  ggsci::scale_fill_jco() +\n  theme_classic(base_size = 14) +\n  theme(legend.position=\"none\", \n        axis.text = element_text(size = 14))\n\n\n\nFigure 2.2: Rain cloud plot.\n\n\n\n\nThe above figure shows that the data are close to symmetry and the assumption of a normal distribution is reasonable.\n \nSummary statistics\nThe HDRS summary statistics for each group are:\n\n\n\n\n\n\nSummary statistics by group\n\n\n\n\n\ndplyr\ndlookr\n\n\n\n\nHDRS_summary <- depression %>%\n  group_by(intervention) %>%\n  dplyr::summarise(\n    n = n(),\n    min = min(HDRS, na.rm = TRUE),\n    q1 = quantile(HDRS, 0.25, na.rm = TRUE),\n    median = quantile(HDRS, 0.5, na.rm = TRUE),\n    q3 = quantile(HDRS, 0.75, na.rm = TRUE),\n    max = max(HDRS, na.rm = TRUE),\n    mean = mean(HDRS, na.rm = TRUE),\n    sd = sd(HDRS, na.rm = TRUE),\n    skewness = EnvStats::skewness(HDRS, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(HDRS, na.rm = TRUE)\n  ) %>%\n  ungroup()\n\nHDRS_summary\n\n# A tibble: 2 × 11\n  intervention     n   min    q1 median    q3   max  mean    sd skewness kurto…¹\n  <fct>        <int> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>   <dbl>\n1 paroxetine      33    13    18     21    22    27  20.3  3.65  0.00167  -0.574\n2 placebo         43    14    19     21    24    28  21.5  3.41  0.0276   -0.403\n# … with abbreviated variable name ¹​kurtosis\n\n\n\n\n\ndepression %>% \n  group_by(intervention) %>% \n  dlookr::describe(HDRS) %>% \n  select(described_variables,  intervention, n, mean, sd, p25, p50, p75, skewness, kurtosis) %>% \n  ungroup()\n\n# A tibble: 2 × 10\n  described_variab…¹ inter…²     n  mean    sd   p25   p50   p75 skewn…³ kurto…⁴\n  <chr>              <fct>   <int> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>   <dbl>\n1 HDRS               paroxe…    33  20.3  3.65    18    21    22 0.00167  -0.574\n2 HDRS               placebo    43  21.5  3.41    19    21    24 0.0276   -0.403\n# … with abbreviated variable names ¹​described_variables, ²​intervention,\n#   ³​skewness, ⁴​kurtosis\n\n\n\n\n\n\n\nThe means are close to medians (20.3 vs 21 and 21.5 vs 21). The skewness is approximately zero (symmetric distribution) and the (excess) kurtosis falls into the acceptable range of [-1, 1] indicating approximately normal distributions for both groups.\n \nNormality test\n\n\n\n\n\n\nRemember: Hypothesis testing for Shapiro-Wilk test for normality\n\n\n\n\\(H_{0}\\): the data came from a normally distributed population.\n\\(H_{1}\\): the data tested are not normally distributed.\n\nIf p − value < 0.05, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\).\n\n\n\nThe Shapiro-Wilk test for normality for each group is:\n\ndepression %>%\n  group_by(intervention) %>%\n  shapiro_test(HDRS) %>% \n  ungroup()\n\n# A tibble: 2 × 4\n  intervention variable statistic     p\n  <fct>        <chr>        <dbl> <dbl>\n1 paroxetine   HDRS         0.976 0.670\n2 placebo      HDRS         0.979 0.614\n\n\nThe tests of normality suggest that the data for the HDRS in both groups are normally distributed (p=0.67 >0.05 and p=0.61 >0.05, respectively).\n \nB. Levene’s test for equality of variances\n\n\n\n\n\n\nRemember: Hypothesis testing for Levene’s test for equality of variances\n\n\n\n\\(H_{0}\\): the variances of HDRs in two groups are equal\n\\(H_{1}\\): the variances of HDRs in two groups are not equal\n\nIf p − value < 0.05, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\).\n\n\n\nThe Levene’s test for equality of variances is:\n\ndepression %>% \n  levene_test(HDRS ~ intervention)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  <int> <int>     <dbl> <dbl>\n1     1    74     0.176 0.676\n\n\nSince the p-value = 0.676 >0.05, the null hypothesis that the variances of HDRs in two groups are equal is not rejected."
  },
  {
    "objectID": "student_t_test.html#run-the-t-test",
    "href": "student_t_test.html#run-the-t-test",
    "title": "2  Two-sample t-test (Student’s t-test)",
    "section": "\n2.5 Run the t-test",
    "text": "2.5 Run the t-test\nWe will perform a pooled variance t-test (Student’s t-test) to test the null hypothesis that the mean HDRS score is the same for both groups (paroxetine and placebo).\n\n\n\n\n\n\nStudent’s t-test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nt.test(HDRS ~ intervention, var.equal = T, data=depression)\n\n\n    Two Sample t-test\n\ndata:  HDRS by intervention\nt = -1.4185, df = 74, p-value = 0.1602\nalternative hypothesis: true difference in means between group paroxetine and group placebo is not equal to 0\n95 percent confidence interval:\n -2.777498  0.467420\nsample estimates:\nmean in group paroxetine    mean in group placebo \n                20.33333                 21.48837 \n\n\n\n\n\ndepression %>% \n    t_test(HDRS ~ intervention, var.equal = T, detailed = T)\n\n# A tibble: 1 × 15\n  estimate estimate1 estim…¹ .y.   group1 group2    n1    n2 stati…²     p    df\n*    <dbl>     <dbl>   <dbl> <chr> <chr>  <chr>  <int> <int>   <dbl> <dbl> <dbl>\n1    -1.16      20.3    21.5 HDRS  parox… place…    33    43   -1.42  0.16    74\n# … with 4 more variables: conf.low <dbl>, conf.high <dbl>, method <chr>,\n#   alternative <chr>, and abbreviated variable names ¹​estimate2, ²​statistic\n\n\n\n\n\n\n\nThe p-value = 0.16 is greater than 0.05. There is no evidence of a significant difference in mean HDRS between the two groups (failed to reject \\(H_0\\)). The difference between means (20.33 - 21.49) equals to -1.16 units of the HDRS and note that the 95% confidence interval of the difference in means (-2.78 to 0.47) includes the hypothesized null value of 0. Based on these results, there is not evidence that paroxetine is effective as a treatment for bipolar depression.\nNote that the paroxetine group (n=33) has df (degrees of freedom) = 33-1 = 32 and the placebo sample (n= 43) has df = 43-1 = 42 , so we have df = 32 + 42 = 74 in total. Another way of thinking of this is that the complete sample size is 76, and we have estimated two parameters from the data (the two means), so we have df = 76-2 = 74 .\nThe Student t-test for two independent samples does not have any restrictions on \\(n_1\\) and \\(n_2\\) —they can be equal or unequal. However, equal samples are preferred because when a total of 2n subjects are available, their equal division among the groups maximizes the power to detect a specified difference.\n\n\n\n\n\n\nRemember\n\n\n\nIf the variance is different between the two groups then the degrees of freedom and the t-value associated with a two-sample t-test are calculated differently. In this case, we have to write var.equal = F (or write nothing because this is the default) in the function so the Welch-Satterthwaite approximation is applied to the degrees of freedom.\n\n\n \nPresent the results in a summary table\n\nShow the codegt_sum1 <- depression %>% \n  tbl_summary(\n    by = intervention, \n    statistic = HDRS ~ \"{mean} ({sd})\", \n    digits = list(everything() ~ 1),\n    label = list(HDRS ~ \"HDRS score\"), \n    missing = c(\"no\")) %>% \n  add_p(test = HDRS ~ \"t.test\", purrr::partial(style_pvalue, digits = 2),\n        test.args = all_tests(\"t.test\") ~ list(var.equal = TRUE)) %>% \n  add_overall() %>%\n  as_gt() \n\ngt_sum1\n\n\n\n\n\n\nCharacteristic\n      \nOverall, N = 761\n\n      \nparoxetine, N = 331\n\n      \nplacebo, N = 431\n\n      \np-value2\n\n    \n\nHDRS score\n21.0 (3.5)\n20.3 (3.7)\n21.5 (3.4)\n0.16\n\n\n\n\n1 Mean (SD)\n    \n\n\n2 Two Sample t-test\n    \n\n\n\n\n\nHence, there is not evidence that HDRS score is significantly different in paroxetine group, mean = 20.3 (sd = 3.7), as compared to placebo group, 21.5 (3.4), (mean difference= -1.16 units, 95% CI = -2.78 to 0.47, p = 0.16 >0.05)."
  },
  {
    "objectID": "wmw_test.html",
    "href": "wmw_test.html",
    "title": "3  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "",
    "text": "The Wilcoxon-Mann-Whitney (WMW) test (sometimes called Mann-Whitney U test or Wilcoxon Rank Sum test) is used to compare two independent samples and is often considered the non-parametric alternative to the Student’s t-test when there is violation of normality or for small sample sizes. However, if the samples are very small (both smaller than four observations) then statistical significance is impossible.\nWhen we have finished this Chapter, we should be able to:"
  },
  {
    "objectID": "wmw_test.html#research-question-and-hypothesis-testing",
    "href": "wmw_test.html#research-question-and-hypothesis-testing",
    "title": "3  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n3.1 Research question and Hypothesis Testing",
    "text": "3.1 Research question and Hypothesis Testing\nWe consider the data in thromboglobulin dataset that contains the urinary \\(\\beta\\) thromboglobulin excretion (pg/ml) measured in 12 non-diabetic patients and 12 diabetic patients. The researchers used \\(\\alpha\\) = 0.05 significance level to test if the distribution of urinary \\(\\beta\\) thromboglobulin (b_TG) differs in the two groups.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): the distribution of urinary \\(\\beta\\) thromboglobulin is the same in the two groups\n\n\\(H_1\\): the distribution of urinary \\(\\beta\\) thromboglobulin is different in the two groups\n\n\n\nNOTE: The null hypothesis is that the observations from one group do not tend to have a higher or lower ranking than observations from the other group. This test does not test the medians of the data as is commonly thought, it tests the whole distribution. In practice, however, we use the medians to present the results. Statistical speaking, if the distributions of the two groups have similar shapes, the Wilcoxon-Mann-Whitney test can be used to determine whether there are differences in the medians between the two groups."
  },
  {
    "objectID": "wmw_test.html#packages-we-need",
    "href": "wmw_test.html#packages-we-need",
    "title": "3  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n3.2 Packages we need",
    "text": "3.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(PupillometryR)\nlibrary(gtsummary)\n\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "wmw_test.html#preraring-the-data",
    "href": "wmw_test.html#preraring-the-data",
    "title": "3  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n3.3 Preraring the data",
    "text": "3.3 Preraring the data\nWe import the data thromboglobulin in R:\n\nlibrary(readxl)\ntg <- read_excel(here(\"data\", \"thromboglobulin.xlsx\"))\n\n\n\n\nFigure 3.1: Table with data from “depression” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(tg)\n\nRows: 24\nColumns: 2\n$ status <chr> \"non-diabetic\", \"non-diabetic\", \"non-diabetic\", \"non-diabetic\",…\n$ b_TG   <dbl> 4.1, 6.3, 7.8, 8.5, 8.9, 10.4, 11.5, 12.0, 13.8, 17.6, 24.3, 37…\n\n\nThe data set tg has 24 patients (rows) and includes two variables (columns). The numeric (<dbl>) b_TG variable and the character (<chr>) status variable which should be converted to a factor (<fct>) variable using the factor() function as follows:\n\ntg <- tg %>% \n  mutate(status = factor(status))\nglimpse(tg)\n\nRows: 24\nColumns: 2\n$ status <fct> non-diabetic, non-diabetic, non-diabetic, non-diabetic, non-dia…\n$ b_TG   <dbl> 4.1, 6.3, 7.8, 8.5, 8.9, 10.4, 11.5, 12.0, 13.8, 17.6, 24.3, 37…"
  },
  {
    "objectID": "wmw_test.html#explore-the-characteristics-of-distribution-for-each-group-and-check-for-normality",
    "href": "wmw_test.html#explore-the-characteristics-of-distribution-for-each-group-and-check-for-normality",
    "title": "3  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n3.4 Explore the characteristics of distribution for each group and check for normality",
    "text": "3.4 Explore the characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\n \nGraph\nWe can visualize the distribution of b_TG for the two groups:\n\nset.seed(123)\nggplot(tg, aes(x=status, y=b_TG)) + \n  geom_flat_violin(aes(fill = status), scale = \"count\") +\n  geom_boxplot(width = 0.14, outlier.shape = NA, alpha = 0.5) +\n  geom_point(position = position_jitter(width = 0.05), \n             size = 1.2, alpha = 0.6) +\n  ggsci::scale_fill_jco() +\n  theme_classic(base_size = 14) +\n  theme(legend.position=\"none\", \n        axis.text = element_text(size = 14))\n\n\n\nFigure 3.2: Rain cloud plot.\n\n\n\n\nThe above figure shows that the data in both groups are positively skewed and they have similar shaped distributions.\n \nSummary statistics\nThe g_TG summary statistics for each group are:\n\n\n\n\n\n\nSummary statistics by group\n\n\n\n\n\ndplyr\ndlookr\n\n\n\n\ntg_summary <- tg %>%\n  group_by(status) %>%\n  dplyr::summarise(\n    n = n(),\n    min = min(b_TG, na.rm = TRUE),\n    q1 = quantile(b_TG, 0.25, na.rm = TRUE),\n    median = quantile(b_TG, 0.5, na.rm = TRUE),\n    q3 = quantile(b_TG, 0.75, na.rm = TRUE),\n    max = max(b_TG, na.rm = TRUE),\n    mean = mean(b_TG, na.rm = TRUE),\n    sd = sd(b_TG, na.rm = TRUE),\n    skewness = EnvStats::skewness(b_TG, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(b_TG, na.rm = TRUE)\n  ) %>%\n  ungroup()\n\ntg_summary\n\n# A tibble: 2 × 11\n  status           n   min    q1 median    q3   max  mean    sd skewness kurto…¹\n  <fct>        <int> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>   <dbl>\n1 diabetic        12  23.8 27.2    29.2  34.8  46.2  31.8  7.17     1.05   0.107\n2 non-diabetic    12   4.1  8.32   11.0  14.8  37.2  13.5  9.19     1.81   3.47 \n# … with abbreviated variable name ¹​kurtosis\n\n\n\n\n\ntg %>% \n  group_by(status) %>% \n  dlookr::describe(b_TG) %>% \n  select(described_variables,  status, n, mean, sd, p25, p50, p75, skewness, kurtosis) %>% \n  ungroup()\n\n# A tibble: 2 × 10\n  described_variables status     n  mean    sd   p25   p50   p75 skewn…¹ kurto…²\n  <chr>               <fct>  <int> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>   <dbl>\n1 b_TG                diabe…    12  31.8  7.17 27.2   29.2  34.8    1.05   0.107\n2 b_TG                non-d…    12  13.5  9.19  8.32  11.0  14.8    1.81   3.47 \n# … with abbreviated variable names ¹​skewness, ²​kurtosis\n\n\n\n\n\n\n\nThe means are not very close to the medians (31.8 vs 29.2 and 13.5 vs 11.0). Moreover, both the skewness (1.81) and the (excess) kurtosis (3.47) for the non-diabetic group falls outside of the acceptable range of [-1, 1] indicating right-skewed and leptokurtic distribution.\n \nNormality test\nThe Shapiro-Wilk test for normality for each group is:\n\ntg %>%\n  group_by(status) %>%\n  shapiro_test(b_TG) %>% \n  ungroup()\n\n# A tibble: 2 × 4\n  status       variable statistic      p\n  <fct>        <chr>        <dbl>  <dbl>\n1 diabetic     b_TG         0.886 0.105 \n2 non-diabetic b_TG         0.817 0.0148\n\n\nWe can see that the data for the non-diabetic group is not normally distributed (p=0.015 <0.05) according to the Shapiro-Wilk test."
  },
  {
    "objectID": "wmw_test.html#run-the-wilcoxon-mann-whitney-test",
    "href": "wmw_test.html#run-the-wilcoxon-mann-whitney-test",
    "title": "3  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n3.5 Run the Wilcoxon-Mann-Whitney test",
    "text": "3.5 Run the Wilcoxon-Mann-Whitney test\nThe difference in medians between the two groups can be tested using a rank test such as Wilcoxon-Mann-Whitney (WMW):\n\n\n\n\n\n\nWilcoxon-Mann-Whitney test\n\n\n\n\n\nBase R\nexactRankTests\nrstatix\n\n\n\n\nwilcox.test(b_TG ~ status, conf.int = T, data = tg)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  b_TG by status\nW = 134, p-value = 0.0001028\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n 13.7 24.0\nsample estimates:\ndifference in location \n                 18.95 \n\n\nHistorical Note: As you can see, in R the Mann-Whitney statistic (symbolized as W) is calculated with the wilcox.test() function and it is called Wilcoxon rank-sum test. What is the reason for this? Henry Mann and Donald Whitney (1947) reported in their article that the test was first proposed by Frank Wilcoxon (1945) and they gave their version for the test. So the right would be to call this test Wilcoxon-Mann-Whitney (WMW) test.\n\n\nAlthough a small number of ties should not have a serious impact on our results, in case of ties we can use the wilcox.exact() function from the package {exactRankTests}:\n\nwilcox.exact(b_TG ~ status, conf.int = T, data = tg)\n\n\n    Exact Wilcoxon rank sum test\n\ndata:  b_TG by status\nW = 134, p-value = 0.0001028\nalternative hypothesis: true mu is not equal to 0\n95 percent confidence interval:\n 13.7 24.0\nsample estimates:\ndifference in location \n                 18.95 \n\n\n\n\n\ntg %>%\n  wilcox_test(b_TG ~ status, detailed = T)\n\n# A tibble: 1 × 12\n  estim…¹ .y.   group1 group2    n1    n2 stati…²       p conf.…³ conf.…⁴ method\n*   <dbl> <chr> <chr>  <chr>  <int> <int>   <dbl>   <dbl>   <dbl>   <dbl> <chr> \n1    19.0 b_TG  diabe… non-d…    12    12     134 1.03e-4    13.7      24 Wilco…\n# … with 1 more variable: alternative <chr>, and abbreviated variable names\n#   ¹​estimate, ²​statistic, ³​conf.low, ⁴​conf.high\n\n\n\n\n\n\n\nBased on the p <0.001, we reject the null hypothesis, the result (difference in medians = 18.95, 95% CI: 13.7 to 24) is significant.\n \nPresent the results in a summary table\nThe median (or another percentile) can be used as a summary measure. The choice is guided by the shape of the distributions.\n\nShow the codegt_sum2 <- tg %>% \n  tbl_summary(\n    by = status, \n    statistic = b_TG ~ \"{median} ({p25}, {p75})\", \n    digits = list(everything() ~ 1),\n    label = list(b_TG ~ \"b_TG \\n(pg/ml)\"), \n    missing = c(\"no\")) %>% \n  add_p(test = b_TG ~ \"wilcox.test\") %>% \n  add_overall() %>%\n  as_gt() \n\ngt_sum2\n\n\n\n\n\n\nCharacteristic\n      \nOverall, N = 241\n\n      \ndiabetic, N = 121\n\n      \nnon-diabetic, N = 121\n\n      \np-value2\n\n    \n\nb_TG \n(pg/ml)\n24.6 (11.2, 30.2)\n29.2 (27.1, 34.8)\n10.9 (8.3, 14.8)\n<0.001\n\n\n\n\n1 Median (IQR)\n    \n\n\n2 Wilcoxon rank sum exact test\n    \n\n\n\n\n\nHence, the urinary \\(\\beta\\) thromboglobulin excretion is significantly higher in diabetic group, median = 29.2 (IQR: 27.1, 34.8) pg/ml, as compared to non-diabetic group, 10.9 (8.3, 14.8) pg/ml, (p <0.001)."
  },
  {
    "objectID": "paired_t_test.html",
    "href": "paired_t_test.html",
    "title": "4  Paired t-test",
    "section": "",
    "text": "A paired t-test is used to estimate whether the means of two related measurements are significantly different from one another.\nExamples of paired study designs are:\nWhen we have finished this Chapter, we should be able to:"
  },
  {
    "objectID": "paired_t_test.html#research-question",
    "href": "paired_t_test.html#research-question",
    "title": "4  Paired t-test",
    "section": "\n4.1 Research question",
    "text": "4.1 Research question\nThe dataset weight contains the birth and discharge weight of 25 newborns. We might ask if the mean difference of the weight in birth and in discharge equals to zero or not. If the differences between the pairs of measurements are normally distributed, a paired t-test is the most appropriate statistical test.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): the mean difference of weight equals to zero (\\(\\mu_{d} = 0\\))\n\n\\(H_1\\): the mean difference of weight does not equal to zero (\\(\\mu_{d} \\neq 0\\))"
  },
  {
    "objectID": "paired_t_test.html#packages-we-need",
    "href": "paired_t_test.html#packages-we-need",
    "title": "4  Paired t-test",
    "section": "\n4.2 Packages we need",
    "text": "4.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(gtsummary)\n\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "paired_t_test.html#preraring-the-data",
    "href": "paired_t_test.html#preraring-the-data",
    "title": "4  Paired t-test",
    "section": "\n4.3 Preraring the data",
    "text": "4.3 Preraring the data\nWe import the data weight in R:\n\nlibrary(readxl)\nweight <- read_excel(here(\"data\", \"weight.xlsx\"))\n\nWe calculate the differences using the mutate() function:\n\nweight <- weight %>%\n  mutate(dif_weight = birth_weight - discharge_weight)\n\n\n\n\nFigure 4.1: Table with data from “weight” file.\n\n\n\nWe inspect the data:\n\nglimpse(weight) \n\nRows: 25\nColumns: 3\n$ birth_weight     <dbl> 3250, 2680, 2960, 3420, 3210, 2740, 3250, 3170, 2970,…\n$ discharge_weight <dbl> 3220, 2640, 2940, 3350, 3140, 2730, 3220, 3150, 2890,…\n$ dif_weight       <dbl> 30, 40, 20, 70, 70, 10, 30, 20, 80, 103, 84, 42, -15,…"
  },
  {
    "objectID": "paired_t_test.html#assumptions",
    "href": "paired_t_test.html#assumptions",
    "title": "4  Paired t-test",
    "section": "\n4.4 Assumptions",
    "text": "4.4 Assumptions\n\n\n\n\n\n\nCheck if the following assumption is satisfied\n\n\n\nThe differences are normally distributed."
  },
  {
    "objectID": "paired_t_test.html#explore-the-characteristics-of-distribution-of-differences",
    "href": "paired_t_test.html#explore-the-characteristics-of-distribution-of-differences",
    "title": "4  Paired t-test",
    "section": "\n4.5 Explore the characteristics of distribution of differences",
    "text": "4.5 Explore the characteristics of distribution of differences\nThe distribution of the differences can be explored with appropriate plots and summary statistics.\nGraph\nWe can explore the distribution of differences visually for symmetry with a density plot (a smoothed version of the histogram):\n\nweight %>%\n  ggplot(aes(x = dif_weight)) +\n  geom_density(fill = \"#76B7B2\", color=\"black\", alpha = 0.2) +\n  geom_vline(aes(xintercept=mean(dif_weight)),\n             color=\"blue\", linetype=\"dashed\", size=1.4) +\n  geom_vline(aes(xintercept=median(dif_weight)),\n             color=\"red\", linetype=\"dashed\", size=1.2) +\n  labs(x = \"Weight difference\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\")\n\n\n\nFigure 4.2: Density plot of the weight differences.\n\n\n\n\nThe above figure shows that the data are following an approximately symmetrical distribution. Note that the arethmetic mean (blue vertical dashed line) is very close to the median (red vertical dashed line) of the data.\nSummary statistics\nSummary statistics can also be calculated for the variables.\n\n\n\n\n\n\nSummary statistics\n\n\n\n\n\ndplyr\ndlookr\n\n\n\nWe can utilize the across function to obtain the results across the three variables simultaneously:\n\nsummary_weight <- weight %>%\n  dplyr::summarise(across(\n    .cols = c(dif_weight, birth_weight, discharge_weight), \n    .fns = list(\n    n = ~n(),\n      min = ~min(., na.rm = TRUE),\n      q1 = ~quantile(., 0.25, na.rm = TRUE),\n      median = ~quantile(., 0.5, na.rm = TRUE),\n      q3 = ~quantile(., 0.75, na.rm = TRUE),\n      max = ~max(., na.rm = TRUE),\n      mean = ~mean(., na.rm = TRUE),\n      sd = ~sd(., na.rm = TRUE),\n      skewness = ~EnvStats::skewness(., na.rm = TRUE),\n      kurtosis= ~EnvStats::kurtosis(., na.rm = TRUE)\n    ),\n    .names = \"{col}_{fn}\")\n    )\n\n# present the results\nsummary_weight <- summary_weight %>% \n  mutate(across(everything(), round, 2)) %>%   # round to 3 decimal places\n  pivot_longer(1:30, names_to = \"Stats\", values_to = \"Values\")  # long format\n\nsummary_weight\n\n# A tibble: 30 × 2\n   Stats               Values\n   <chr>                <dbl>\n 1 dif_weight_n         25   \n 2 dif_weight_min      -30   \n 3 dif_weight_q1        20   \n 4 dif_weight_median    40   \n 5 dif_weight_q3        70   \n 6 dif_weight_max      103   \n 7 dif_weight_mean      39.6 \n 8 dif_weight_sd        32.3 \n 9 dif_weight_skewness  -0.16\n10 dif_weight_kurtosis  -0.08\n# … with 20 more rows\n\n\n\n\n\nweight %>% \n  dlookr::describe(dif_weight, birth_weight, discharge_weight) %>% \n  select(described_variables, n, mean, sd, p25, p50, p75, skewness, kurtosis) %>% \n  ungroup()\n\n# A tibble: 3 × 9\n  described_variables     n   mean    sd   p25   p50   p75 skewness kurtosis\n  <chr>               <int>  <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>    <dbl>\n1 dif_weight             25   39.6  32.3    20    40    70   -0.157  -0.0752\n2 birth_weight           25 3076.  248.   2960  3150  3210   -0.291  -0.935 \n3 discharge_weight       25 3036.  248.   2880  3100  3220   -0.219  -1.02  \n\n\n\n\n\n\n\nAs it was previously mentioned, the mean of the differences (39.64) is close to median (40). Moreover, both the skewness and the kurtosis are approximately zero indicating a symmetric and mesokurtic distribution for the weight differences.\nNormality test\nAdditionally, we can check the statistical test for normality of the differences.\n\n weight %>%\n    shapiro_test(dif_weight)\n\n# A tibble: 1 × 3\n  variable   statistic     p\n  <chr>          <dbl> <dbl>\n1 dif_weight     0.974 0.742\n\n\nThe Shapiro-Wilk test suggests that the weight differences are normally distributed (p=0.74 > 0.05)."
  },
  {
    "objectID": "paired_t_test.html#run-the-paired-t-test",
    "href": "paired_t_test.html#run-the-paired-t-test",
    "title": "4  Paired t-test",
    "section": "\n4.6 Run the paired t-test",
    "text": "4.6 Run the paired t-test\nWe will perform a paired t-test to test the null hypothesis that the mean differences of weight equals to zero.\n\n\n\n\n\n\nPaired t-test\n\n\n\n\n\nBase R (1st way)\nBase R (2nd way)\nrstatix\n\n\n\nOur data are in a wide format. However, we are going to use only the dif_weight variable, inside the t.test():\n\nt.test(weight$dif_weight)\n\n\n    One Sample t-test\n\ndata:  weight$dif_weight\nt = 6.1428, df = 24, p-value = 2.401e-06\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 26.32139 52.95861\nsample estimates:\nmean of x \n    39.64 \n\n\n\n\n\nt.test(weight$birth_weight, weight$discharge_weight, paired = T)\n\n\n    Paired t-test\n\ndata:  weight$birth_weight and weight$discharge_weight\nt = 6.1428, df = 24, p-value = 2.401e-06\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 26.32139 52.95861\nsample estimates:\nmean difference \n          39.64 \n\n\n\n\n\nweight %>% \n  t_test(dif_weight ~ 1, detailed = T)\n\n# A tibble: 1 × 12\n  estimate .y.   group1 group2     n stati…¹      p    df conf.…² conf.…³ method\n*    <dbl> <chr> <chr>  <chr>  <int>   <dbl>  <dbl> <dbl>   <dbl>   <dbl> <chr> \n1     39.6 dif_… 1      null …    25    6.14 2.4e-6    24    26.3    53.0 T-test\n# … with 1 more variable: alternative <chr>, and abbreviated variable names\n#   ¹​statistic, ²​conf.low, ³​conf.high"
  },
  {
    "objectID": "paired_t_test.html#present-the-results-in-a-summary-table",
    "href": "paired_t_test.html#present-the-results-in-a-summary-table",
    "title": "4  Paired t-test",
    "section": "\n4.7 Present the results in a summary table",
    "text": "4.7 Present the results in a summary table\n\nShow the codetb1 <- weight %>% \n  mutate(id = row_number()) %>% \n  select(-dif_weight) %>% \n  pivot_longer(!id, names_to = \"group\", values_to = \"weights\")\n\ngt_sum3 <- tb1 %>% \ntbl_summary(by = group, include = -id,\n            label = list(weights ~ \"weights (grams)\"),\n            statistic =  weights ~ \"{mean} ({sd})\") %>%\n  #add_difference(test = weights ~ \"paired.t.test\", group = id) %>%\n  add_p(test = weights ~ \"paired.t.test\", group = id) %>% \n  #modify_spanning_header(all_stat_cols() ~ \"**Weight**\") %>% \n  as_gt()\n  \ngt_sum3 \n\n\n\n\n\n\nCharacteristic\n      \nbirth_weight, N = 251\n\n      \ndischarge_weight, N = 251\n\n      \np-value2\n\n    \n\nweights (grams)\n3,076 (248)\n3,036 (248)\n<0.001\n\n\n\n\n1 Mean (SD)\n    \n\n\n2 Paired t-test\n    \n\n\n\n\n\nThere was a significant reduction in weight (39.6 g) after the discharge (p-value <0.001 that is lower than 0.05; reject \\(H_0\\)). Note that the 95% confidence interval (26.3 to 52.9) doesn’t include the null hypothesized value of 0. However, is this reduction of clinical importance?"
  },
  {
    "objectID": "wilcoxon_test.html",
    "href": "wilcoxon_test.html",
    "title": "5  Wilcoxon Signed-Rank test",
    "section": "",
    "text": "Wilcoxon Signed-Rank test (Wilcoxon test) is a non-parametric test that can be conducted to compare paired samples when the differences are not normally distributed. It is based on the signs of the differences and the magnitude of the rank of the differences between pairs of measurements, rather than the actual values. The null hypothesis is that there is no tendency for values under each paired variable to be higher or lower. It is often thought of as a test for small samples. However, if the sample is smaller than 6, then statistical significance is impossible.\nWhen we have finished this Chapter, we should be able to:"
  },
  {
    "objectID": "wilcoxon_test.html#research-question",
    "href": "wilcoxon_test.html#research-question",
    "title": "5  Wilcoxon Signed-Rank test",
    "section": "\n5.1 Research question",
    "text": "5.1 Research question\nThe dataset eyes contains thickness of the cornea (in microns) in patients with one eye affected by glaucoma; the other eye is unaffected. We investigate if there is evidence for difference in corneal thickness in affected and unaffected eyes.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\\(H_0\\): The distribution of the differences in thickness of the cornea is symmetrical about zero\n\\(H_1\\): The distribution of the differences in thickness of the cornea is not symmetrical about zero\n\n\n\nNOTE: If we are testing the null hypothesis that the median of the paired rank differences is zero, then the paired rank differences must all come from a symmetrical distribution. Note that we do not have to assume that the distributions of the original populations are symmetrical - two very positively skewed distributions that differ only by location will produce a set of paired rank differences that are symmetrical."
  },
  {
    "objectID": "wilcoxon_test.html#packages-we-need",
    "href": "wilcoxon_test.html#packages-we-need",
    "title": "5  Wilcoxon Signed-Rank test",
    "section": "\n5.2 Packages we need",
    "text": "5.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(gtsummary)\n\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "wilcoxon_test.html#preraring-the-data",
    "href": "wilcoxon_test.html#preraring-the-data",
    "title": "5  Wilcoxon Signed-Rank test",
    "section": "\n5.3 Preraring the data",
    "text": "5.3 Preraring the data\nWe import the data eyes in R:\n\nlibrary(readxl)\neyes <- read_excel(here(\"data\", \"eyes.xlsx\"))\n\nWe calculate the differences using the function mutate():\n\neyes <- eyes %>%\n  mutate(dif_thickness = affected_eye - unaffected_eye)\n\n\n\n\nFigure 5.1: Table with data from “weight” file.\n\n\n\nWe inspect the data:\n\nglimpse(eyes) \n\nRows: 8\nColumns: 4\n$ id             <dbl> 1, 2, 3, 4, 5, 6, 7, 8\n$ affected_eye   <dbl> 488, 478, 480, 426, 440, 410, 458, 460\n$ unaffected_eye <dbl> 484, 478, 492, 444, 436, 398, 464, 476\n$ dif_thickness  <dbl> 4, 0, -12, -18, 4, 12, -6, -16"
  },
  {
    "objectID": "wilcoxon_test.html#explore-the-characteristics-of-distribution-of-differences",
    "href": "wilcoxon_test.html#explore-the-characteristics-of-distribution-of-differences",
    "title": "5  Wilcoxon Signed-Rank test",
    "section": "\n5.4 Explore the characteristics of distribution of differences",
    "text": "5.4 Explore the characteristics of distribution of differences\nThe distributions of differences can be explored with appropriate plots and summary statistics.\nGraph\nWe can explore the data visually for symmetry with a density plot.\n\neyes %>%\n  ggplot(aes(x = dif_thickness)) +\n  geom_density(fill = \"#76B7B2\", color=\"black\", alpha = 0.2) +\n  geom_vline(aes(xintercept=mean(dif_thickness)),\n            color=\"blue\", linetype=\"dashed\", size=1.2) +\n  geom_vline(aes(xintercept=median(dif_thickness)),\n            color=\"red\", linetype=\"dashed\", size=1.2) +\n  labs(x = \"Differences of thickness (micron)\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\")\n\n\n\nFigure 5.2: Density plot of the thickness differences.\n\n\n\n\nSummary statistics\nSummary statistics can also be calculated for the variables.\n\n\n\n\n\n\nSummary statistics\n\n\n\n\n\ndplyr\ndlookr\n\n\n\nWe can utilize the across() function to obtain the results across the three variables simultaneously:\n\nsummary_eyes <- eyes %>%\n  dplyr::summarise(across(\n    .cols = c(dif_thickness, affected_eye, unaffected_eye), \n    .fns = list(\n    n = ~n(),\n      min = ~min(., na.rm = TRUE),\n      q1 = ~quantile(., 0.25, na.rm = TRUE),\n      median = ~quantile(., 0.5, na.rm = TRUE),\n      q3 = ~quantile(., 0.75, na.rm = TRUE),\n      max = ~max(., na.rm = TRUE),\n      mean = ~mean(., na.rm = TRUE),\n      sd = ~sd(., na.rm = TRUE),\n      skewness = ~EnvStats::skewness(., na.rm = TRUE),\n      kurtosis= ~EnvStats::kurtosis(., na.rm = TRUE)\n    ),\n    .names = \"{col}_{fn}\")\n    )\n\n# present the results\nsummary_eyes <- summary_eyes %>% \n  mutate(across(everything(), round, 2)) %>%   # round to 3 decimal places\n  pivot_longer(1:30, names_to = \"Stats\", values_to = \"Values\")  # long format\n\nsummary_eyes\n\n# A tibble: 30 × 2\n   Stats                  Values\n   <chr>                   <dbl>\n 1 dif_thickness_n          8   \n 2 dif_thickness_min      -18   \n 3 dif_thickness_q1       -13   \n 4 dif_thickness_median    -3   \n 5 dif_thickness_q3         4   \n 6 dif_thickness_max       12   \n 7 dif_thickness_mean      -4   \n 8 dif_thickness_sd        10.7 \n 9 dif_thickness_skewness   0.03\n10 dif_thickness_kurtosis  -1.37\n# … with 20 more rows\n\n\n\n\n\neyes %>% \n  dlookr::describe(dif_thickness, affected_eye, unaffected_eye) %>% \n  select(described_variables, n, mean, sd, p25, p50, p75, skewness, kurtosis) %>% \n  ungroup()\n\n# A tibble: 3 × 9\n  described_variables     n  mean    sd   p25   p50   p75 skewness kurtosis\n  <chr>               <int> <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>    <dbl>\n1 dif_thickness           8    -4  10.7  -13     -3    4    0.0295   -1.37 \n2 affected_eye            8   455  27.7  436.   459  478.  -0.493    -0.985\n3 unaffected_eye          8   459  31.3  442    470  480.  -1.11      0.794\n\n\n\n\n\n\n\nThe differences seems to come from a population with a symmetrical distribution and the skewness is close to zero (0.03). However, the (excess) kurtosis equals to -1.37 (platykurtic) and the sample size is small. Therefore, the data may not follow the normal distribution.\nNormality test We can use Shapiro-Wilk test to check for normality of the differences. However, here, normality test is not helpful because of the small sample (the test is under-powered).\n\n eyes %>%\n    shapiro_test(dif_thickness)\n\n# A tibble: 1 × 3\n  variable      statistic     p\n  <chr>             <dbl> <dbl>\n1 dif_thickness     0.944 0.651\n\n\nThe Shapiro-Wilk test suggests that the weight differences are normally distributed (p=0.65 > 0.05)."
  },
  {
    "objectID": "wilcoxon_test.html#run-the-wilcoxon-signed-rank-test",
    "href": "wilcoxon_test.html#run-the-wilcoxon-signed-rank-test",
    "title": "5  Wilcoxon Signed-Rank test",
    "section": "\n5.5 Run the Wilcoxon Signed-Rank test",
    "text": "5.5 Run the Wilcoxon Signed-Rank test\nThe differences between the two measurements can be tested using a rank test such as Wilcoxon Signed-Rank test.\n\n\n\n\n\n\nWilcoxon Signed-Rank test\n\n\n\n\n\nBase R (1st way)\nBase R (2nd way)\nrstatix\n\n\n\n\nwilcox.test(eyes$dif_thickness)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  eyes$dif_thickness\nV = 7.5, p-value = 0.3088\nalternative hypothesis: true location is not equal to 0\n\n\n\n\n\nwilcox.test(eyes$affected_eye, eyes$unaffected_eye, paired = TRUE)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  eyes$affected_eye and eyes$unaffected_eye\nV = 7.5, p-value = 0.3088\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n\n eyes %>%\n     wilcox_test(dif_thickness ~ 1)\n\n# A tibble: 1 × 6\n  .y.           group1 group2         n statistic     p\n* <chr>         <chr>  <chr>      <int>     <dbl> <dbl>\n1 dif_thickness 1      null model     8       7.5 0.309\n\n\n\n\n\n\n\nThe result is not significant (p = 0.31 > 0.05). However, we can’t be certain that there is not difference in corneal thickness in affected and unaffected eyes because the sample size is very small."
  },
  {
    "objectID": "wilcoxon_test.html#present-the-results-in-a-summary-table",
    "href": "wilcoxon_test.html#present-the-results-in-a-summary-table",
    "title": "5  Wilcoxon Signed-Rank test",
    "section": "\n5.6 Present the results in a summary table",
    "text": "5.6 Present the results in a summary table\n\nShow the codetb2 <- eyes %>%\n  select(-dif_thickness) %>% \n  pivot_longer(!id, names_to = \"groups\", values_to = \"thickness\")\n\ngt_sum4 <- tb2 %>% \ntbl_summary(by = groups, include = -id,\n            label = list(thickness ~ \"thickness (microns)\"),\n            digits = list(everything() ~ 1)) %>%\n  add_p(test = thickness ~ \"paired.wilcox.test\", group = id) %>% \n  as_gt()\n  \ngt_sum4 \n\n\n\n\n\n\nCharacteristic\n      \naffected_eye, N = 81\n\n      \nunaffected_eye, N = 81\n\n      \np-value2\n\n    \n\nthickness (microns)\n459.0 (436.5, 478.5)\n470.0 (442.0, 479.5)\n0.3\n\n\n\n\n1 Median (IQR)\n    \n\n\n2 Wilcoxon signed rank test with continuity correction\n    \n\n\n\n\n\nThere is not evidence from this small study with patients of glaucoma that the thickness of the cornea in affected eyes, median = 459 \\(\\mu{m}\\) (IQR: 436.5, 478.5) \\(\\mu{m}\\), differs from unaffected eyes 470 \\(\\mu{m}\\) (442, 479.5) (p=0.30 >0.05)."
  },
  {
    "objectID": "anova.html",
    "href": "anova.html",
    "title": "6  One-way ANOVA test",
    "section": "",
    "text": "One-way analysis of variance, usually referred to as one-way ANOVA, is a statistical test used when we want to compare several means. We may think of it as an extension of Student’s t-test to the case of more than two independent samples.\nAlthough, this test can detect a difference between several groups it does not inform us about which groups are different from the others. At first glance, we might think to compare all groups in pairs with multiple t-tests. However, this procedure may lead to incorrect conclusions (known as multiple comparisons problem) because each comparison increases the likelihood of committing at least one Type I error within a set of comparisons (familly-wise Type I error rate).\nThis is the reason why, after an ANOVA test concluding on a significant difference between group means, we should not just compare all possible pairs of groups with t-tests. Instead we perform statistical tests that take into account the number of planned comparisons (post hoc tests) and make the necessary adjustments to ensure that Type I error is not inflated.\nWhen we have finished this Chapter, we should be able to:"
  },
  {
    "objectID": "anova.html#research-question-and-hypothesis-testing",
    "href": "anova.html#research-question-and-hypothesis-testing",
    "title": "6  One-way ANOVA test",
    "section": "\n6.1 Research question and Hypothesis Testing",
    "text": "6.1 Research question and Hypothesis Testing\nWe consider the data in dataDWL dataset. In this example we explore the variations between weight loss according to four different types of diet. The question that may be asked is: does the average weight loss differ according to the diet?\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): all group means are equal (the means of weight loss in the four diets are equal; \\(\\mu_{1} = \\mu_{2} = \\mu_{3} = \\mu_{4}\\))\n\n\\(H_1\\): at least one group mean differs from the others (there is at least one diet with mean weight loss different from the others)"
  },
  {
    "objectID": "anova.html#packages-we-need",
    "href": "anova.html#packages-we-need",
    "title": "6  One-way ANOVA test",
    "section": "\n6.2 Packages we need",
    "text": "6.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(PupillometryR)\nlibrary(gtsummary)\n\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "anova.html#preraring-the-data",
    "href": "anova.html#preraring-the-data",
    "title": "6  One-way ANOVA test",
    "section": "\n6.3 Preraring the data",
    "text": "6.3 Preraring the data\nWe import the data dataDWL in R:\n\nlibrary(readxl)\ndataDWL <- read_excel(here(\"data\", \"dataDWL.xlsx\"))\n\n\n\n\nFigure 6.1: Table with data from “dataDWL” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(dataDWL)\n\nRows: 60\nColumns: 2\n$ WeightLoss <dbl> 9.9, 9.6, 8.0, 4.9, 10.2, 9.0, 9.8, 10.8, 6.2, 8.3, 12.9, 1…\n$ Diet       <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\",…\n\n\nThe dataset dataDWL has 60 participants and includes two variables. The numeric (<dbl>) WeightLoss variable and the character (<chr>) Diet variable (with levels “A”, “B”, “C” and “D”) which should be converted to a factor variable using the factor() function as follows:\n\ndataDWL <- dataDWL %>% \n  mutate(Diet = factor(Diet))\nglimpse(dataDWL)\n\nRows: 60\nColumns: 2\n$ WeightLoss <dbl> 9.9, 9.6, 8.0, 4.9, 10.2, 9.0, 9.8, 10.8, 6.2, 8.3, 12.9, 1…\n$ Diet       <fct> A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, B, B, B, B, B,…"
  },
  {
    "objectID": "anova.html#assumptions",
    "href": "anova.html#assumptions",
    "title": "6  One-way ANOVA test",
    "section": "\n6.4 Assumptions",
    "text": "6.4 Assumptions\n\n\n\n\n\n\nCheck if the following assumptions are satisfied\n\n\n\n\nThe data are normally distributed in all groups\nThe data in all groups have similar variance (also named as homogeneity of variance or homoscedasticity)\n\n\n\nA. Explore the characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\nGraphs\nWe can visualize the distribution of WeightLoss for the four Diet groups:\n\nset.seed(123)\nggplot(dataDWL, aes(x=Diet, y=WeightLoss)) + \n  geom_flat_violin(aes(fill = Diet), scale = \"count\") +\n  geom_boxplot(width = 0.14, outlier.shape = NA, alpha = 0.5) +\n  geom_point(position = position_jitter(width = 0.05), \n             size = 1.2, alpha = 0.6) +\n  ggsci::scale_fill_jco() +\n  theme_classic(base_size = 14) +\n  theme(legend.position=\"none\", \n        axis.text = element_text(size = 14))\n\n\n\nFigure 6.2: Rain cloud plot.\n\n\n\n\nThe above figure shows that the data are close to symmetry and the assumption of a normal distribution is reasonable. Additionally, we can observe that the largest weight loss seems to have been achieved by the participants in C diet.\nSummary statistics\nThe WeightLoss summary statistics for each diet group are:\n\n\n\n\n\n\nSummary statistics by group\n\n\n\n\n\ndplyr\ndlookr\n\n\n\n\nDWL_summary <- dataDWL %>%\n  group_by(Diet) %>%\n  dplyr::summarise(\n    n = n(),\n    min = min(WeightLoss, na.rm = TRUE),\n    q1 = quantile(WeightLoss, 0.25, na.rm = TRUE),\n    median = quantile(WeightLoss, 0.5, na.rm = TRUE),\n    q3 = quantile(WeightLoss, 0.75, na.rm = TRUE),\n    max = max(WeightLoss, na.rm = TRUE),\n    mean = mean(WeightLoss, na.rm = TRUE),\n    sd = sd(WeightLoss, na.rm = TRUE),\n    skewness = EnvStats::skewness(WeightLoss, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(WeightLoss, na.rm = TRUE)\n  ) %>%\n  ungroup()\n\nDWL_summary\n\n# A tibble: 4 × 11\n  Diet      n   min    q1 median    q3   max  mean    sd skewness kurtosis\n  <fct> <int> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>    <dbl>\n1 A        15   4.9  8.15    9.6  10.5  12.9  9.18  2.30  -0.471    -0.302\n2 B        15   3.8  7.85    9.2  10.8  12.7  8.91  2.78  -0.467    -0.515\n3 C        15   8.7 10.8    12.2  13    15.1 12.1   1.79  -0.0451   -0.530\n4 D        15   5.8  9.5    10.5  11.8  13.7 10.5   2.23  -0.475     0.229\n\n\n\n\n\ndataDWL %>% \n  group_by(Diet) %>% \n  dlookr::describe(WeightLoss) %>% \n  select(described_variables,  Diet, n, mean, sd, p25, p50, p75, skewness, kurtosis) %>% \n  ungroup()\n\n# A tibble: 4 × 10\n  described_variables Diet      n  mean    sd   p25   p50   p75 skewness kurto…¹\n  <chr>               <fct> <int> <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>   <dbl>\n1 WeightLoss          A        15  9.18  2.30  8.15   9.6  10.5  -0.471   -0.302\n2 WeightLoss          B        15  8.91  2.78  7.85   9.2  10.8  -0.467   -0.515\n3 WeightLoss          C        15 12.1   1.79 10.8   12.2  13    -0.0451  -0.530\n4 WeightLoss          D        15 10.5   2.23  9.5   10.5  11.8  -0.475    0.229\n# … with abbreviated variable name ¹​kurtosis\n\n\n\n\n\n\n\nThe means are close to medians and the standard deviations are also similar. Moreover, both skewness and (excess) kurtosis falls into the acceptable range of [-1, 1] indicating approximately normal distributions for all diet groups.\n \nNormality test\nThe Shapiro-Wilk test for normality for each diet group is:\n\ndataDWL %>%\n  group_by(Diet) %>%\n  shapiro_test(WeightLoss) %>% \n  ungroup()\n\n# A tibble: 4 × 4\n  Diet  variable   statistic     p\n  <fct> <chr>          <dbl> <dbl>\n1 A     WeightLoss     0.958 0.662\n2 B     WeightLoss     0.941 0.390\n3 C     WeightLoss     0.964 0.768\n4 D     WeightLoss     0.944 0.435\n\n\nThe tests of normality suggest that the data for the WeightLoss in all groups are normally distributed (p > 0.05).\nB. Levene’s test for equality of variances\nThe Levene’s test for equality of variances is:\n\ndataDWL %>% \n  levene_test(WeightLoss ~ Diet)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  <int> <int>     <dbl> <dbl>\n1     3    56     0.600 0.617\n\n\nSince the p = 0.617 > 0.05, the null hypothesis (\\(H_{0}\\): the variances of WeighLoss in four diet groups are equal) can not be rejected."
  },
  {
    "objectID": "anova.html#run-the-one-way-anova-test",
    "href": "anova.html#run-the-one-way-anova-test",
    "title": "6  One-way ANOVA test",
    "section": "\n6.5 Run the one-way ANOVA test",
    "text": "6.5 Run the one-way ANOVA test\nNow, we will perform an one-way ANOVA (with equal variances: Fisher’s classic ANOVA) to test the null hypothesis that the mean weight loss is the same for all the diet groups.\n\n\n\n\n\n\nOne-way ANOVA test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\n# Compute the analysis of variance\nanova_one_way <- aov(WeightLoss ~ Diet, data = dataDWL)\n\n# Summary of the analysis\nsummary(anova_one_way)\n\n            Df Sum Sq Mean Sq F value  Pr(>F)   \nDiet         3  97.33   32.44   6.118 0.00113 **\nResiduals   56 296.99    5.30                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\ndataDWL %>% \n  anova_test(WeightLoss ~ Diet, detailed = T)\n\nANOVA Table (type II tests)\n\n  Effect   SSn     SSd DFn DFd     F     p p<.05   ges\n1   Diet 97.33 296.987   3  56 6.118 0.001     * 0.247\n\n\n\n\n\n\n\nThe statistic F=6.118 indicates the obtained F-statistic = (variation between sample means \\(/\\) variation within the samples). Note that we are comparing to an F-distribution (F-test). The degrees of freedom in the numerator (DFn) and the denominator (DFd) are 3 and 56, respectively (numarator: variation between sample means; denominator: variation within the samples).\nThe p=0.001 is lower than 0.05. There is at least one diet with mean weight loss which is different from the others means.\nFrom ANOVA table provided by the {rstatix} we can also calculate the generalized effect size (ges). The ges is the proportion of variability explained by the factor Diet (SSn) to total variability of the dependent variable (SSn + SSd), so:\n\\[\\ ges= 97.33 / (97.33 + 296.987) = 97.33 / 394.317 = 0.247\\] A ges of 0.247 (24.7%) means that 24.7% of the change in the weight loss can be accounted for the diet conditions.\n \nPresent the results in a summary table\nA summary table can also be presented:\n\nShow the codegt_sum4 <- dataDWL %>% \n  tbl_summary(\n    by = Diet, \n    statistic = WeightLoss ~ \"{mean} ({sd})\", \n    digits = list(everything() ~ 1),\n    label = list(WeightLoss ~ \"Weight Loss (kg)\"), \n    missing = c(\"no\")) %>% \n  add_p(test = WeightLoss ~ \"aov\", purrr::partial(style_pvalue, digits = 2)) %>% \n  as_gt() \n\ngt_sum4\n\n\n\n\n\n\nCharacteristic\n      \nA, N = 151\n\n      \nB, N = 151\n\n      \nC, N = 151\n\n      \nD, N = 151\n\n      \np-value2\n\n    \n\nWeight Loss (kg)\n9.2 (2.3)\n8.9 (2.8)\n12.1 (1.8)\n10.5 (2.2)\n0.001\n\n\n\n\n1 Mean (SD)\n    \n\n\n2 One-way ANOVA"
  },
  {
    "objectID": "anova.html#post-hoc-tests",
    "href": "anova.html#post-hoc-tests",
    "title": "6  One-way ANOVA test",
    "section": "\n6.6 Post-hoc tests",
    "text": "6.6 Post-hoc tests\nA significant one-way ANOVA is generally followed up by post-hoc tests to perform multiple pairwise comparisons between groups:\n\n\n\n\n\n\nPost-hoc tests\n\n\n\n\n\nTukey test\nBonferroni\n\n\n\nIt is appropriate to use this test when one desires all the possible comparisons between a large set of means (e.g., 6 or more means) and the variances are supposed to be equal.\n\n# Pairwise comparisons\npwc_Tukey <- dataDWL %>% \n  tukey_hsd(WeightLoss ~ Diet)\n\npwc_Tukey \n\n# A tibble: 6 × 9\n  term  group1 group2 null.value estimate conf.low conf.high   p.adj p.adj.sig…¹\n* <chr> <chr>  <chr>       <dbl>    <dbl>    <dbl>     <dbl>   <dbl> <chr>      \n1 Diet  A      B               0   -0.273   -2.50      1.95  0.988   ns         \n2 Diet  A      C               0    2.93     0.707     5.16  0.00513 **         \n3 Diet  A      D               0    1.36    -0.867     3.59  0.377   ns         \n4 Diet  B      C               0    3.21     0.980     5.43  0.0019  **         \n5 Diet  B      D               0    1.63    -0.593     3.86  0.222   ns         \n6 Diet  C      D               0   -1.57    -3.80      0.653 0.252   ns         \n# … with abbreviated variable name ¹​p.adj.signif\n\n\nThe output contains the following columns of interest:\n\nestimate: estimate of the difference between means of the two groups\nconf.low, conf.high: the lower and the upper end point of the confidence interval at 95% (default)\np.adj: p-value after adjustment for the multiple comparisons.\n\n\n\nAlternatively, we can perform pairwise comparisons using pairwise t-test with the assumption of equal variances (pool.sd = TRUE) and calculate the adjusted p-values using Bonferroni correction:\n\npwc_Bonferroni <- dataDWL %>% \n  pairwise_t_test(\n    WeightLoss ~ Diet, pool.sd = TRUE,\n    p.adjust.method = \"bonferroni\"\n    )\npwc_Bonferroni \n\n# A tibble: 6 × 9\n  .y.        group1 group2    n1    n2        p p.signif   p.adj p.adj.signif\n* <chr>      <chr>  <chr>  <int> <int>    <dbl> <chr>      <dbl> <chr>       \n1 WeightLoss A      B         15    15 0.746    ns       1       ns          \n2 WeightLoss A      C         15    15 0.000954 ***      0.00572 **          \n3 WeightLoss B      C         15    15 0.000344 ***      0.00206 **          \n4 WeightLoss A      D         15    15 0.111    ns       0.669   ns          \n5 WeightLoss B      D         15    15 0.0571   ns       0.343   ns          \n6 WeightLoss C      D         15    15 0.0666   ns       0.399   ns          \n\n\n\n\n\n\n\nPairwise comparisons were carried out using the method of Tukey (or Bonferroni) and the adjusted p-values were calculated.\nThe results in Tukey post hoc table show that the weight loss from diet C seems to be significantly larger than diet A (mean difference = 2.91 kg, 95%CI [0.71, 5.16], p=0.005 <0.05) and diet B (mean difference = 3.21 kg, 95%CI [0.98, 5.43], p=0.002 <0.05)."
  },
  {
    "objectID": "anova.html#welch-one-way-anova",
    "href": "anova.html#welch-one-way-anova",
    "title": "6  One-way ANOVA test",
    "section": "\n6.7 Welch one-way ANOVA",
    "text": "6.7 Welch one-way ANOVA\nIf the variance is different between the groups (unequal variances) then the degrees of freedom associated with the ANOVA test are calculated differently (Welch one-way ANOVA).\n\n# Welch one-way ANOVA test (not assuming equal variance)\n\ndataDWL %>% \n  welch_anova_test(WeightLoss ~ Diet)\n\n# A tibble: 1 × 7\n  .y.            n statistic   DFn   DFd        p method     \n* <chr>      <int>     <dbl> <dbl> <dbl>    <dbl> <chr>      \n1 WeightLoss    60      7.02     3  30.8 0.000989 Welch ANOVA\n\n\nIn this case, the Games-Howell post hoc test (or pairwise t-tests with no assumption of equal variances with Bonferroni correction) can be used to compare all possible combinations of group differences.\nGames-Howell post hoc test\n\n# Pairwise comparisons (Games-Howell)\n\npwc_GH <- dataDWL %>% \n  games_howell_test(WeightLoss ~ Diet)\n\npwc_GH\n\n# A tibble: 6 × 8\n  .y.        group1 group2 estimate conf.low conf.high p.adj p.adj.signif\n* <chr>      <chr>  <chr>     <dbl>    <dbl>     <dbl> <dbl> <chr>       \n1 WeightLoss A      B        -0.273   -2.82      2.28  0.991 ns          \n2 WeightLoss A      C         2.93     0.872     4.99  0.003 **          \n3 WeightLoss A      D         1.36    -0.898     3.62  0.371 ns          \n4 WeightLoss B      C         3.21     0.849     5.56  0.005 **          \n5 WeightLoss B      D         1.63    -0.889     4.16  0.308 ns          \n6 WeightLoss C      D        -1.57    -3.60      0.452 0.17  ns"
  },
  {
    "objectID": "kruskal_wallis.html",
    "href": "kruskal_wallis.html",
    "title": "7  Kruskal-Wallis test",
    "section": "",
    "text": "The Kruskal-Wallis test is a rank-based non-parametric alternative to the one-way ANOVA and an extension of the Wilcoxon-Mann-Whitney test to allow the comparison of more than two independent groups. It’s usually recommended when the assumptions of one-way ANOVA test are not met (non-normal distributions) or with small samples.\nWhen we have finished this Chapter, we should be able to:"
  },
  {
    "objectID": "kruskal_wallis.html#research-question-and-hypothesis-testing",
    "href": "kruskal_wallis.html#research-question-and-hypothesis-testing",
    "title": "7  Kruskal-Wallis test",
    "section": "\n7.1 Research question and Hypothesis Testing",
    "text": "7.1 Research question and Hypothesis Testing\nWe consider the data in dataVO2 dataset. We wish to compare the VO2max in three different sports (runners, rowers, and triathletes).\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): the distribution of VO2max is the same in all groups (the medians of VO2max in the three sports are the same)\n\n\\(H_1\\): there is at least one group with VO2max distribution different from the others (there is at least one sport with median VO2max different from the others)\n\n\n\nNOTE: The Kruskal-Wallis test should be regarded as a test of dominance between distributions comparing the mean ranks. The null hypothesis is that the observations from one group do not tend to have a higher or lower ranking than observations from the other groups. This test does not test the medians of the data as is commonly thought, it tests the whole distribution. However, if the distributions of the two groups have similar shapes, the Kruskal-Wallis test can be used to determine whether there are differences in the medians in the two groups. In practice, we use the medians to present the results."
  },
  {
    "objectID": "kruskal_wallis.html#packages-we-need",
    "href": "kruskal_wallis.html#packages-we-need",
    "title": "7  Kruskal-Wallis test",
    "section": "\n7.2 Packages we need",
    "text": "7.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(PupillometryR)\nlibrary(gtsummary)\n\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "kruskal_wallis.html#preraring-the-data",
    "href": "kruskal_wallis.html#preraring-the-data",
    "title": "7  Kruskal-Wallis test",
    "section": "\n7.3 Preraring the data",
    "text": "7.3 Preraring the data\nWe import the data dataVO2 in R:\n\nlibrary(readxl)\ndataVO2 <- read_excel(here(\"data\", \"dataVO2.xlsx\"))\n\n\n\n\nFigure 7.1: Table with data from “dataVO2” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(dataVO2)\n\nRows: 30\nColumns: 2\n$ sport  <chr> \"runners\", \"runners\", \"runners\", \"runners\", \"runners\", \"runners…\n$ VO2max <dbl> 73.8, 79.9, 75.5, 72.5, 82.2, 78.3, 77.9, 76.5, 72.3, 80.2, 71.…\n\n\nThe dataset dataVO2 has 30 participants and two variables. The numeric VO2max variable and the sport variable (with levels “roweres”, “runners”, and “triathletes”) which should be converted to a factor variable using the factor() function as follows:\n\ndataVO2 <- dataVO2 %>% \n  mutate(sport = factor(sport))\n\nglimpse(dataVO2)\n\nRows: 30\nColumns: 2\n$ sport  <fct> runners, runners, runners, runners, runners, runners, runners, …\n$ VO2max <dbl> 73.8, 79.9, 75.5, 72.5, 82.2, 78.3, 77.9, 76.5, 72.3, 80.2, 71.…"
  },
  {
    "objectID": "kruskal_wallis.html#explore-the-characteristics-of-distribution-for-each-group-and-check-for-normality",
    "href": "kruskal_wallis.html#explore-the-characteristics-of-distribution-for-each-group-and-check-for-normality",
    "title": "7  Kruskal-Wallis test",
    "section": "\n7.4 Explore the characteristics of distribution for each group and check for normality",
    "text": "7.4 Explore the characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\nGraph\nWe can visualize the distribution of VO2max for the three sport groups:\n\nset.seed(123)\nggplot(dataVO2, aes(x=sport, y=VO2max)) + \n  geom_flat_violin(aes(fill = sport), scale = \"count\") +\n  geom_boxplot(width = 0.14, outlier.shape = NA, alpha = 0.5) +\n  geom_point(position = position_jitter(width = 0.05), \n             size = 1.2, alpha = 0.6) +\n  ggsci::scale_fill_jco() +\n  theme_classic(base_size = 14) +\n  theme(legend.position=\"none\", \n        axis.text = element_text(size = 14))\n\n\n\nFigure 7.2: Rain cloud plot.\n\n\n\n\nThe above figure shows that the data in triathletes group have some outliers. Additionally, we can observe that the runners group seems to have the largest VO2max.\nSummary statistics\nThe VO2max summary statistics for each sport group are:\n\n\n\n\n\n\nSummary statistics by group\n\n\n\n\n\ndplyr\ndlookr\n\n\n\n\nVO2_summary <- dataVO2 %>%\n  group_by(sport) %>%\n  dplyr::summarise(\n    n = n(),\n    min = min(VO2max, na.rm = TRUE),\n    q1 = quantile(VO2max, 0.25, na.rm = TRUE),\n    median = quantile(VO2max, 0.5, na.rm = TRUE),\n    q3 = quantile(VO2max, 0.75, na.rm = TRUE),\n    max = max(VO2max, na.rm = TRUE),\n    mean = mean(VO2max, na.rm = TRUE),\n    sd = sd(VO2max, na.rm = TRUE),\n    skewness = EnvStats::skewness(VO2max, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(VO2max, na.rm = TRUE)\n  ) %>%\n  ungroup()\n\nVO2_summary\n\n# A tibble: 3 × 11\n  sport           n   min    q1 median    q3   max  mean    sd skewness kurtosis\n  <fct>       <int> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>    <dbl>\n1 rowers         10  67.1  67.8   69.6  73.1  74.7  70.3  3.04  0.502      -1.53\n2 runners        10  72.3  74.2   77.2  79.5  82.2  76.9  3.39 -0.00950    -1.16\n3 triathletes    10  63.2  64     65.4  67.6  76.6  67.0  4.40  1.51        1.60\n\n\n\n\n\ndataVO2 %>% \n  group_by(sport) %>% \n  dlookr::describe(VO2max) %>% \n  select(described_variables,  sport, n, mean, sd, p25, p50, p75, skewness, kurtosis) %>% \n  ungroup()\n\n# A tibble: 3 × 10\n  described_variables sport     n  mean    sd   p25   p50   p75 skewness kurto…¹\n  <chr>               <fct> <int> <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>   <dbl>\n1 VO2max              rowe…    10  70.3  3.04  67.8  69.6  73.1  0.502     -1.53\n2 VO2max              runn…    10  76.9  3.39  74.2  77.2  79.5 -0.00950   -1.16\n3 VO2max              tria…    10  67.0  4.40  64    65.4  67.6  1.51       1.60\n# … with abbreviated variable name ¹​kurtosis\n\n\n\n\n\n\n\nThe sample size is relative small (10 observations in each group). Moreover, the skewness (1.5) and the (excess) kurtosis (1.6) for the triathletes fall outside of the acceptable range of [-1, 1] indicating right-skewed and leptokurtic distribution.\nNormality test\nThe Shapiro-Wilk test for normality for each sport group is:\n\ndataVO2 %>%\n  group_by(sport) %>%\n  shapiro_test(VO2max) %>% \n  ungroup()\n\n# A tibble: 3 × 4\n  sport       variable statistic      p\n  <fct>       <chr>        <dbl>  <dbl>\n1 rowers      VO2max       0.865 0.0872\n2 runners     VO2max       0.954 0.712 \n3 triathletes VO2max       0.816 0.0229\n\n\nWe can see that the data for the triathletes is not normally distributed (p=0.023 <0.05) according to the Shapiro-Wilk test.\nBy considering all of the information together (small samples, graphs, normality test) the overall decision is against of normality."
  },
  {
    "objectID": "kruskal_wallis.html#run-the-kruskal-wallis-test",
    "href": "kruskal_wallis.html#run-the-kruskal-wallis-test",
    "title": "7  Kruskal-Wallis test",
    "section": "\n7.5 Run the Kruskal-Wallis test",
    "text": "7.5 Run the Kruskal-Wallis test\nNow, we will perform a Kruskal-Wallis test to compare the VO2max in three sports.\n\n\n\n\n\n\nKruskal-Wallis test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nkruskal.test(VO2max ~ sport, data = dataVO2)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  VO2max by sport\nKruskal-Wallis chi-squared = 16.351, df = 2, p-value = 0.0002815\n\n\n\n\n\ndataVO2 %>% \n  kruskal_test(VO2max ~ sport)\n\n# A tibble: 1 × 6\n  .y.        n statistic    df        p method        \n* <chr>  <int>     <dbl> <int>    <dbl> <chr>         \n1 VO2max    30      16.4     2 0.000281 Kruskal-Wallis\n\n\n\n\n\n\n\nThe p-value (<0.001) is lower than 0.05. There is at least one sport in which the VO2max is different from the others.\n \nPresent the results in a summary table\nA summary table can also be presented:\n\nShow the codegt_sum10 <- dataVO2 %>% \n  tbl_summary(\n    by = sport, \n    statistic = VO2max ~ \"{median} ({p25}, {p75})\", \n    digits = list(everything() ~ 1),\n    label = list(VO2max ~ \"VO2max (mL/kg/min)\"), \n    missing = c(\"no\")) %>% \n  add_p(test = VO2max ~ \"kruskal.test\", purrr::partial(style_pvalue, digits = 2)) %>%\n  as_gt() \n\ngt_sum10\n\n\n\n\n\n\nCharacteristic\n      \nrowers, N = 101\n\n      \nrunners, N = 101\n\n      \ntriathletes, N = 101\n\n      \np-value2\n\n    \n\nVO2max (mL/kg/min)\n69.6 (67.8, 73.1)\n77.2 (74.2, 79.5)\n65.4 (64.0, 67.6)\n<0.001\n\n\n\n\n1 Median (IQR)\n    \n\n\n2 Kruskal-Wallis rank sum test"
  },
  {
    "objectID": "kruskal_wallis.html#post-hoc-tests",
    "href": "kruskal_wallis.html#post-hoc-tests",
    "title": "7  Kruskal-Wallis test",
    "section": "\n7.6 Post-hoc tests",
    "text": "7.6 Post-hoc tests\nA significant WMW is generally followed up by post-hoc tests to perform multiple pairwise comparisons between groups:\n\n\n\n\n\n\nPost-hoc tests\n\n\n\n\n\nDunn’s approach\nWMW with Bonferroni\n\n\n\n\n# Pairwise comparisons\npwc_Dunn <- dataVO2 %>% \n  dunn_test(VO2max ~ sport, p.adjust.method = \"bonferroni\")\n\npwc_Dunn \n\n# A tibble: 3 × 9\n  .y.    group1  group2         n1    n2 statistic         p    p.adj p.adj.si…¹\n* <chr>  <chr>   <chr>       <int> <int>     <dbl>     <dbl>    <dbl> <chr>     \n1 VO2max rowers  runners        10    10      2.43 0.0152    0.0457   *         \n2 VO2max rowers  triathletes    10    10     -1.59 0.112     0.337    ns        \n3 VO2max runners triathletes    10    10     -4.01 0.0000596 0.000179 ***       \n# … with abbreviated variable name ¹​p.adj.signif\n\n\n\n\nAlternatively, we can perform pairwise comparisons using pairwise WMW’s test and calculate the adjusted p-values using Bonferroni correction:\n\n# Pairwise comparisons\n\npwc_BW <- dataVO2 %>% \n  pairwise_wilcox_test(VO2max ~ sport, p.adjust.method = \"bonferroni\")\n\npwc_BW\n\n# A tibble: 3 × 9\n  .y.    group1  group2         n1    n2 statistic        p p.adj p.adj.signif\n* <chr>  <chr>   <chr>       <int> <int>     <dbl>    <dbl> <dbl> <chr>       \n1 VO2max rowers  runners        10    10       8.5 0.002    0.006 **          \n2 VO2max rowers  triathletes    10    10      80.5 0.023    0.07  ns          \n3 VO2max runners triathletes    10    10      93   0.000487 0.001 **          \n\n\n\n\n\n\n\nDunn’s pairwise comparisons were carried out using the method of Bonferroni and adjusting the p-values were calculated.\nThe runners’ VO2max (median= 77.2, IQR=[74.2, 79.5] mL/kg/min) seems to differ significantly (larger based on the medians) from rowers (69.6 [67.8, 73.1] mL/kg/min, p=0.046 <0.05) and triathletes (65.4 [64.0, 67.6] mL/kg/min, p <0.001)."
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "8  Correlation",
    "section": "",
    "text": "Correlation is a statistical method used to assess a possible association between two numeric variables, X and Y. There are several statistical coefficients that we can use to quantify correlation depending on the underlying relation of the data. In this chapter, we’ll learn about four correlation coefficients:\nWhen we have finished this Chapter, we should be able to:"
  },
  {
    "objectID": "correlation.html#research-question-and-hypothesis-testing",
    "href": "correlation.html#research-question-and-hypothesis-testing",
    "title": "8  Correlation",
    "section": "\n8.1 Research question and Hypothesis Testing",
    "text": "8.1 Research question and Hypothesis Testing\nWe consider the data in Birthweight dataset. Let’s say that we want to explore the association between weight (in Kg) and height (in cm) for a sample of 550 infants of 1 month age.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There is no association between the two numeric variables (they are independent)\n\n\\(H_1\\): There is association between the two numeric variables (they are dependent)"
  },
  {
    "objectID": "correlation.html#packages-we-need",
    "href": "correlation.html#packages-we-need",
    "title": "8  Correlation",
    "section": "\n8.2 Packages we need",
    "text": "8.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(XICOR)\nlibrary(ggExtra)\n\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "correlation.html#preraring-the-data",
    "href": "correlation.html#preraring-the-data",
    "title": "8  Correlation",
    "section": "\n8.3 Preraring the data",
    "text": "8.3 Preraring the data\nWe import the data BirthWeight in R:\n\nlibrary(readxl)\nBirthWeight <- read_excel(here(\"data\", \"BirthWeight.xlsx\"))\n\n\n\n\nFigure 8.1: Table with data from “BirthWeight” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(BirthWeight)\n\nRows: 550\nColumns: 7\n$ id        <chr> \"L001\", \"L003\", \"L004\", \"L005\", \"L006\", \"L007\", \"L008\", \"L00…\n$ weight    <dbl> 4.0, 4.6, 4.8, 3.9, 4.6, 3.6, 3.6, 4.5, 5.0, 3.7, 4.2, 4.4, …\n$ height    <dbl> 55.5, 57.0, 56.0, 56.0, 55.0, 51.5, 56.0, 57.0, 58.5, 52.0, …\n$ headc     <dbl> 37.5, 38.5, 38.5, 39.0, 39.5, 34.5, 38.0, 39.7, 39.0, 38.0, …\n$ gender    <chr> \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Femal…\n$ education <chr> \"tertiary\", \"tertiary\", \"year12\", \"tertiary\", \"year10\", \"ter…\n$ parity    <chr> \"2 or more siblings\", \"Singleton\", \"2 or more siblings\", \"On…\n\n\nThe data set BirthWeight has 550 infants of 1 month age (rows) and includes seven variables (columns). Both the weight and height are numeric (<dbl>) variables."
  },
  {
    "objectID": "correlation.html#plot-the-data",
    "href": "correlation.html#plot-the-data",
    "title": "8  Correlation",
    "section": "\n8.4 Plot the data",
    "text": "8.4 Plot the data\nA first step that is usually useful in studying the association between two numeric variables is to prepare a scatter plot of the data. The pattern made by the points plotted on the scatter plot usually suggests the basic nature and strength of the association between two variables.\n\np <- ggplot(BirthWeight, aes(height, weight)) +\n  geom_point(color = \"blue\", size = 2) +\n  theme_minimal(base_size = 14)\n\nggMarginal(p, type = \"histogram\", \n           xparams = list(fill = 7),\n           yparams = list(fill = 3))\n\n\n\nFigure 8.2: Scatter plot of the association between height and weight in 550 infants of 1 month age.\n\n\n\n\nThe points in the scatter plot seem to be scattered around an invisible line. The scatter plot also shows that, in general, infants with high height tend to have high weight (positive association).\nAdditionally, the marginal histograms show that the data are approximately normally distributed (we have a large sample so the graphs are reliable) for both weight and height."
  },
  {
    "objectID": "correlation.html#correlation-between-two-numeric-variables",
    "href": "correlation.html#correlation-between-two-numeric-variables",
    "title": "8  Correlation",
    "section": "\n8.5 Correlation between two numeric variables",
    "text": "8.5 Correlation between two numeric variables\nCorrelation coefficients\nPearson’s coefficient measures linear correlation, while the Spearman’s and Kendall’s coefficients compare the ranks of data and measures monotonic associations. These coefficients are very powerful for detecting linear or monotonic associations, respectively. The new \\(ξ\\) correlation coefficient is more appropriate to measure the strength of non-monotonic associations.\nNote that the correlation between variables X and Y is equal to the correlation between variables Y and X so the order of the variables in the functions does not matter. The four correlations coefficients are:\n\n\n\n\n\n\nCorrelation coefficients\n\n\n\n\n\nPearson’s \\(r\\)\nSpearman’s \\(r_{s}\\)\nKendall’s \\(\\tau\\)\nCoefficient \\(ξ\\)\n\n\n\nThe Pearson’s correlation coefficient can be calculated for any dataset with two numeric variables. However, before we calculate the Pearson’s coefficient we should make sure that the following assumptions are met:\nAssumptions for Pearson’s \\(r\\) coefficient\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\nThere is a linear association between the two variables.\nFor a valid hypothesis testing and calculation of confidence intervals both variables should have an approximately normal distribution.\n\nAbsence of outliers in the data set.\n\nPearson’s \\(r\\) coefficient is a dimensionless quantity that takes a value in the range -1 to +1. A positive value indicates that both variables increase (or decrease) together while a negative coefficient indicates that one variable decreases as the other variable increases and vice versa. The stronger the correlation, the closer the correlation coefficient comes to ±1.\n\ncor(BirthWeight$height, BirthWeight$weight)\n\n[1] 0.7127154\n\n\n\n\nThe basic idea of Spearman’s rank correlation is that the ranks of X and Y are obtained by first separately ordering their values from small to large and then computing the correlation between the two sets of ranks. The strength of correlation is denoted by the coefficient of rank correlation, named Spearman’s rank correlation coefficient, \\(r_{s}\\).\nAssumptions for Spearman’s \\(r_{s}\\) coefficient\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\nThere is a monotonic association between the two variables. In a monotonic association, the variables tend to move in the same relative direction, but not necessarily at a constant rate. So all linear correlations are monotonic but the opposite is not always true, because we can have also monotonic non-linear associations.\n\nSpearman’s \\(rho\\) coefficient is dimensionless quantity that take value in the range -1 to +1. A positive correlation coefficient indicates that both variables increase (or decrease) in value together and a negative coefficient indicates that one variable decreases in value as the other variable increases and vice versa. The stronger the correlation, the closer the correlation coefficient comes to ±1.\n\ncor(BirthWeight$height, BirthWeight$weight, method = \"spearman\")\n\n[1] 0.7109399\n\n\n\n\nThe Kendall’s \\(\\tau\\) coefficient is the best alternative to Spearman’s \\(rho\\) correlation when the sample size is small and has many tied ranks. It is used to test the similarities in the ordering of data when it is ranked by quantities. Kendall’s correlation coefficient uses pairs of observations and determines the strength of association based on the patter on concordance and discordance between the pairs.\nAssumptions for Kendall’s \\(\\tau\\) coefficient\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\nThere is a monotonic association between the two variables. In a monotonic association, the variables tend to move in the same relative direction, but not necessarily at a constant rate. So all linear correlations are monotonic but the opposite is not always true, because we can have also monotonic non-linear associations.\n\nKendall’s \\(\\tau\\) coefficient is dimensionless quantity that takes value in the range -1 to +1. A positive correlation coefficient indicates that both variables increase (or decrease) in value together and a negative coefficient indicates that one variable decreases in value as the other variable increases and vice versa. The stronger the correlation, the closer the correlation coefficient comes to ±1.\n\ncor(BirthWeight$height, BirthWeight$weight, method = \"kendal\")\n\n[1] 0.5511955\n\n\n\n\nThe correlation coefficient \\(ξ\\) converges to a limit which has an easy interpretation as a measure of dependence. The limit ranges from 0 to 1. It is 1 if and only if Y is a measurable function of X and 0 if and only if X and Y are independent. Thus, \\(ξ\\) gives an actual measure of the strength of the association and it can be used for non-monotonic associations. However, for monotonic associations, it does not indicate the direction of the association.\nAssumptions for \\(ξ\\) coefficient\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\n\n\nxicor(BirthWeight$height, BirthWeight$weight)\n\n[1] 0.3379234\n\n\n\n\n\n\n\nCorrelation tests\nA correlation test is used to test whether the correlation (denoted ρ) between two numeric variables is significantly different from 0 or not in the population.\n\n\n\n\n\n\nHypothesis Testing for correlation coefficients\n\n\n\n\n\nPearson’s \\(r\\) test\nSpearman’s \\(r_{s}\\) test\nKendal’s \\(\\tau\\) test\nCoefficient \\(ξ\\) test\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\\(H_0\\): There is no linear association between the two numeric variables (they are independent, \\(ρ = 0\\))\n\n\\(H_1\\): There is linear association between the two numeric variables (they are dependent, \\(ρ \\neq 0\\))\n\n\ncor.test(BirthWeight$height, BirthWeight$weight) # the default method is \"pearson\"\n\n\n    Pearson's product-moment correlation\n\ndata:  BirthWeight$height and BirthWeight$weight\nt = 23.785, df = 548, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6689714 0.7515394\nsample estimates:\n      cor \n0.7127154 \n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\\(H_0\\): There is no monotonic association between the two numeric variables (they are independent)\n\n\\(H_1\\): There is monotonic association between the two numeric variables (they are dependent)\n\n\ncor.test(BirthWeight$height, BirthWeight$weight, method = \"spearman\")\n\nWarning in cor.test.default(BirthWeight$height, BirthWeight$weight, method =\n\"spearman\"): Cannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  BirthWeight$height and BirthWeight$weight\nS = 8015368, p-value < 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.7109399 \n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\\(H_0\\): There is no monotonic association between the two numeric variables (they are independent)\n\n\\(H_1\\): There is monotonic association between the two numeric variables (they are dependent)\n\n\ncor.test(BirthWeight$height, BirthWeight$weight, method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  BirthWeight$height and BirthWeight$weight\nz = 18.34, p-value < 2.2e-16\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.5511955 \n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\\(H_0\\): There is not association between the two numeric variables (they are independent)\n\n\\(H_1\\): There is association between the two numeric variables (they are dependent)\n\n\nxicor(BirthWeight$height, BirthWeight$weight, pvalue = TRUE)\n\n$xi\n[1] 0.3379234\n\n$sd\n[1] 0.02701652\n\n$pval\n[1] 0"
  },
  {
    "objectID": "chi_square.html",
    "href": "chi_square.html",
    "title": "9  Chi-sqaure test of independence",
    "section": "",
    "text": "If we want to see whether there’s an association between two categorical variables we can use the Pearson’s chi-square test, often called chi-square test of independence. This is an extremely elegant statistic based on the simple idea of comparing the frequencies we observe in certain categories to the frequencies we might expect to get in those categories by chance.\nWhen we have finished this Chapter, we should be able to:"
  },
  {
    "objectID": "chi_square.html#research-question-and-hypothesis-testing",
    "href": "chi_square.html#research-question-and-hypothesis-testing",
    "title": "9  Chi-sqaure test of independence",
    "section": "\n9.1 Research question and Hypothesis Testing",
    "text": "9.1 Research question and Hypothesis Testing\nWe will use the “Survival from Malignant Melanoma” dataset named “meldata”. The data consist of measurements made on patients with malignant melanoma, a type of skin cancer. Each patient had their tumor removed by surgery at the Department of Plastic Surgery, University Hospital of Odense, Denmark, between 1962 and 1977.\nSuppose we are interested in the association between tumor ulceration and death from melanoma.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There is no association between the two categorical variables (they are independent)\n\n\\(H_1\\): There is association between the two categorical variables (they are dependent)\n\n\n\nNOTE: In practice, the null hypothesis of independence, for our particular question, is no difference in the proportion of patients with ulcerated tumors who die compared with non-ulcerated tumors (\\(p_{ulcerated} = p_{non-ucerated}\\))."
  },
  {
    "objectID": "chi_square.html#packages-we-need",
    "href": "chi_square.html#packages-we-need",
    "title": "9  Chi-sqaure test of independence",
    "section": "\n9.2 Packages we need",
    "text": "9.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\n\n\nlibrary(patchwork)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "chi_square.html#preraring-the-data",
    "href": "chi_square.html#preraring-the-data",
    "title": "9  Chi-sqaure test of independence",
    "section": "\n9.3 Preraring the data",
    "text": "9.3 Preraring the data\nWe import the data meldata in R:\n\nlibrary(readxl)\nmeldata <- read_excel(here(\"data\", \"meldata.xlsx\"))\n\n\n\n\nFigure 9.1: Table with data from “meldata” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(meldata)\n\nRows: 205\nColumns: 7\n$ time      <dbl> 10, 30, 35, 99, 185, 204, 210, 232, 232, 279, 295, 355, 386,…\n$ status    <chr> \"Alive\", \"Alive\", \"Alive\", \"Alive\", \"Died\", \"Died\", \"Died\", …\n$ sex       <chr> \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"F…\n$ age       <dbl> 76, 56, 41, 71, 52, 28, 77, 60, 49, 68, 53, 64, 68, 63, 14, …\n$ year      <dbl> 1972, 1968, 1977, 1968, 1965, 1971, 1972, 1974, 1968, 1971, …\n$ thickness <dbl> 6.76, 0.65, 1.34, 2.90, 12.08, 4.84, 5.16, 3.22, 12.88, 7.41…\n$ ulcer     <chr> \"Present\", \"Absent\", \"Absent\", \"Absent\", \"Present\", \"Present…\n\n\nThe data set meldata has 250 patients (rows) and includes seven variables (columns). We are interested in the character (<chr>) ulcer variable and the character (<chr>) status variable which should be converted to factor (<fct>) variables using the convert_as_factor() function as follows:\n\nmeldata <- meldata %>%\n  convert_as_factor(status, ulcer)\n\nglimpse(meldata)\n\nRows: 205\nColumns: 7\n$ time      <dbl> 10, 30, 35, 99, 185, 204, 210, 232, 232, 279, 295, 355, 386,…\n$ status    <fct> Alive, Alive, Alive, Alive, Died, Died, Died, Alive, Died, D…\n$ sex       <chr> \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"F…\n$ age       <dbl> 76, 56, 41, 71, 52, 28, 77, 60, 49, 68, 53, 64, 68, 63, 14, …\n$ year      <dbl> 1972, 1968, 1977, 1968, 1965, 1971, 1972, 1974, 1968, 1971, …\n$ thickness <dbl> 6.76, 0.65, 1.34, 2.90, 12.08, 4.84, 5.16, 3.22, 12.88, 7.41…\n$ ulcer     <fct> Present, Absent, Absent, Absent, Present, Present, Present, …"
  },
  {
    "objectID": "chi_square.html#plot-the-data",
    "href": "chi_square.html#plot-the-data",
    "title": "9  Chi-sqaure test of independence",
    "section": "\n9.4 Plot the data",
    "text": "9.4 Plot the data\nWe are interested in the association between tumor ulceration and death from melanoma. It is useful to plot the data as counts but also as percentages. It is percentages we are comparing, but we really want to know the absolute numbers as well.\n\np1 <- meldata %>%\n  ggplot(aes(x = ulcer, fill = status)) +\n  geom_bar(width = 0.7) +\n  scale_fill_jco() +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"bottom\")\n\n\np2 <- meldata %>%\n  ggplot(aes(x = ulcer, fill = status)) +\n  geom_bar(position = \"fill\", width = 0.7) +\n  scale_y_continuous(labels=scales::percent) +\n  scale_fill_jco() +\n  ylab(\"Percentage\") +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"bottom\") \n\np1 + p2 + \n  plot_layout(guides = \"collect\") & theme(legend.position = 'bottom')\n\n\n\nFigure 9.2: Bar plot.\n\n\n\n\nJust from the plot, death from melanoma in the ulcerated tumor group is around 40% and in the non-ulcerated group around 13%. The number of patients included in the study is not huge, however, this still looks like a real difference given its effect size."
  },
  {
    "objectID": "chi_square.html#contigency-table-and-expected-frequencies",
    "href": "chi_square.html#contigency-table-and-expected-frequencies",
    "title": "9  Chi-sqaure test of independence",
    "section": "\n9.5 Contigency table and Expected frequencies",
    "text": "9.5 Contigency table and Expected frequencies\nFirst, we will create a contingency 2x2 table (two categorical variables with exactly two levels each) with the frequencies using the Base R.\n\ntb1 <- table(meldata$ulcer, meldata$status)\ntb1\n\n         \n          Alive Died\n  Absent     99   16\n  Present    49   41\n\n\nNext, we will also create a more informative table with row percentages and marginal totals.\n\n\n\n\n\n\nTable with row percentages and marginal totals\n\n\n\n\n\nfinalfit\nmodelsummary\n\n\n\nUsing the function summary_factorlist() which is included in finalfit package for obtaining row percentages and marginal totals:\n\nrow_tb1 <- meldata %>%\n  finalfit::summary_factorlist(dependent = \"status\", add_dependent_label = T,\n                     explanatory = \"ulcer\", add_col_totals = T,\n                     include_col_totals_percent = F,\n                     column = FALSE, total_col = TRUE)\n\nknitr::kable(row_tb1) \n\n\n\nDependent: status\n\nAlive\nDied\nTotal\n\n\n\nTotal N\n\n148\n57\n205\n\n\nulcer\nAbsent\n99 (86.1)\n16 (13.9)\n115 (100)\n\n\n\nPresent\n49 (54.4)\n41 (45.6)\n90 (100)\n\n\n\n\n\n\n\nThe contingency table using the datasummary_crosstab() from the modelsummary package:\n\nmodelsummary::datasummary_crosstab(ulcer ~ status, data = meldata)\n\n\n\n\n ulcer \n      \n    Alive \n    Died \n    All \n  \n\n\n Absent \n    N \n    99 \n    16 \n    115 \n  \n\n  \n    % row \n    86.1 \n    13.9 \n    100.0 \n  \n\n Present \n    N \n    49 \n    41 \n    90 \n  \n\n  \n    % row \n    54.4 \n    45.6 \n    100.0 \n  \n\n All \n    N \n    148 \n    57 \n    205 \n  \n\n  \n    % row \n    72.2 \n    27.8 \n    100.0 \n  \n\n\n\n\n\n\n\n\n\nFrom the raw frequencies, there seems to be a large difference, as we noted in the plot we made above. The proportion of patients with ulcerated tumors who die equals to 45.6% compared with non-ulcerated tumors 13.9%."
  },
  {
    "objectID": "chi_square.html#assumptions",
    "href": "chi_square.html#assumptions",
    "title": "9  Chi-sqaure test of independence",
    "section": "\n9.6 Assumptions",
    "text": "9.6 Assumptions\n\n\n\n\n\n\nCheck if the following assumption is satisfied\n\n\n\nA commonly stated assumption of the chi-square test is the requirement to have an expected count of at least 5 in each cell of the 2x2 table.\nFor larger tables, all expected counts should be > 1 and no more than 20% of all cells should have expected counts < 5.\n\n\nWe can calculate the expected frequencies for each cell using the expected() function from {epitools} package:\n\nepitools::expected(tb1)\n\n         \n             Alive     Died\n  Absent  83.02439 31.97561\n  Present 64.97561 25.02439\n\n\nHere, as we observe the assumption is fulfilled."
  },
  {
    "objectID": "chi_square.html#run-pearsons-chi-square-test",
    "href": "chi_square.html#run-pearsons-chi-square-test",
    "title": "9  Chi-sqaure test of independence",
    "section": "\n9.7 Run Pearson’s chi-square test",
    "text": "9.7 Run Pearson’s chi-square test\nFinally, we run the chi-square test:\n\n\n\n\n\n\nchi-square test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nchisq.test(tb1)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tb1\nX-squared = 23.631, df = 1, p-value = 1.167e-06\n\n\n\n\n\nchisq_test(tb1)\n\n# A tibble: 1 × 6\n      n statistic          p    df method          p.signif\n* <int>     <dbl>      <dbl> <int> <chr>           <chr>   \n1   205      23.6 0.00000117     1 Chi-square test ****    \n\n\n\n\n\n\n\nThere is evidence for an association between the ulcer and status (reject \\(H_0\\)). The proportion of patients with ulcerated tumors who died (45.6%) is significant larger compared with non-ulcerated tumors (13.9%) (p<0.001)."
  },
  {
    "objectID": "chi_square.html#risk-ratio-and-odds-ratio",
    "href": "chi_square.html#risk-ratio-and-odds-ratio",
    "title": "9  Chi-sqaure test of independence",
    "section": "\n9.8 Risk Ratio and Odds ratio",
    "text": "9.8 Risk Ratio and Odds ratio\nRisk ratio\nFrom the data in the following table\n\nepitools::table.margins(tb1)\n\n         \n          Alive Died Total\n  Absent     99   16   115\n  Present    49   41    90\n  Total     148   57   205\n\n\nwe can calculate the risk ratio by hand: \\[ Risk \\ Ratio = \\frac{\\frac{41}{90}}{\\frac{16}{115}} =\\frac{0.4556}{0.1391} = 3.27\\]\nThe risk ratio with the 95% CI using R:\n\nepitools::riskratio(tb1)$measure\n\n         risk ratio with 95% C.I.\n          estimate    lower    upper\n  Absent  1.000000       NA       NA\n  Present 3.274306 1.970852 5.439819\n\n\nThe risk of dying is 3.27 (95% CI: 1.97, 5.4) times higher for patients with ulcerated tumors compared to non-ulcerated tumors.\nOdds ratio\nWe can also calculate the odds ratio by hand: \\[ Odds \\ Ratio = \\frac{\\frac{41}{49}}{\\frac{16}{99}} =\\frac{0.837}{0.162} = 5.17\\] The odds ratio with the 95% CI using R:\n\nepitools::oddsratio(tb1, method = \"wald\")$measure\n\n         odds ratio with 95% C.I.\n          estimate    lower   upper\n  Absent  1.000000       NA      NA\n  Present 5.177296 2.645152 10.1334\n\n\nThe odds of dying is 5.17 (95% CI: 2.65, 10.13) times higher for patients with ulcerated tumors compared to non-ulcerated tumors patients.\nFinnaly, we can also reverse the odds ratio: \\[ \\frac{1}{OR} = \\frac{1}{5.17} = 0.193\\]\n\nepitools::oddsratio(tb1, method = \"wald\", rev = \"rows\")$measure\n\n         odds ratio with 95% C.I.\n          estimate      lower   upper\n  Present 1.000000         NA      NA\n  Absent  0.193151 0.09868354 0.37805\n\n\nThe non-ulcerated tumors patients has 0.193 (95% CI: 0.098, 0.378) times the odds (of dying) of the ulcerated tumors. This means that the non-ulcerated tumors patients has (0.193 - 1= -0.807) 80.7% lower odds of dying than ulcerated tumors."
  },
  {
    "objectID": "fisher_exact.html",
    "href": "fisher_exact.html",
    "title": "10  Fisher’s exact test",
    "section": "",
    "text": "If we want to see whether there’s an association between two categorical variables and the assumption for the expected frequencies in the contingency table is not fulfilled, an alternative test to the chi-square test can be used.\nFisher came up with a method for computing the exact probability of the chi-square statistic that is accurate when sample sizes are small. This method is called Fisher’s exact test even though it’s not so much a test as a way of computing the exact probability of the chi-square statistic. This procedure is normally used on 2×2 contingency tables and with small samples. However, it can be used on larger contingency tables and with large samples, but in this case it becomes computationally intensive.\nWhen we have finished this Chapter, we should be able to:"
  },
  {
    "objectID": "fisher_exact.html#research-question-and-hypothesis-testing",
    "href": "fisher_exact.html#research-question-and-hypothesis-testing",
    "title": "10  Fisher’s exact test",
    "section": "\n10.1 Research question and Hypothesis Testing",
    "text": "10.1 Research question and Hypothesis Testing\nWe consider the data in hemophilia dataset. In a survey there are two treatment regimens studied for controlling bleeding in 28 patients with hemophilia undergoing surgery. We want to investigate if there is an association between the treatment regimen (treatment A or B) and the bleeding complications (no or yes). The null hypothesis (\\(H_0\\)) is that the bleeding complications are independent from the treatment regimen, while the alternative (\\(H_1\\)) is that are dependent.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There is no association between the two categorical variables (they are independent)\n\n\\(H_1\\): There is association between the two categorical variables (they are dependent)\n\n\n\nNOTE: In practice, the null hypothesis of independence, for our particular question, is no difference in the proportion of patients with bleeding complications compared with patients with no bleeding complications (\\(p_{bleeding} = p_{no bleeding}\\))."
  },
  {
    "objectID": "fisher_exact.html#packages-we-need",
    "href": "fisher_exact.html#packages-we-need",
    "title": "10  Fisher’s exact test",
    "section": "\n10.2 Packages we need",
    "text": "10.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\n\n\nlibrary(patchwork)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "fisher_exact.html#preraring-the-data",
    "href": "fisher_exact.html#preraring-the-data",
    "title": "10  Fisher’s exact test",
    "section": "\n10.3 Preraring the data",
    "text": "10.3 Preraring the data\nWe import the data meldata in R:\n\nlibrary(readxl)\nhemophilia <- read_excel(here(\"data\", \"hemophilia.xlsx\"))\n\n\n\n\nFigure 10.1: Table with data from “meldata” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(hemophilia)\n\nRows: 28\nColumns: 2\n$ treatment <chr> \"A\", \"A\", \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"A\", \"A\", \"B\", \"A\", …\n$ bleeding  <chr> \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\"…\n\n\nThe dataset hemophilia has 28 patients (rows) and includes 2 variables (columns), the character (<chr>) variable named treatment and the character (<chr>) variable named bleeding. Both of them should be converted to factor (<fct>) variables using the convert_as_factor() function as follows:\n\nhemophilia <- hemophilia %>%\n  convert_as_factor(treatment, bleeding)\n\nglimpse(hemophilia)\n\nRows: 28\nColumns: 2\n$ treatment <fct> A, A, A, B, A, B, B, A, A, A, B, A, B, B, A, A, B, B, B, A, …\n$ bleeding  <fct> no, no, no, yes, no, no, no, no, yes, no, no, no, yes, no, n…"
  },
  {
    "objectID": "fisher_exact.html#plot-the-data",
    "href": "fisher_exact.html#plot-the-data",
    "title": "10  Fisher’s exact test",
    "section": "\n10.4 Plot the data",
    "text": "10.4 Plot the data\nWe count the number of patients with bleeding in the two regimens. It is useful to plot this as counts but also as percentages and compare them.\n\np3 <- hemophilia %>%\n  ggplot(aes(x = treatment, fill = bleeding)) +\n  geom_bar(width = 0.7) +\n  scale_fill_jama() +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"bottom\")\n\n\np4 <- hemophilia %>%\n  ggplot(aes(x = treatment, fill = bleeding)) +\n  geom_bar(position = \"fill\", width = 0.7) +\n  scale_y_continuous(labels=scales::percent) +\n  scale_fill_jama() +\n  ylab(\"Percentage\") +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"bottom\") \n\np3 + p4 + \n  plot_layout(guides = \"collect\") & theme(legend.position = 'bottom')\n\n\n\nFigure 10.2: Bar plot.\n\n\n\n\nThe above bar plots with counts show graphically that the number of patients who had bleeding complications was similar in the two regimens. Note that the number of patients included in the study is small (n=28)."
  },
  {
    "objectID": "fisher_exact.html#contigency-table-and-expected-frequencies",
    "href": "fisher_exact.html#contigency-table-and-expected-frequencies",
    "title": "10  Fisher’s exact test",
    "section": "\n10.5 Contigency table and Expected frequencies",
    "text": "10.5 Contigency table and Expected frequencies\nFirst, we will create a contingency 2x2 table (two categorical variables with exactly two levels each) with the frequencies using the Base R.\n\ntb2 <- table(hemophilia$treatment, hemophilia$bleeding)\ntb2\n\n   \n    no yes\n  A 13   2\n  B 10   3\n\n\nNext, we will also create a more informative table with row percentages and marginal totals.\n\n\n\n\n\n\nTable with row percentages and marginal totals\n\n\n\n\n\nfinalfit\nmodelsummary\n\n\n\nUsing the function summary_factorlist() which is included in finalfit package for obtaining row percentages and marginal totals:\n\nrow_tb2 <- hemophilia %>%\n  finalfit::summary_factorlist(dependent = \"bleeding\", add_dependent_label = T,\n                     explanatory = \"treatment\", add_col_totals = T,\n                     include_col_totals_percent = F,\n                     column = FALSE, total_col = TRUE)\n\nknitr::kable(row_tb2) \n\n\n\nDependent: bleeding\n\nno\nyes\nTotal\n\n\n\nTotal N\n\n23\n5\n28\n\n\ntreatment\nA\n13 (86.7)\n2 (13.3)\n15 (100)\n\n\n\nB\n10 (76.9)\n3 (23.1)\n13 (100)\n\n\n\n\n\n\n\nThe contingency table using the datasummary_crosstab() from the modelsummary package:\n\nmodelsummary::datasummary_crosstab(treatment ~ bleeding, data = hemophilia)\n\n\n\n\n treatment \n      \n    no \n    yes \n    All \n  \n\n\n A \n    N \n    13 \n    2 \n    15 \n  \n\n  \n    % row \n    86.7 \n    13.3 \n    100.0 \n  \n\n B \n    N \n    10 \n    3 \n    13 \n  \n\n  \n    % row \n    76.9 \n    23.1 \n    100.0 \n  \n\n All \n    N \n    23 \n    5 \n    28 \n  \n\n  \n    % row \n    82.1 \n    17.9 \n    100.0 \n  \n\n\n\n\n\n\n\n\n\nFrom the row frequencies, there is not actually difference, as we noted in the plot we made above.\nNow, we will calculate the expected frequencies for each cell using the expected() function from {epitools} package:\n\nepitools::expected(tb2)\n\n   \n          no      yes\n  A 12.32143 2.678571\n  B 10.67857 2.321429\n\n\nIn the above table there are 2 cells (50%) with expected counts less than 5 (specifically 2.67 and 2.32), so the Chi-square test is not the appropriate one. In this case the Fisher’s exact test should be used instead."
  },
  {
    "objectID": "fisher_exact.html#run-fishers-exact-test",
    "href": "fisher_exact.html#run-fishers-exact-test",
    "title": "10  Fisher’s exact test",
    "section": "\n10.6 Run Fisher’s exact test",
    "text": "10.6 Run Fisher’s exact test\nFinally, we run the Fisher’s exact test:\n\n\n\n\n\n\nFisher’s exact test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nfisher.test(tb2)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  tb2\np-value = 0.6389\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.1807204 26.9478788\nsample estimates:\nodds ratio \n   1.90363 \n\n\n\n\n\nfisher_test(tb2)\n\n# A tibble: 1 × 3\n      n     p p.signif\n* <int> <dbl> <chr>   \n1    28 0.639 ns      \n\n\n\n\n\n\n\nThe p = 0.64 is higher than 0.05. There is absence of evidence for an association between the treatment regimens and bleeding complications (failed to reject \\(H_0\\))."
  },
  {
    "objectID": "fisher_exact.html#having-only-the-counts",
    "href": "fisher_exact.html#having-only-the-counts",
    "title": "10  Fisher’s exact test",
    "section": "\n10.7 Having only the counts",
    "text": "10.7 Having only the counts\nWhen we read an article which reports a chi-square or a fisher exact analysis we will see only the counts in a table without having the raw data of the categorical variables. In this instance, we can create the table using the matrix() function and run the tests. For our example of hemophilia we have the following table:\n\ndat <- c(13, 10, 2, 3)\nmx <- matrix(dat, nrow = 2, dimnames = list(c(\"A\", \"B\"), c(\"no\", \"yes\")))\nmx\n\n  no yes\nA 13   2\nB 10   3"
  },
  {
    "objectID": "mcnemar.html",
    "href": "mcnemar.html",
    "title": "11  McNemar’s test",
    "section": "",
    "text": "The McNemar’s test (also known as the paired or matched chi-square) is used to determine if there are differences on a dichotomous dependent variable between two related groups. It can be considered to be similar to the paired-samples t-test, but for a dichotomous rather than a continuous dependent variable. The McNemar’s test is used to analyze pretest-posttest study designs (observing categorical outcomes more than once in the same patient), as well as being commonly employed in analyzing matched pairs and case-control studies.\nWhen we have finished this Chapter, we should be able to:"
  },
  {
    "objectID": "mcnemar.html#research-question-and-hypothesis-testing",
    "href": "mcnemar.html#research-question-and-hypothesis-testing",
    "title": "11  McNemar’s test",
    "section": "\n11.1 Research question and Hypothesis Testing",
    "text": "11.1 Research question and Hypothesis Testing\nWe consider the data in asthma dataset. The dataset contains data from a survey of 86 children with asthma who attended a camp to learn how to self-manage their asthmatic episodes. The children were asked whether they knew (yes or not) how to manage their asthmatic episodes appropriately at both the start and completion of the camp.\nIn other words, was a significant change in children’s knowledge of asthma management between the beginning and completion of the health camp?\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There was no change in children’s knowledge of asthma management between the beginning and completion of the health camp\n\n\\(H_1\\): There was change in children’s knowledge of asthma management between the beginning and completion of the health camp"
  },
  {
    "objectID": "mcnemar.html#packages-we-need",
    "href": "mcnemar.html#packages-we-need",
    "title": "11  McNemar’s test",
    "section": "\n11.2 Packages we need",
    "text": "11.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(janitor)\nlibrary(modelsummary)\nlibrary(exact2x2)\n\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "mcnemar.html#preraring-the-data",
    "href": "mcnemar.html#preraring-the-data",
    "title": "11  McNemar’s test",
    "section": "\n11.3 Preraring the data",
    "text": "11.3 Preraring the data\nWe import the data asthma in R:\n\nlibrary(readxl)\nasthma <- read_excel(here(\"data\", \"asthma.xlsx\"))\n\n\n\n\nFigure 11.1: Table with data from “asthma” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(asthma)\n\nRows: 86\nColumns: 2\n$ know_begin <chr> \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"y…\n$ know_end   <chr> \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"…\n\n\nThe dataset asthma includes 86 children with asthma (rows) and 2 columns, the character (<chr>) know_begin and the character (<chr>) know_end. Therefore, we consider the dichotomous dependent variable asthma knowledge (yes/no) between two time points, know_begin and know_end.\nBoth measurements know_begin and know_end should be converted to factors (<fct>) using the convert_as_factor() function as follows:\n\nasthma <- asthma %>%\n  convert_as_factor(know_begin, know_end)\n\nglimpse(asthma)\n\nRows: 86\nColumns: 2\n$ know_begin <fct> yes, no, yes, no, no, no, yes, no, no, yes, no, no, yes, ye…\n$ know_end   <fct> yes, no, no, no, no, no, yes, yes, yes, yes, yes, no, yes, …"
  },
  {
    "objectID": "mcnemar.html#contigency-table",
    "href": "mcnemar.html#contigency-table",
    "title": "11  McNemar’s test",
    "section": "\n11.4 Contigency table",
    "text": "11.4 Contigency table\nWe can obtain the cross-tabulation table of the two measurements for the children’s knowledge of asthma:\n\ntb3 <- table(know_begin = asthma$know_begin, know_end = asthma$know_end)\ntb3\n\n          know_end\nknow_begin no yes\n       no  27  29\n       yes  6  24\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThere is a basic difference between this table and the more common two-way table. In this case, the count represents the number of pairs, not the number of individuals.\n\n\nWe want to compare the proportion of children’s knowledge of asthma management at the beginning with the proportion of children’s knowledge of asthma management at the end. We can create a more informative table using the functions from janitor package for obtaining total percentages and marginal totals.\n\n\n\n\n\n\nTable with total percentages and marginal totals\n\n\n\n\n\njanitor\nmodelsummary\n\n\n\nWe can create an informative table using the functions from janitor package for obtaining total percentages and marginal totals:\n\ntotal_tb2 <- asthma %>%\n  tabyl(know_begin, know_end) %>%\n  adorn_totals(c(\"row\", \"col\")) %>%\n  adorn_percentages(\"all\") %>%\n  adorn_pct_formatting(digits = 1) %>%\n  adorn_ns %>%\n  adorn_title\n\nknitr::kable(total_tb2) \n\n\n\n\nknow_end\n\n\n\n\n\nknow_begin\nno\nyes\nTotal\n\n\nno\n31.4% (27)\n33.7% (29)\n65.1% (56)\n\n\nyes\n7.0% (6)\n27.9% (24)\n34.9% (30)\n\n\nTotal\n38.4% (33)\n61.6% (53)\n100.0% (86)\n\n\n\n\n\n\n\nThe contingency table using the datasummary_crosstab() from the modelsummary package:\n\nmodelsummary::datasummary_crosstab(know_begin ~ know_end, \n                     statistic = 1 ~ 1 + N + Percent(), \n                     data = asthma)\n\n\n\n\n know_begin \n      \n    no \n    yes \n    All \n  \n\n\n no \n    N \n    27 \n    29 \n    56 \n  \n\n  \n    % \n    31.4 \n    33.7 \n    65.1 \n  \n\n yes \n    N \n    6 \n    24 \n    30 \n  \n\n  \n    % \n    7.0 \n    27.9 \n    34.9 \n  \n\n All \n    N \n    33 \n    53 \n    86 \n  \n\n  \n    % \n    38.4 \n    61.6 \n    100.0 \n  \n\n\n\n\n\n\n\n\n\nThe proportion of children who knew to manage asthma at the beginning is (6+24)/86= 30/86 = 0.349 or 34.9%. The proportion of children who knew to mange asthma at the end is (29+24)/86 = 53/86 = 0.616 or 61.6%.\n\n\n\n\n\n\nAssumption\n\n\n\nThe basic assumption of the test is that the sum of the discordant cells should be larger than 25 (that is fulfilled in our example)."
  },
  {
    "objectID": "mcnemar.html#run-mcnemars-test",
    "href": "mcnemar.html#run-mcnemars-test",
    "title": "11  McNemar’s test",
    "section": "\n11.5 Run McNemar’s test",
    "text": "11.5 Run McNemar’s test\nFinally, we run the McNemar’s test:\n\n\n\n\n\n\nMcNemar’s test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nmcnemar.test(tb3)\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  tb3\nMcNemar's chi-squared = 13.829, df = 1, p-value = 0.0002003\n\n\n\n\n\nmcnemar_test(tb3)\n\n# A tibble: 1 × 6\n      n statistic    df      p p.signif method      \n* <int>     <dbl> <dbl>  <dbl> <chr>    <chr>       \n1    86      13.8     1 0.0002 ***      McNemar test\n\n\n\n\n\n\n\nThe proportion of children who knew to manage asthma at the end (61.6%) is significant larger compared with the proportion of children who knew to manage asthma at the beginning (34.9%) (p-value <0.001)."
  },
  {
    "objectID": "mcnemar.html#exact-binomial-test",
    "href": "mcnemar.html#exact-binomial-test",
    "title": "11  McNemar’s test",
    "section": "\n11.6 Exact binomial test",
    "text": "11.6 Exact binomial test\nExact binomial test for 2x2 table when the sum of the discordant cells are less than 25:\n\nmcnemar.exact(tb3)\n\n\n    Exact McNemar test (with central confidence intervals)\n\ndata:  tb3\nb = 29, c = 6, p-value = 0.0001168\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  1.971783 14.238838\nsample estimates:\nodds ratio \n  4.833333"
  }
]