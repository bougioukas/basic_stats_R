# Foundations for statistical inference {#sec-inference}

::: {.callout-caution icon="false"}
## Learning objectives

-   Understand the hypothesis testing
-   Understand the type of errors
:::

## Hypothesis Testsing

Hypothesis testing is a method of deciding whether the data are consistent with the null hypothesis. The calculation of the p-value is an important part of the procedure. Given a study with a single outcome measure and a statistical test, hypothesis testing can be summarized in five steps.

::: {.callout-tip icon="false"}
## Steps of Hypothesis Testsing

**Step 1:** State the null hypothesis, $H_{0}$, and alternative hypothesis, $H_{1}$, based on the research question.

> NOTE: We decide a non-directional $H_{1}$ (also known as two-sided hypothesis) whether we test for effects in both directions (most common), otherwise a directional (also known as one-sided) hypothesis.

**Step 2:** Set the level of significance, α (usually 0.05).

**Step 3:** Identify the appropriate test statistic and check the assumptions. Calculate the test statistic using the data.

> NOTE: There are two basic types of statistical tests and they are described as parametric and non-parametric. The parametric tests (e.g., t-test, ANOVA), make certain assumptions about the distribution of the unknown parameter of interest and thus the test statistic is valid under these assumptions. For non-parametric tests (e.g., Mann-Whitney U test, Kruskal-Wallis test), there are no such assumptions. Most nonparametric tests use some way of ranking the measurements. Non-parametric tests are about 95% as powerful as parametric tests.

**Step 4:** Decide whether or not the result is statistically significant.

> The p-value is the **probability of obtaining the observed results, or something more extreme, if the null hypothesis is true**.

Using the known distribution of the test statistic and according to the result of our statistical test, we calculate the corresponding p-value. Then we compare the p-value to the significance level α:

-   If p − value \< α, reject the null hypothesis, $H_{0}$.
-   If p − value ≥ α, do not reject the null hypothesis, $H_{0}$.

The @tbl-p demonstrates how to interpret the strength of the evidence. However, always keep in mind the size of the study being considered.

| p-value              | Interpretation                         |
|----------------------|----------------------------------------|
| $p \geq{0.10}$       | No evidence to reject $H_{0}$          |
| $0.05\leq p < 0.10$  | Weak evidence to reject $H_{0}$        |
| $0.01\leq p < 0.05$  | Evidence to reject $H_{0}$             |
| $0.001\leq p < 0.01$ | Strong evidence to reject $H_{0}$      |
| $p < 0.001$          | Very strong evidence to reject $H_{0}$ |

: Strength of the evidence against $H_{0}$. {#tbl-p}

**Step 5:** Interpret the results.
:::

## Type of Errors in hypothesis testing

**Type I error:** we reject the null hypothesis when it is true (false positive), and conclude that there is an effect when, in reality, there is none. The maximum chance (probability) of making a Type I error is denoted by α (alpha). This is the significance level of the test; we reject the null hypothesis if our p-value is less than the significance level, i.e. if p \< a (@fig-type_errors).

**Type II error:** we do not reject the null hypothesis when it is false (false negative), and conclude that there is no effect when one really exists. The chance of making a Type II error is denoted by β (beta); its compliment, (1 - β), is the power of the test. The power, therefore, is the probability of rejecting the null hypothesis when it is false; i.e. it is the chance (usually expressed as a percentage) of detecting, as statistically significant, a real treatment effect of a given size (@fig-type_errors). @tbl-power presents the main factors that can influence the power in a study.

```{r}
#| label: fig-type_errors
#| fig-align: center
#| out-width: "75%"
#| echo: false
#| fig-cap: Types of error in hypothesis testing.

knitr::include_graphics(here::here("images", "type_errors.png"))
```

| Factor                   | Influence on study's power                                                                                                                                                                                 |
|-------------------------|------------------------------------------------|
| **Effect Size**          | As effect size increases, power tends to increase (a larger effect size is easier to be detected by the statistical test, leading to a greater probability of a statistically significant result).         |
| **Sample Size**          | As the sample size goes up, power generally goes up (this factor is the most easily manipulated by researchers).                                                                                           |
| **Standard deviation**   | As variability decreases, power tends to increase (variability can be reduced by controlling extraneous variables such as inclusion and exclusion criteria defining the sample in a study).                |
| **Significance level α** | As α goes up, power goes up (it would be easier to find statistical significance with a larger significance level, e.g. α=0.1, compared with finding a significant result with a smaller α, e.g. α=0.05).  |

: Factors Influencing Power. {#tbl-power}
